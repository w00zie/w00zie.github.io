[{"authors":["admin"],"categories":null,"content":"This is my blog. I might write here some random stuff that interests me during my studies in Information Engineering. I\u0026rsquo;m not much of a coder but I may also embed some poorly-written python code along my posts. Hope you will find my content (sort of) interesting.\nCiao!\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https:w00zie.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"This is my blog. I might write here some random stuff that interests me during my studies in Information Engineering. I\u0026rsquo;m not much of a coder but I may also embed some poorly-written python code along my posts. Hope you will find my content (sort of) interesting.\nCiao!","tags":null,"title":"Giovanni Bindi","type":"authors"},{"authors":["open"],"categories":null,"content":" Videos English  Information Theory, Pattern Recognition and Neural Networks - David McKay, Cambridge (also book freely available online, thank you David!). Reinforcement Learning - David Silver, UCL/DeepMind. Advanced Deep Learning \u0026amp; Reiforcement Learning - Various, UCL. Deep Learning - Nando de Freitas, Oxford (check his channel for other courses!). General Relativity - Leonard Susskind, Stanford.  Italian  Fisica dei Sistemi Complessi e Teoria dell\u0026rsquo;Informazione - Università di Firenze. Relatività Generale - Random Physics. Geometria Differenziale - Università di Padova.  Tutorials \u0026amp; Channels  Tensorflow tutorials - Hvass Laboratories. deeplearning.ai - An YouTube channel run by Andrew Ng.  Books English Italian Podcasts English Various ","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"c72708463944c028601583f0552cfe44","permalink":"https:w00zie.github.io/authors/open/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/open/","section":"authors","summary":"Videos English  Information Theory, Pattern Recognition and Neural Networks - David McKay, Cambridge (also book freely available online, thank you David!). Reinforcement Learning - David Silver, UCL/DeepMind. Advanced Deep Learning \u0026amp; Reiforcement Learning - Various, UCL. Deep Learning - Nando de Freitas, Oxford (check his channel for other courses!). General Relativity - Leonard Susskind, Stanford.  Italian  Fisica dei Sistemi Complessi e Teoria dell\u0026rsquo;Informazione - Università di Firenze.","tags":null,"title":"Open Courses","type":"authors"},{"authors":["wabri"],"categories":null,"content":"Gabriele is a friend of mine.\nHe is an active developer interested in different fields of Computer Science, make sure to check out his blog!\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"5231dca2ff835897771ddc2d48aefd90","permalink":"https:w00zie.github.io/authors/wabri/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/wabri/","section":"authors","summary":"Gabriele is a friend of mine.\nHe is an active developer interested in different fields of Computer Science, make sure to check out his blog!","tags":null,"title":"Gabriele Puliti","type":"authors"},{"authors":null,"categories":null,"content":"One of the most known and widely used learning model is SVM (Support Vector Machine).\nIn this first post we are going to analyze the formal structure of the problem and we are going through the (simple) math needed to write down the SVM equations for both the linear and non-linear cases. Let's start with definitions.\nProblem Structure We are given a set of couples, named $T$, which is going to be our training set.\n\\[ T = \\{ (x_i, y_i) : x_i \\in X \\subseteq \\mathbb{R}^n, y_i \\in Y, i=1,\\dots,N \\}\\]\nThe $x_i$ are usually referred as observations while the $y_i$ are called labels or targets. We have $|T|=N$ data points.\nIf $Y = \\mathbb{R}$ we are dealing with regression, otherwise with classification. For this walkthrough we will deal with classification and so we'll adopt, without loss of generality, \\(Y = \\{+1, -1\\}\\).\n Linearly separable sets This is the simplest case: we have two disjoint sets, namely $P$ and $N$:\n\\[ P = \\{ x_i \\in T: y_i = +1 \\} \\\\ N = \\{ x_i \\in T: y_i = -1 \\} \\]\nthat are linearly separable. This condition is expressed through the existence of a separating hyperplane\n\\[ H(w,b) = \\{ x \\in \\mathbb{R}^n : w \\cdot x + b = 0 \\} \\]\nsuch that the points $x_i \\in P$ belong to an half space and the points $x_i \\in N$ to the other one. Formally $\\exists \\hspace{3pt} w \\in \\mathbb{R}^n, b \\in \\mathbb{R}$:\n\\[ w \\cdot x_i + b \\geq \\epsilon, \\hspace{15pt} \\forall x_i \\in P \\\\ w \\cdot x_j + b \\leq -\\epsilon, \\hspace{15pt} \\forall x_j \\in N \\]\nwith $\\epsilon \u0026gt; 0$. Dividing by $\\epsilon$ both members of both equations we obtain:\n\\[ w \\cdot x_i + b \\geq 1, \\hspace{15pt} \\forall x_i \\in P \\\\ w \\cdot x_j + b \\leq -1, \\hspace{15pt} \\forall x_j \\in N \\]\n\n Margin A fundamental concept in SVM is the margin, expressed as the minimum distance $\\gamma$ between the points in $D \\overset{\\Delta}{=} P \\cup N$ and the separating hyperplane $H(w,b)$:\n\\[ \\gamma(w, b) = \\min_{x \\in D} \\bigg \\{ \\frac{|w \\cdot x + b|}{||w||} \\bigg \\} \\]\nWhere $|| \\cdot || := || \\cdot ||_2$ represents the euclidean norm.\nDerivation This minimum distance formulation is well known in Geometry, but we can derive it from an optimization point of view as well. Let's start calculating the distance between a point $\\bar{x}$ and the hyperplane $H(w,b)$. We can write down this minimization problem as:\n\\[ \\min_{x \\in \\mathbb{R}^n} \\quad ||\\bar{x} - x|| \\\\ \\hspace{15pt}\\textrm{s.t.} \\quad w \\cdot x + b = 0 \\]\nWe can apply a strictly increasing transformation to our objective function (to get rid of the non-differentiability) without changing the global optima of our problem:\n\\[ \\min_{x \\in \\mathbb{R}^n} \\quad \\frac{1}{2}||\\bar{x} - x||^2 \\\\ \\hspace{2pt} \\textrm{s.t.} \\quad w \\cdot x + b = 0 \\]\nWe now have a strictly convex objective function subject to continuously differentiable linear constraints so we can apply the Karush–Kuhn–Tucker optimality conditions:\n\\[ \\nabla_x \\bigg [ \\frac{1}{2} ||\\bar{x} - x||^2 + \\lambda(w \\cdot x + b) \\bigg ] = 0 \\]\n\\[ \\hspace{67pt}= (\\bar{x} - x) + \\lambda w = 0 \\]\nExpressing $x = \\bar{x} + \\lambda w$ and substituting it into the constraint equation we obtain:\n\\[ w \\cdot (\\bar{x} + \\lambda w) + b \\]\n\\[ \\hspace{3pt} = w \\cdot \\bar{x} + \\lambda ( w \\cdot w) + b \\]\n\\[ \\hspace{14pt}= w \\cdot \\bar{x} + \\lambda ||w||^2 + b = 0 \\]\nFrom which we can isolate the lagrangean multiplier and write\n\\[\\lambda = -\\frac{w \\cdot \\bar{x} + b}{||w||^2}\\]\nWe were originally interested in $||\\bar{x} - x||$ and from the optimality conditions we can now re-write this quantity as:\n\\[||\\bar{x} - x|| = ||\\lambda w|| = \\frac{|w \\cdot \\bar{x} + b|}{||w||}\\]\nWhich is the projection of a given point $\\bar{x}$ on an hyperplane $H(w,b)$.\nWe can now obtain the definition of margin given above using this result, extending the definition so that we are interested in the minimum distance between every point $x \\in D$ to $H(w,b)$.\nIt is quite intuitive that the margin of a given separating hyperplane is related to the generalization capability of the corresponding linear classifier. This relationship is analyzed in the Statistical Learning Theory, which theoretically motivates the importance of defining the hyperplane with maximum margin, the so-called optimal separating hyperplane.\n Optimal Separating Hyperplane The optimal separating hyperplane is a separating hyperplane \\(H(w^*, b^*)\\) having maximum margin:\n\\[ \\max_{w \\in \\mathbb{R}^n, b \\in \\mathbb{R}} \\min_{x \\in D} \\bigg \\{ \\frac{|w \\cdot x + b|}{||w||} \\bigg \\} \\]\n Lagrange Dual ","date":1564704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564704000,"objectID":"5387f46fd4d5ac5a0e82e20e2fa80c12","permalink":"https:w00zie.github.io/post/svm/","publishdate":"2019-08-02T00:00:00Z","relpermalink":"/post/svm/","section":"post","summary":"One of the most known and widely used learning model is SVM (Support Vector Machine).\nIn this first post we are going to analyze the formal structure of the problem and we are going through the (simple) math needed to write down the SVM equations for both the linear and non-linear cases. Let's start with definitions.\nProblem Structure We are given a set of couples, named $T$, which is going to be our training set.","tags":null,"title":"SVM from an optimization point of view","type":"post"},{"authors":null,"categories":null,"content":"In this first post we'll give a brief introduction to the necessary linear algebra for Quantum Mechanics.\nLinear Algebra Vectors The basic objects of linear algebra are vector spaces. The vector space of most interest in QM is $\\mathbb{C}^n$, the space of all $n$-tuples of complex numbers $(z_1, \\dots, z_n)$. The elements of a vector space are called vectors. The standard quantum mechanical notation for a vector in a vector space is the following:\n\\[ \\lvert \\psi \\rangle \\]\nA spanning set for a vector space is a set of vectors \\(\\lvert v_1 \\rangle, \\dots, \\lvert v_n \\rangle\\) such that any vector \\(\\lvert v \\rangle\\) can be written as a linear combination \\(\\lvert v \\rangle = \\sum_{i} a_i \\lvert v_i \\rangle\\) of vectors in that set. For example, a spanning set for the vector space $\\mathbb{C}^2$ is the set:\n$$ \\lvert v_1 \\rangle =\n\\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix}; \\quad \\lvert v_2 \\rangle = \\begin{bmatrix} 0 \\\\ 1 \\\\ \\end{bmatrix}, $$  since any vector\n$$ \\lvert v \\rangle =\n\\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\end{bmatrix} $$  can be expressed as a linear combination $\\lvert v \\rangle = a_1 \\lvert v_1 \\rangle + a_2 \\lvert v_2 \\rangle$ of the vectors $\\lvert v_1 \\rangle$ and $\\lvert v_2 \\rangle$.\nGenerally, a vector space may have many different spanning sets. A second spanning set for $\\mathbb{C}^2$ is the set:\n$$ \\lvert v_1 \\rangle = \\frac{1}{\\sqrt2}\n\\begin{bmatrix} 1 \\\\ 1 \\\\ \\end{bmatrix}; \\quad \\lvert v_2 \\rangle = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ -1 \\\\ \\end{bmatrix}, $$  A set of non-zero vectors $\\lvert v_1 \\rangle, \\dots, \\lvert v_n \\rangle$ are linearly dependent if there exists a set of complex numbers $a_1, \\dots, a_n$ with $a_i \\neq 0$ for at least one value of $i$, such that:\n\\[ a_1 \\lvert v_1 \\rangle + a_2 \\lvert v_2 \\rangle + \\dots a_n \\lvert v_n \\rangle = 0 \\]\nA set of vectors is linearly independent if it is not linearly dependent. A set of linearly independent vectors form a basis for their relative vector space.\nLinear operators and matrices A linear operator between vector spaces $V$ and $W$ is defined to be any function $A:V \\to W$ which is linear in its inputs:\n\\[ A \\bigg ( \\sum_{i} a_i \\lvert v_i \\rangle \\bigg ) = \\sum_{i} a_i A(\\lvert v_i \\rangle)\\]\nAnd we just write $A \\lvert v \\rangle$ to denote this application.\nThe most convenient way to understand linear operators is in terms of their matrix representations. In fact, the linear operator and matrix viewpoints turn out to be completely equivalent.\nSuppose $\\lvert v_1 \\rangle, \\dots, \\lvert v_m \\rangle$ is a basis for \\(V\\) and \\(\\lvert w_1 \\rangle, \\dots, \\lvert w_n \\rangle\\) is a basis for \\(W\\). Then $\\forall j = {1, \\dots, m }$ there exists complex numbers $A_{1j}, \\dots, A_{nj}$ such that:\n\\[ A \\lvert v_j \\rangle = \\sum_{i} A_{ij} \\lvert w_i \\rangle \\]\nInner products An inner product is a function which takes as input two vectors $\\lvert v \\rangle$ and $\\lvert w \\rangle$ from a vector space and produces a complex number as output. The standard notation for the inner product is $\\langle v \\lvert w \\rangle$ where $\\langle v \\rvert$ is the dual vector to the vector $\\lvert v \\rangle$. A function $(\\cdot , \\cdot): V \\times V \\to \\mathbb{C}$ is an inner product if it satisfies:\n Linearity in the second argument: \\( \\bigg (\\lvert v \\rangle, \\sum_{i} \\lambda_i \\lvert w_i \\rangle \\bigg ) = \\sum_{i} \\lambda_{i} (\\lvert v \\rangle, \\lvert w_i \\rangle) \\)\n $(\\lvert v \\rangle , \\lvert w \\rangle) = (\\lvert w \\rangle, \\lvert v \\rangle)^*$\n $(\\lvert v \\rangle, \\lvert v \\rangle) \\geq 0$ with equality iff $\\lvert v \\rangle = 0$.\n  For example $\\mathbb{C}^n$ has an inner product defined by:\n\\[ ((y_1, \\dots, y_n ), (z_1, \\dots z_n)) = \\sum_{i} y^*_{i}z_i \\]\nWe call a vector space with an inner product an inner product space or (usually) an Hilbert space.\nWe define the norm of a vector $\\lvert v \\rangle$ by:\n\\[ || \\lvert v \\rangle || = \\sqrt{\\langle v \\lvert v \\rangle}\\]\nFor any non-zero vector $\\lvert v \\rangle$ we talk about normalization or normalized form the vector $\\lvert v \\rangle / || \\lvert v \\rangle ||$.\n","date":1555977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555977600,"objectID":"38a12ab686f836bad3b63eedaa5c1a41","permalink":"https:w00zie.github.io/post/quantum_1_linalg/","publishdate":"2019-04-23T00:00:00Z","relpermalink":"/post/quantum_1_linalg/","section":"post","summary":"In this first post we'll give a brief introduction to the necessary linear algebra for Quantum Mechanics.\nLinear Algebra Vectors The basic objects of linear algebra are vector spaces. The vector space of most interest in QM is $\\mathbb{C}^n$, the space of all $n$-tuples of complex numbers $(z_1, \\dots, z_n)$. The elements of a vector space are called vectors. The standard quantum mechanical notation for a vector in a vector space is the following:","tags":null,"title":"Quantum Information - Part 1: Linear Algebra","type":"post"}]