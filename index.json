[{"authors":["admin"],"categories":null,"content":"This is my blog. I might write here some random stuff that interests me during my studies in Information Engineering. I\u0026rsquo;m not much of a coder but I might also embed some (I hope not-too-much-poorly-written) python code along my posts. Hope you will find my content (sort of) interesting.\nCiao!\n","date":1596293173,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1596293173,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://w00zie.github.io/author/giovanni-bindi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/giovanni-bindi/","section":"authors","summary":"This is my blog. I might write here some random stuff that interests me during my studies in Information Engineering. I\u0026rsquo;m not much of a coder but I might also embed some (I hope not-too-much-poorly-written) python code along my posts.","tags":null,"title":"Giovanni Bindi","type":"authors"},{"authors":["books"],"categories":null,"content":"Books   Deep Learning - Ian Goodfellow, Yoshua Bengio, Aaron Courville, MIT press, 2016.  Information Theory, Pattern Recognition and Neural Networks - David McKay, Cambridge University Press, 2003.  Reinforcement Learning, an Introduction - Richard S. Sutton, Andrew G. Barto, MIT press, 2015.  Neural Networks and Deep Learning - Michael A. Nielsen1, 2015.  An Introduction to Statistical Learning - Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani, Springer, 2017.  The Elements of Statistical Learning - Trevor Hastie, Robert Tibshirani, Jerome Friedman, Springer, 2009.  Bayesian Reasoning and Machine Learning - David Barber, Cambridge, 2012.  Foundations of Machine Learning - Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar, MIT Press, 2018.  Python Data Science Handbook - Jake VanderPlas, O\u0026rsquo;Reilly, 2016.  Applied Optimum Signal Processing - Sophocles J. Orfanidis, 2018.   Articles   Computational Complexity for Physicists - Stephan Mertens, 2002.  The Discipline of Machine Learning - Tom M. Mitchell, 2006.  The Unreasonable Effectiveness of Data - Alon Halevy, Peter Norvig, and Fernando Pereira, 2009.  Automatic Differentiation in Machine Learning: A Survey - Baydin et al., 2018.  Stochastic Gradient Descent Tricks - Léon Bottou, 2012.  Memory Networks - Weston et al., 2016.  Understanding Dropout - Pierre Baldi, Peter Sadowski, 2013.     Michael Nielsen is also a co-author of a beautiful book: Quantum Computation \u0026amp; Quantum Information. \u0026#x21a9;\u0026#xfe0e;\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6c0c4357bafb20bffeea0f4b3fc20200","permalink":"https://w00zie.github.io/author/articles-books/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/articles-books/","section":"authors","summary":"Books   Deep Learning - Ian Goodfellow, Yoshua Bengio, Aaron Courville, MIT press, 2016.  Information Theory, Pattern Recognition and Neural Networks - David McKay, Cambridge University Press, 2003.  Reinforcement Learning, an Introduction - Richard S.","tags":null,"title":"Articles \u0026 Books","type":"authors"},{"authors":["courses"],"categories":null,"content":"English Information Theory   Information Theory, Pattern Recognition and Neural Networks - David McKay, Cambridge (also book freely available online, thank you David!).  Information Theory I \u0026amp; Information Theory II - Jeffrey A. Bilmes, University of Washington.  Reinforcement Learning   Reinforcement Learning - David Silver, UCL/DeepMind.  Advanced Deep Learning \u0026amp; Reiforcement Learning - Various, UCL.  Deep Reinforcement Learning - Various, Berkeley.  Learning Theory   Statistical Learning Theory and Applications - Tomaso Poggio, MIT.  Statistical Machine Learning - Ulrike von Luxburg, University of Tübingen.  Machine Learning   Probabilistic Machine Learning - Philipp Henning, University of Tübingen.  Deep Learning   Deep Learning - Nando de Freitas, Oxford (check his channel for other courses!).  Geometric Deep Learning - Michael Bronstein, TUM.  Deep Learning - Various, UCL/DeepMind.  Physics   General Relativity - Leonard Susskind, Stanford.  Statistical Mechanics - Leonard Susskind, Stanford.  Quantum Mechanics - Allan Adams, MIT.  Nonlinear Dynamics and Chaos - Steven Strogatz, Cornell University.  Italian   Fisica dei Sistemi Complessi e Teoria dell\u0026rsquo;Informazione - Università di Firenze.  Relatività Generale - Random Physics.  Geometria Differenziale - Università di Padova.  Tutorials \u0026amp; Channels   3Blue1Brown - One of the best scientific YouTube channels out there. Both contents and visualization techniques are stunningly good.  Computerphile and Numberphile - By their definitions: the first one is a channel on videos all about computers and computer stuff and the second one is videos about numbers - it\u0026rsquo;s that simple.  Tensorflow tutorials - Hvass Laboratories.  deeplearning.ai - An YouTube channel run by Andrew Ng.  M\u0026amp;MoCS - Centro Internazionale di Ricerca per la “Matematica \u0026amp; Meccanica dei Sistemi Complessi”, Università dell\u0026rsquo;Aquila.  Institute for Advanced Studies  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f99bddc40a85a0a14845e951ff056799","permalink":"https://w00zie.github.io/author/courses/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/courses/","section":"authors","summary":"English Information Theory   Information Theory, Pattern Recognition and Neural Networks - David McKay, Cambridge (also book freely available online, thank you David!).  Information Theory I \u0026amp; Information Theory II - Jeffrey A.","tags":null,"title":"Courses","type":"authors"},{"authors":["wabri"],"categories":null,"content":"Gabriele is a friend of mine.\nHe is an active developer interested in different fields of Computer Science, make sure to check out his blog!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5231dca2ff835897771ddc2d48aefd90","permalink":"https://w00zie.github.io/author/gabriele-puliti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/gabriele-puliti/","section":"authors","summary":"Gabriele is a friend of mine.\nHe is an active developer interested in different fields of Computer Science, make sure to check out his blog!","tags":null,"title":"Gabriele Puliti","type":"authors"},{"authors":["various"],"categories":null,"content":"Podcasts  Artificial Intelligence Podcast - A podcast about AI \u0026amp; beyond with a lot of interesting interviewees. Sometimes the interviewer gets a little too much philosophical (not in a good way) but generally speaking is good content, imo.\n Il podcast di Alessandro Barbero: Lezioni e Conferenze di Storia - History lessons by one of the most known historians in Italy (in Italian).\nRandom stuff  Papers With Code - A site where you can dig among papers (in the field of ML) with open source code officialy released.\n Haskell for Imperative Programmers - A playlist for starting out Haskell programming. I\u0026rsquo;ve watched a few episodes and I liked the teaching style.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8fd2cf8a02668b63551e652b80c1bbc2","permalink":"https://w00zie.github.io/author/various/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/various/","section":"authors","summary":"Podcasts  Artificial Intelligence Podcast - A podcast about AI \u0026amp; beyond with a lot of interesting interviewees. Sometimes the interviewer gets a little too much philosophical (not in a good way) but generally speaking is good content, imo.","tags":null,"title":"Various","type":"authors"},{"authors":["Giovanni Bindi"],"categories":[],"content":"Table of Contents  Introduction The data Theory  WAE-GAN WAE-MMD   Experiments details  Architechtures Prior Latent space dimensionality Adversarial training Cost function Hyperparameters   Results Survey    Introduction This is a project I started working on for my Machine Learning class. The aim, as you can read from the title, is being able to generate music that is, at least, not unpleasant.\nIn order to do so I investigated the use of Wasserstein Autoencoders (WAE), a generative model firstly proposed by Tolstikhin et al. 1, which employs a regularizer derived from the theory of optimal transport.\nThe data The data I used was published by a group of researchers from the Academia Sinica (Taiwan), as part of a paper2 where they jointly proposed a novel GAN (MuseGAN) and released the dataset where the model was trained on. They open-sourced almost everything so you can access to their code and data.\nThis dataset contains 174154 unique 4/4 multi-track music pieces of 4 bars, coming from different rock music compositions. Every element of this dataset is represented as a multi-track (binary-valued) pianoroll. For those of you who are not familiar with DAWs or audio processing, a single-track pianoroll is basically a matrix whose entries represent the behaviour of the notes played by the single instrument. The number of columns in this matrix is the number of time-steps used to quantize the composition while the number of rows represents the octave extension, i.e. the number of possible notes that the instrument can execute.\n Example of a pianoroll   In this dataset a single bar is quantized into 48 time-steps (the minimum duration of a note is then 1/48th) and can span between 84 notes from C1 to B7. Every element of the dataset contains 4 bars of 5 instruments, resulting in a binary-valued (note on/off - velocity information is discarded) tensor of shape (4*48, 84, 5) = (192, 84, 5). The tempo was set to 100 bpm and the authors transposed every element to the same tonality. The instruments involved are Drums, Bass, Guitar, Piano and Strings.\n One element of the dataset: 4 bars of 5 instruments (best with the white theme)   The pianoroll is one of the most commonly used representations, although it has some limitations. An important one, compared to the MIDI representation, is that there is no note-off information. As a result, there is no way to distinguish between a long note and repeated short notes. In order to partially solve this issue the authors did as following\n Note that during the conversion from MIDI files to pianorolls, an additional minimal-length (of one time step) pause is added between two consecutive (without a pause) notes of the same pitch to distinguish them from one single note.\n A few samples from the dataset3 (lower your audio output):\n        Theory Disclaimer: a lot of this theory is taken from the original paper1, I did not re-write it from scratch!\nWAEs are a class of Autoencoders derived by theory of optimal transport. Given a collection of (unlabelled) data points $S_X$, the ultimate goal of the task is to tune a model capable of generating sets of synthetic points $S_G$ which look similar to $S_X$. There are various ways of defining the notion of similarity between two sets of data points: the most common approach assumes that both $S_X$ and $S_G$ are sampled independently from two probability distributions $P_X$ and $P_G$ respectively, and employ some of the known divergence measures for distributions.\nTwo major approaches currently dominate this field. Variational Auto-Encoders (VAEs) minimize the Kullback-Leibler (KL) divergence $D_{KL}(P_X, P_G)$, which is equivalent to maximizing the marginal log-likelihood of the model $P_G$. Generative Adversarial Networks (GANs) employ an elegant framework, commonly referred to as adversarial training, which is suitable for many different divergence measures, including (but not limited to) $f-$divergences, 1-Wasserstein distance, and Maximum Mean Discrepancy (MMD).\nSimilarly to VAEs, WAEs describe a particular way to train probabilistic latent variable models (LVMs) $P_G$. LVMs act by first sampling a code (feature) vector $Z$ from a prior distribution $P_Z$ defined over the latent space $\\mathcal{Z}$ and then mapping it to a random input point $X \\in \\mathcal{X}$ using a conditional distribution $P_G(X|Z)$ also known as the decoder. This results is a density of the form\n$$ p_G(x) = \\int_{\\mathcal{Z}} p_G(x|z)p(z)dz $$\nassuming all involved densities are properly defined.\nInstead of minimizing the KL divergence between the LVM $P_G$ and the unknown data distribution $P_X$ as done by VAEs, WAEs aim at minimizing any optimal transport distance between them. Kantorovich\u0026rsquo;s formulation of the optimal transport problem (OT) is given by\n$$ W_c(P_X, P_G) = \\inf_{\\Gamma \\in \\mathcal{P}(X \\sim P_X, Y \\sim P_G)} \\mathbb{E}_{(X,Y) \\sim \\Gamma} [c(X,Y)] $$\nwhere $c:\\mathcal{X} \\times \\mathcal{Y} \\to \\mathbb{R}^+$ is any measurable cost function and $\\mathcal{P}(X \\sim P_X, Y \\sim P_G)$ is the set of all joint distributions of $(X, Y)$ with marginals $P_X$ and $P_G$ respectively. When $(\\mathcal{X}, d)$ is a metric space (with distance $d$) and $c(x,y) = d^p(x,y)$ for $p \\geq 1$ then $W_p$ (the $p$-th root of $W_c$) is known as the $p$-Wasserstein distance. When $p=1$ we then have the $1$-Wasserstein distance and this duality holds (Kantorovich-Rubinstein duality)\n$$ W_1(P_X, P_G) = \\sup_{f \\in \\mathcal{F}_L} \\mathbb{E}_{X \\sim P_X} [f(X)] - \\mathbb{E}_{Y \\sim P_G}[f(Y)] $$\nwhere $\\mathcal{F}_L$ is the set of all bounded 1-Lipschitz functions on $(\\mathcal{X}, d)$. The authors state (and prove) that for deterministic decoders, i.e. $P_G(X|Z=z) = \\delta_{G(z)}$ deterministically mapping latent codes $z \\in \\mathcal{Z}$ to the input space for a given map $G: \\mathcal{Z} \\to \\mathcal{X}$, there is a simpler formulation for the OT problem: instead of finding a coupling $\\Gamma$ between two random variables in $\\mathcal{X}$ it is sufficient to find a conditional distribution $Q(Z|X)$ such that its $Z$ marginal $Q(Z) = \\mathbb{E}_{X \\sim P_X} [Q(Z|X)]$ is identical to the prior distribution $P_Z$. This resulted in a theorem where the OT problem was re-formulated (under these assumptions) as\n$$ W_c(P_X, P_G) = \\inf_{Q:Q_Z = P_Z} \\mathbb{E}_{P_X}\\big [ \\mathbb{E}_{Q(Z|X)}[c(X, G(Z))] \\big ] $$\nIn order to implement a numerical solution to this problem the authors relax the constraint on $Q_Z$ by adding a penalty term to the objective, which, in the end, takes the following form\n$$ D_{WAE}(P_X, P_G) = \\inf_{Q(Z|X) \\in \\mathcal{Q}} \\mathbb{E}_{P_X}\\big [ \\mathbb{E}_{Q(Z|X)}[c(X, G(Z))] \\big ] + \\lambda \\mathcal{D}_Z(Q_Z, P_Z) $$\nwhere $\\mathcal{Q}$ is any nonparametric set of probabilistic encoders, $\\mathcal{D}_Z$ is an arbitrary divergence between $Q_Z$ and $P_Z$ , and $\\lambda \u0026gt; 0$ is a hyperparameter. The authors propose two implementations of the WAE model, based on two different divergences.\nWAE-GAN The WAE-GAN model employs the Jensen-Shannon divergence, i.e.\n$$ \\mathcal{D}_Z(Q_Z, P_Z) = \\mathcal{D}_{JS}(Q_Z, P_Z) = \\frac{1}{2} D_{KL}(P_Z, M) + \\frac{1}{2} D_{KL}(Q_Z, M) $$\nwhere $M = \\frac{1}{2}(P_Z + Q_Z)$ and uses the adversarial training to estimate it. The authors introduce an adversary (discriminator) in the latent space $\\mathcal{Z}$ trying to separate \u0026ldquo;true\u0026rdquo; points sampled from $P_Z$ and \u0026ldquo;fake\u0026rdquo; ones sampled from $Q_Z$.\nWAE-MMD For a positive-definite reproducing kernel $k : \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ the following expression is called the Maximum Mean Discrepancy (MMD):\n$$ \\text{MMD}_k(P_Z, Q_Z) = \\Big | \\Big | \\int_\\mathcal{Z} k(z, \\cdot) dP_Z(z) - \\int_\\mathcal{Z} k(z, \\cdot) dQ_Z(z) \\Big | \\Big |_{\\mathcal{H}_k} $$\nwhere $\\mathcal{H}_k$ is the RKHS of real-valued functions mapping $\\mathcal{Z}$ to $\\mathcal{R}$. If $k$ is characteristic then MMD$_k$ defines a metric and can be used as a divergence measure. The authors propose to use an unbiased estimate of the MMD that is the sum of two $U$-statistics and a sample average. Given i.i.d. samples $\\mathbf{P} = { z_1, \\dots, z_n }$ from $P_Z$ and i.i.d. samples $\\mathbf{Q} = {\\tilde{z}_1, \\dots, \\tilde{z}_n}$ from $Q(Z|z_i)$ , the unbiased estimate4 is\n$$ \\widehat{MMD}^2_U \\big [\\mathcal{H}_k, \\mathbf{P}, \\mathbf{Q} \\big ] = \\frac{1}{n(n-1)} \\sum_{i=1}^n \\sum_{j \\neq i} k(z_i, z_j) + \\sum_{i=1}^n \\sum_{j \\neq i} k(\\tilde{z}_i, \\tilde{z}_j) - \\frac{2}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n k(z_i, \\tilde{z}_j) $$\nExperiments details Architechtures Encoder $Q_\\phi$, decoder $G_{\\theta}$ and discriminator $D_{\\gamma}$ are parametrized by deep neural nets. Given the structure of the data, binary-valued tensors of shape (H,W,5) (where 5, the number of instruments, plays naturally the role of the number of channels), both encoder and decoder are CNNs, while the discriminator (which acts in the latent space $\\mathcal{Z}$) is a \u0026ldquo;vanilla\u0026rdquo; NN. You can check out the various models I\u0026rsquo;ve coded here, they are almost all equals except for the strides of the convolutions.\nThe main architecture I used was inspired by DCGAN 5. In the following picture I\u0026rsquo;m showing the encoder that, starting from the (192, 84, 5) input tensor, applies 4 convolutions with $[6,6]$ filters, followed by Batch Normalizations and ReLUs. The last layer employs a reshape (not shown in the picture) followed by a fully-connected layer that maps the previous inputs into a vector of size $dim(\\mathcal{Z}) = d_{\\mathcal{Z}}$ (in this case I went with $d_{\\mathcal{Z}} = 8$).\n My encoder (made with NN-SVG)   The decoder acts simmetrically wrt the encoder, using 2D transposed convolutions instead of convolutions. The discriminator network maps a vector from $\\mathcal{Z}$ to $[0,1]$ through 6 dense layers of 512 neurons followed by ReLUs.\nPrior I\u0026rsquo;ve used a Gaussian prior $P_Z = \\mathcal{N} \\big ( 0, \\sigma_z^2 \\cdot I_{d_z} \\big )$ where the variance $\\sigma_z^2$ is an hyperparameter.\nLatent space dimensionality The authors studied the choice of the dimensionality of the latent space in a follow-up paper6. They investigated the relationship between the latent space dimension $d_{\\mathcal{Z}}$ and the *intrinsic dimensionality* of the data distribution, $d_{\\mathcal{I}}$. This last quantity is, informally, *the minimum number of parameters required to continuously parametrise the data manifold*. If this quantity is greater than $d_{\\mathcal{Z}}$ then the encoder will map the data distribution $P_X$ to $Q_Z$ supported on a latent manifold of dimension at most $d_{\\mathcal{I}}$ while the divergence regularizer $\\lambda \\mathcal{D}_Z (Q_Z, P_Z)$ will encourage the encoder to fill the latent space similarly to the prior $P_Z$ as much as possible. This can potentially lead to the absence of any learning capability.\nAdversarial training I\u0026rsquo;ve decided to use (as the authors suggest) the non-saturating GAN loss 7: this resulted in a discriminator $D_\\gamma$ trained by the maximization of\n$$ \\frac{\\lambda}{n} \\sum_{i=1}^n \\log D_\\gamma(z_i) + \\log D_\\gamma(1-\\tilde{z}_i) $$\nand in an autoencoder ($Q_\\phi, G_\\theta$) trained by the minimization of\n$$ \\frac{1}{n}\\sum_{i=1}^n c(z_i, G_\\theta(\\tilde{z}_i)) - \\lambda D_\\gamma \\log (\\tilde{z}_i) $$\nAs the authors point out this procedure falls into the min-max problem which makes GAN unstable. This training strategy can then possibly yield instabilities in the training phase but the authors claim that by executing an adversarial training in a \u0026ldquo;simpler\u0026rdquo; space (wrt the pixel space where GANs are usually trained on), the whole process could benefit.\nCost function I\u0026rsquo;ve used $c(x, y) = || x - y ||_2^2$ for both implementations (WAE-GAN and WAE-MMD).\nHyperparameters Since there is no objective measure of performace for this kind of task, running an hyperparameter optimization procedure is hard. I\u0026rsquo;ve adjusted the parameters manually and so the results I obtained might not be the best that these model can produce.\nResults As mentioned in the data section, the input samples are binary-valued tensors of shape (192, 84, 5). The decoder, although, maps a latent code into $[0, 1]^{192 \\times 84 \\times 5}$ through the action of the sigmoid function. These output tensors $\\hat{x}$ must then be binarized, and I\u0026rsquo;ve used two strategies to do so:\n  Hard thresholding (HT): The resulting tensor entry $\\hat{x}_{ijk}$ needs to be greater than a fixed threshold (0.5) in order to be mapped to 1. $$\\hat{x}_{ijk}^{HT} = \\begin{cases} 1 \u0026amp; \\text{if } \\hspace{10pt} x_{ijk} \u0026gt; 0.5 \\\\\n0 \u0026amp; \\text {o.w }\\end{cases}$$\n  Bernoulli sampling (BS): The resulting tensor entry $\\hat{x}_{ijk}$ needs to be greater than a sample from the uniform distribution over scalars between 0 and 1. $$\\hat{x}_{ijk}^{BS} = \\begin{cases} 1 \u0026amp; \\text{if } \\hspace{10pt} x_{ijk} \u0026gt; b \\sim \\mathcal{U}[0,1] \\\\\n0 \u0026amp; \\text {o.w }\\end{cases}$$\n  I\u0026rsquo;m presenting samples binarized with the HT method.\n For a \u0026ldquo;fair\u0026rdquo; comparison the following results have been obtained with the same architecture and hyperparameters. Both WAE-GAN and WAE-MMD models have been trained with a latent dimension of $d_{\\mathcal{Z}} = 16$. The number of filters in the four convolutions (see the picture above) are [16, 32, 64, 128] for the encoder and [128, 64, 32, 16] for the decoder. Both sequences of convolution used a fixed filter size of 6x6. I\u0026rsquo;ve trained both models for 300 epochs with Adam. The autoencoder\u0026rsquo;s optimizer started with a learning rate of $10^{-3}$ while the discriminator optimizer (in WAE-GAN) started with a learning rate of $10^{-4}$.\nThe following are some of the \u0026ldquo;best\u0026rdquo; samples (wrt my personal taste) that I picked: please make sure to lower the level of your audio output3 before playing! I suggest listening with headphones.\n   WAE-GAN  WAE-MMD                                                                                       The next samples, instead, are randomly picked. Some of them totally lack the basic musical structures.\n   WAE-GAN  WAE-MMD                                                       The samples I was able to generete suffer the problem of over-fragmentation. The notes generated tend to be fragmented, sometimes yielding chaotic and unpleasant results. This problem has also been recognized in 2 and the authors of MuseGAN proposed a new version of the model where their generator was equipped with binary neurons8, making it able to map the input noise directly into binary-valued output tensors. This partially solved the issue.\nThe following samples come from a different set of WAE-GAN models I\u0026rsquo;ve trained (doubling the number of filters for each convolutional layer, using $d_{\\mathcal{Z}} = 128$ and more epochs), I\u0026rsquo;ve cherry picked them hence they are not the \u0026ldquo;average\u0026rdquo; case, but the drums seem to be more predominant wrt the \u0026ldquo;simpler\u0026rdquo; models proposed above.\n   WAE-GANs                                                        Survey TODO\nReferences    Wasserstein Auto-Encoders \u0026#x21a9;\u0026#xfe0e;\n  MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment \u0026#x21a9;\u0026#xfe0e;\n I\u0026rsquo;ve done the MIDI to MP3 conversions with the help of VLC and its integrated synthesizer FluidSynth. I\u0026rsquo;ve done no post-processing (eq, fx, mixing, mastering) so what you hear comes straight out-of-the box. The sounds produced are unpleasant: please think of them as part of an experiment and not music meant to be played. \u0026#x21a9;\u0026#xfe0e;\n  Kernel Mean Embedding of Distributions: A Review and Beyond page 52 \u0026#x21a9;\u0026#xfe0e;\n  Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks \u0026#x21a9;\u0026#xfe0e;\n  On the Latent Space of Wasserstein Auto-Encoders \u0026#x21a9;\u0026#xfe0e;\n  Are GANs Created Equal? A Large-Scale Study \u0026#x21a9;\u0026#xfe0e;\n  Convolutional Generative Adversarial Networks with Binary Neurons for Polyphonic Music Generation \u0026#x21a9;\u0026#xfe0e;\n   ","date":1596293173,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596293173,"objectID":"4b0abaabd8b95711a47845b486d43498","permalink":"https://w00zie.github.io/post/wae/","publishdate":"2020-08-01T16:46:13+02:00","relpermalink":"/post/wae/","section":"post","summary":"Table of Contents  Introduction The data Theory  WAE-GAN WAE-MMD   Experiments details  Architechtures Prior Latent space dimensionality Adversarial training Cost function Hyperparameters   Results Survey    Introduction This is a project I started working on for my Machine Learning class.","tags":["generative-models","autoencoders","wasserstein"],"title":"Music generation with Wasserstein Autoencoders","type":"post"}]