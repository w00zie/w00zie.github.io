[{"authors":["admin"],"categories":null,"content":"This is my blog. I might write here some random stuff that interests me during my studies in Information Engineering. I\u0026rsquo;m not much of a coder but I might also embed some (I hope not-too-much-poorly-written) python code along my posts. Hope you will find my content (sort of) interesting.\nCiao!\n","date":1607785753,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607785753,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://w00zie.github.io/author/giovanni-bindi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/giovanni-bindi/","section":"authors","summary":"This is my blog. I might write here some random stuff that interests me during my studies in Information Engineering. I\u0026rsquo;m not much of a coder but I might also embed some (I hope not-too-much-poorly-written) python code along my posts.","tags":null,"title":"Giovanni Bindi","type":"authors"},{"authors":["books"],"categories":null,"content":"Books   Deep Learning - Ian Goodfellow, Yoshua Bengio, Aaron Courville, MIT press, 2016.  Information Theory, Pattern Recognition and Neural Networks - David McKay, Cambridge University Press, 2003.  Reinforcement Learning, an Introduction - Richard S. Sutton, Andrew G. Barto, MIT press, 2015.  Neural Networks and Deep Learning - Michael A. Nielsen1, 2015.  An Introduction to Statistical Learning - Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani, Springer, 2017.  The Elements of Statistical Learning - Trevor Hastie, Robert Tibshirani, Jerome Friedman, Springer, 2009.  Bayesian Reasoning and Machine Learning - David Barber, Cambridge, 2012.  Foundations of Machine Learning - Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar, MIT Press, 2018.  Python Data Science Handbook - Jake VanderPlas, O\u0026rsquo;Reilly, 2016.  Applied Optimum Signal Processing - Sophocles J. Orfanidis, 2018.  Physics   From Classical to Quantum ShannonTheory - Mark M. Wilde, Cambridge University Press, 2019.  Quantum Country - A. Matuschak, M. Nielsen, 2020.   Articles These are papers that I liked. I think they are good examples of well-written scientific papers.\n  Computational Complexity for Physicists - Stephan Mertens, 2002.  The Discipline of Machine Learning - Tom M. Mitchell, 2006.  The Unreasonable Effectiveness of Data - Alon Halevy, Peter Norvig, and Fernando Pereira, 2009.  Automatic Differentiation in Machine Learning: A Survey - Baydin et al., 2018.  Stochastic Gradient Descent Tricks - Léon Bottou, 2012.  Memory Networks - Weston et al., 2016.  Understanding Dropout - Pierre Baldi, Peter Sadowski, 2013.  Auto-Encoding Variational Bayes - Diederik P. Kingma, Max Welling, 2014.  Wasserstein Auto-Encoders - Ilya Tolstikhin \u0026amp; al., 2018.  Monte Carlo Gradient Estimation in Machine Learning - Shakir Mohamed \u0026amp; al., 2019.     Michael Nielsen is also a co-author of a beautiful book: Quantum Computation \u0026amp; Quantum Information. \u0026#x21a9;\u0026#xfe0e;\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6c0c4357bafb20bffeea0f4b3fc20200","permalink":"https://w00zie.github.io/author/articles-books/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/articles-books/","section":"authors","summary":"Books   Deep Learning - Ian Goodfellow, Yoshua Bengio, Aaron Courville, MIT press, 2016.  Information Theory, Pattern Recognition and Neural Networks - David McKay, Cambridge University Press, 2003.  Reinforcement Learning, an Introduction - Richard S.","tags":null,"title":"Articles \u0026 Books","type":"authors"},{"authors":["courses"],"categories":null,"content":"English Information Theory   Information Theory, Pattern Recognition and Neural Networks - David McKay, Cambridge (also book freely available online, thank you David!).  Information Theory I \u0026amp; Information Theory II - Jeffrey A. Bilmes, University of Washington.  Reinforcement Learning   Reinforcement Learning - David Silver, UCL/DeepMind.  Advanced Deep Learning \u0026amp; Reinforcement Learning - Various, UCL.  Deep Reinforcement Learning - Various, Berkeley.  Learning Theory   Statistical Learning Theory and Applications - Tomaso Poggio, MIT.  Statistical Machine Learning - Ulrike von Luxburg, University of Tübingen.  Machine Learning   Probabilistic Machine Learning - Philipp Henning, University of Tübingen.  Advanced Machine Learning - Various, ETH Zürich.  Deep Learning   Deep Learning - Nando de Freitas, Oxford (check his channel for other courses!).  Geometric Deep Learning - Michael Bronstein, TUM.  Deep Learning - Various, UCL/DeepMind.  Physics   General Relativity - Leonard Susskind, Stanford.  Statistical Mechanics - Leonard Susskind, Stanford.  Statistical Mechanics - John Preskill, Caltech.  Quantum Mechanics - Allan Adams, MIT.  Quantum Computation - John Preskill, Caltech.  Nonlinear Dynamics and Chaos - Steven Strogatz, Cornell University.  Lectures on Geometrical Anatomy of Theoretical Physics - Frederic Schuller, FAUERN.  Italian   Fisica dei Sistemi Complessi e Teoria dell\u0026rsquo;Informazione - Università di Firenze.  Relatività Generale - Random Physics.  Geometria Differenziale - Università di Padova.  Tutorials \u0026amp; Channels   3Blue1Brown - One of the best scientific YouTube channels out there. Both contents and visualization techniques are stunningly good.  Computerphile and Numberphile - By their definitions: the first one is a channel on videos all about computers and computer stuff and the second one is videos about numbers - it\u0026rsquo;s that simple.  Tensorflow tutorials - Hvass Laboratories.  deeplearning.ai - An YouTube channel run by Andrew Ng.  M\u0026amp;MoCS - Centro Internazionale di Ricerca per la “Matematica \u0026amp; Meccanica dei Sistemi Complessi”, Università dell\u0026rsquo;Aquila.  Institute for Advanced Studies  Simons Institute for the Theory of Computing  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f99bddc40a85a0a14845e951ff056799","permalink":"https://w00zie.github.io/author/courses/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/courses/","section":"authors","summary":"English Information Theory   Information Theory, Pattern Recognition and Neural Networks - David McKay, Cambridge (also book freely available online, thank you David!).  Information Theory I \u0026amp; Information Theory II - Jeffrey A.","tags":null,"title":"Courses","type":"authors"},{"authors":["wabri"],"categories":null,"content":"Gabriele is a friend of mine.\nHe is an active developer interested in different fields of Computer Science, make sure to check out his blog!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5231dca2ff835897771ddc2d48aefd90","permalink":"https://w00zie.github.io/author/gabriele-puliti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/gabriele-puliti/","section":"authors","summary":"Gabriele is a friend of mine.\nHe is an active developer interested in different fields of Computer Science, make sure to check out his blog!","tags":null,"title":"Gabriele Puliti","type":"authors"},{"authors":["various"],"categories":null,"content":"Podcasts  Artificial Intelligence Podcast - A podcast about AI \u0026amp; beyond with a lot of interesting interviewees. Sometimes the interviewer gets a little too much philosophical (not in a good way) but generally speaking is good content, imo.\n Il podcast di Alessandro Barbero: Lezioni e Conferenze di Storia - History lessons by one of the most known historians in Italy (in Italian).\n Limes Online - Rivista Italiana di Geopolitica - Italian geopolitical monitor (in Italian).\nRandom stuff  Papers With Code - A site where you can dig among papers (in the field of ML) with open source code officialy released.\n Haskell for Imperative Programmers - A playlist for starting out Haskell programming. I\u0026rsquo;ve watched a few episodes and I liked the teaching style.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8fd2cf8a02668b63551e652b80c1bbc2","permalink":"https://w00zie.github.io/author/various/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/various/","section":"authors","summary":"Podcasts  Artificial Intelligence Podcast - A podcast about AI \u0026amp; beyond with a lot of interesting interviewees. Sometimes the interviewer gets a little too much philosophical (not in a good way) but generally speaking is good content, imo.","tags":null,"title":"Various","type":"authors"},{"authors":["Giovanni Bindi"],"categories":[],"content":"Table of Contents  Introduction Description of the game Deterministic strategy Quantum strategy Winning at the game without designing a winning strategy    Introduction In this post we\u0026rsquo;ll be going through a simple experiment I\u0026rsquo;ve set up in order to violate the Bell inequality.\nThe goal is to \u0026ldquo;win\u0026rdquo; (one of the formulations of) the CHSH game: this \u0026ldquo;game\u0026rdquo; was proposed by Clauser, Horn, Shimony and Holt in their famous 1969 PRL paper 1 as a generalizatio of 1964 Bell\u0026rsquo;s Theorem. The formulation we\u0026rsquo;ll be dealing with is the one proposed by Mark Wilde in his book 2 and by Ronald de Wolf in his lecture notes 3.\nYou can find the code here.\nDescription of the game Here\u0026rsquo;s the game description by Mark Wilde:\n This game is a 2-player game where the two players, Alice and Bob, are spatially separated from each other from the time that the game starts until it is over. The game begins with a referee selecting two bits $x$ and $y$ uniformly at random. The referee then sends $x$ to Alice and $y$ to Bob. Alice and Bob are not allowed to communicate with each other in any way at this point. Alice sends back to the referee a bit $a$, and Bob sends back a bit $b$. Since they are spatially separated, Alice’s response bit $a$ cannot depend on Bob’s input bit $y$, and similarly, Bob’s response bit $b$ cannot depend on Alice’s input bit $x$. After receiving the response bits $a$ and $b$, the referee determines if the AND of $x$ and $y$ is equal to the exclusive OR of $a$ and $b$. If so, then Alice and Bob win the game.\n Given that, the winning conditions is\n$$ x \\land y = a \\oplus b $$\nDeterministic strategy A deterministic strategy would have Alice select a bit $a_x$ conditioned on the bit $x$ that she receives, and similarly, Bob would select a bit $b_y$ conditioned on $y$. The following table presents the winning conditions for the four different values of $x$ and $y$ with this deterministic strategy:\n    $x$ $y$ $x \\land y$ $a_x \\oplus b_y$     0 0 0 $a_0 \\oplus b_0$   0 1 0 $a_0 \\oplus b_1$   1 0 0 $a_1 \\oplus b_0$   1 1 1 $a_1 \\oplus b_1$     At most, only three out of four of them can be satisfied, so that the maximal winning probability with a classical deterministic strategy is at most $3 / 4$. We can then see that a strategy for them to achieve this upper bound is for Alice and Bob always to return $a = 0$ and $b = 0$ no matter the values of $x$ and $y$.\nQuantum strategy Now consider the same problem but where Alice and Bob are supplied with a shared 2-qubit system initialized to the entangled state\n$$ \\frac{1}{\\sqrt{2}} (|00\\rangle - |11\\rangle) $$\nThe strategy is the following: if $x = 0$ then Alice applies $R(-\\pi/16)$ to her qubit and if $x = 1$ she applies $R(3\\pi/16)$, where $R(\\theta)$ denotes a rotation of $\\theta$:\n$$ R(\\theta) = \\begin{bmatrix} cos(\\theta) \u0026amp; -sin(\\theta) \\\\ sin(\\theta) \u0026amp; \\cos(\\theta) \\end{bmatrix} $$\nBob’s procedure is the same, depending on his input bit $y$.\n If Alice now rotates her qubit by $\\theta_A$ and Bob rotates his qubit by $\\theta_B$ then the state becomes:\n$$ \\frac{1}{\\sqrt{2}} (cos(\\theta_A + \\theta_B))(|00\\rangle - |11\\rangle) + \\sin(\\theta_A + \\theta_B)(|01\\rangle + |10\\rangle) $$\nMeasuring in the computational basis we see that, after the measurements, the probability that $a \\oplus b = 0$ is $cos(\\theta_A + \\theta_B)^2$. If $x \\land y = 0$ then $\\theta_A + \\theta_B = \\pm \\pi / 8$, else if $x \\land y = 1$ then $\\theta_A + \\theta_B = 3 \\pi / 8$.\nThe winning condition is then satisfied with probability $cos(\\pi / 8)^2 \\approx 0.853$, for all four input possibilities, showing that quantum entanglement allows Alice and Bob to win the game with a probability that\u0026rsquo;s higher than what the best classical strategy can achieve. Tsirelson 5 showed that $cos(\\pi / 8)^2$ is the best that quantum strategies can do for CHSH, even if they are allowed to use much more entanglement than one Bell state.\nWinning at the game without designing a winning strategy Quantum computing (QC) is one of the currently \u0026ldquo;hot\u0026rdquo; research areas, gaining substantious advantages year after year. Many industries and Universities are developing interesting tools for experimenting with the power of QC, such as IBM\u0026rsquo;s qiskit and Google\u0026rsquo;s cirq.\nThese two examples are Python-based frameworks for programming quantum computers and, in this experiment, I used Cirq. My idea is simple: I wanted to construct a parameterized quantum circuit (PQC) that, along with an optimization strategy, could find the best parameters for winning at the CHSH game.\nThe optimization strategy I employed is Bayesian Optimization, which is a strategy for global optimization of black-box functions. Bayesian optimization is particularly advantageous for problems where the function we want to maximize is difficult to evaluate, is a black box with some known structure, relies upon less than 20 dimensions, and where derivatives are not evaluated 4.\nTurns out this is exactly my setting, since I want to maximize the probability of winning the CHSH game through a series of simulations of a few-parameterized-gates quantum circuit. The circuit I designed is the following\n  Parameterized Quantum Circuit   where the four qubits are initialized to $|0\\rangle$. The transformations which operate on these qubits are Hadamard gates (denoted by $H$), rotations $R_X$ and $R_Z$ (respectively by the $X$ and $Z$ axis) and CNOTs. Rotations and controlled-not gates are parameterized by the $x*$ you see in the pictures: I\u0026rsquo;ve constrained the rotational parameters into $[0, 2\\pi]$ and the exponents of the CNOTs to $[-2, 2]$. The last *moment* of the circuit is the one dedicated to Measurements, which happen in the computational basis.\nFor every set of fixed parameters a simulation of $N=5000$ runs is operated and, in the end, a winning statistic is calculated as $N^w / N \\in [0, 1]$, where $N^w$ is the number of times that the winning condition is satisfied. This function is then maximized through Bayesian Optimization resulting in a set of parameters which are able to win the game:\n  \u0026lsquo;\u0026lsquo;Convergence\u0026rsquo;\u0026rsquo; of BO   ","date":1607785753,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607785753,"objectID":"3ab2144a2be41b9d0be2f00a2dbaec21","permalink":"https://w00zie.github.io/post/bell/","publishdate":"2020-12-12T17:09:13+02:00","relpermalink":"/post/bell/","section":"post","summary":"Table of Contents  Introduction Description of the game Deterministic strategy Quantum strategy Winning at the game without designing a winning strategy    Introduction In this post we\u0026rsquo;ll be going through a simple experiment I\u0026rsquo;ve set up in order to violate the Bell inequality.","tags":["quantum","cirq","bayesian","optimization","bell"],"title":"Winning the CHSH game with Cirq and Bayesian Optimization","type":"post"},{"authors":["Giovanni Bindi"],"categories":[],"content":"Table of Contents  Introduction The data Theory  WAE-GAN WAE-MMD   Experiments details  Architechtures Prior Latent space dimensionality Adversarial training Cost function Hyperparameters   Results  Random Generation Reconstruction      Introduction This is a project I started working on for my Machine Learning class. The aim, as you can read from the title, is being able to generate music that is, at least, not unpleasant.\nIn order to do so I investigated the use of Wasserstein Autoencoders (WAE), a generative model firstly proposed by Tolstikhin et al. 1, which employs a regularizer derived from the theory of optimal transport.\nThe data The data I used was published by a group of researchers from the Academia Sinica (Taiwan), as part of a paper2 where they jointly proposed a novel GAN (MuseGAN) and released the dataset where the model was trained on. They open-sourced almost everything so you can access to their code and data.\nThis dataset contains 174154 unique 4/4 multi-track music pieces of 4 bars, coming from different rock music compositions. Every element of this dataset is represented as a multi-track (binary-valued) pianoroll. For those of you who are not familiar with DAWs or audio processing, a single-track pianoroll is basically a matrix whose entries represent the behaviour of the notes played by the single instrument. The number of columns in this matrix is the number of time-steps used to quantize the composition while the number of rows represents the octave extension, i.e. the number of possible notes that the instrument can execute.\n Example of a pianoroll   In this dataset a single bar is quantized into 48 time-steps (the minimum duration of a note is then 1/48th) and can span between 84 notes from C1 to B7. Every element of the dataset contains 4 bars of 5 instruments, resulting in a binary-valued (note on/off - velocity information is discarded) tensor of shape (4*48, 84, 5) = (192, 84, 5). The tempo was set to 100 bpm and the authors transposed every element to the same tonality. The instruments involved are Drums, Bass, Guitar, Piano and Strings.\n One element of the dataset: 4 bars of 5 instruments (best with the white theme)   The pianoroll is one of the most commonly used representations, although it has some limitations. An important one, compared to the MIDI representation, is that there is no note-off information. As a result, there is no way to distinguish between a long note and repeated short notes. In order to partially solve this issue the authors did as following\n Note that during the conversion from MIDI files to pianorolls, an additional minimal-length (of one time step) pause is added between two consecutive (without a pause) notes of the same pitch to distinguish them from one single note.\n A few samples from the dataset3 (lower your audio output):\n        Theory Disclaimer: a lot of this theory is taken from the original paper1, I did not re-write it from scratch!\nWAEs are a class of Autoencoders derived by theory of optimal transport. Given a collection of (unlabelled) data points $S_X$, the ultimate goal of the task is to tune a model capable of generating sets of synthetic points $S_G$ which look similar to $S_X$. There are various ways of defining the notion of similarity between two sets of data points: the most common approach assumes that both $S_X$ and $S_G$ are sampled independently from two probability distributions $P_X$ and $P_G$ respectively, and employ some of the known divergence measures for distributions.\nTwo major approaches currently dominate this field. Variational Auto-Encoders (VAEs) minimize the Kullback-Leibler (KL) divergence $D_{KL}(P_X, P_G)$, which is equivalent to maximizing the marginal log-likelihood of the model $P_G$. Generative Adversarial Networks (GANs) employ an elegant framework, commonly referred to as adversarial training, which is suitable for many different divergence measures, including (but not limited to) $f-$divergences, 1-Wasserstein distance, and Maximum Mean Discrepancy (MMD).\nSimilarly to VAEs, WAEs describe a particular way to train probabilistic latent variable models (LVMs) $P_G$. LVMs act by first sampling a code (feature) vector $Z$ from a prior distribution $P_Z$ defined over the latent space $\\mathcal{Z}$ and then mapping it to a random input point $X \\in \\mathcal{X}$ using a conditional distribution $P_G(X|Z)$ also known as the decoder. This results is a density of the form\n$$ p_G(x) = \\int_{\\mathcal{Z}} p_G(x|z)p(z)dz $$\nassuming all involved densities are properly defined.\nInstead of minimizing the KL divergence between the LVM $P_G$ and the unknown data distribution $P_X$ as done by VAEs, WAEs aim at minimizing any optimal transport distance between them. Kantorovich\u0026rsquo;s formulation of the optimal transport problem (OT) is given by\n$$ W_c(P_X, P_G) = \\inf_{\\Gamma \\in \\mathcal{P}(X \\sim P_X, Y \\sim P_G)} \\mathbb{E}_{(X,Y) \\sim \\Gamma} [c(X,Y)] $$\nwhere $c:\\mathcal{X} \\times \\mathcal{Y} \\to \\mathbb{R}^+$ is any measurable cost function and $\\mathcal{P}(X \\sim P_X, Y \\sim P_G)$ is the set of all joint distributions of $(X, Y)$ with marginals $P_X$ and $P_G$ respectively. When $(\\mathcal{X}, d)$ is a metric space (with distance $d$) and $c(x,y) = d^p(x,y)$ for $p \\geq 1$ then $W_p$ (the $p$-th root of $W_c$) is known as the $p$-Wasserstein distance. When $p=1$ we then have the $1$-Wasserstein distance and this duality holds (Kantorovich-Rubinstein duality)\n$$ W_1(P_X, P_G) = \\sup_{f \\in \\mathcal{F}_L} \\mathbb{E}_{X \\sim P_X} [f(X)] - \\mathbb{E}_{Y \\sim P_G}[f(Y)] $$\nwhere $\\mathcal{F}_L$ is the set of all bounded 1-Lipschitz functions on $(\\mathcal{X}, d)$. The authors state (and prove) that for deterministic decoders, i.e. $P_G(X|Z=z) = \\delta_{G(z)}$ deterministically mapping latent codes $z \\in \\mathcal{Z}$ to the input space for a given map $G: \\mathcal{Z} \\to \\mathcal{X}$, there is a simpler formulation for the OT problem: instead of finding a coupling $\\Gamma$ between two random variables in $\\mathcal{X}$ it is sufficient to find a conditional distribution $Q(Z|X)$ such that its $Z$ marginal $Q(Z) = \\mathbb{E}_{X \\sim P_X} [Q(Z|X)]$ is identical to the prior distribution $P_Z$. This resulted in a theorem where the OT problem was re-formulated (under these assumptions) as\n$$ W_c(P_X, P_G) = \\inf_{Q:Q_Z = P_Z} \\mathbb{E}_{P_X}\\big [ \\mathbb{E}_{Q(Z|X)}[c(X, G(Z))] \\big ] $$\nIn order to implement a numerical solution to this problem the authors relax the constraint on $Q_Z$ by adding a penalty term to the objective, which, in the end, takes the following form\n$$ D_{WAE}(P_X, P_G) = \\inf_{Q(Z|X) \\in \\mathcal{Q}} \\mathbb{E}_{P_X}\\big [ \\mathbb{E}_{Q(Z|X)}[c(X, G(Z))] \\big ] + \\lambda \\mathcal{D}_Z(Q_Z, P_Z) $$\nwhere $\\mathcal{Q}$ is any nonparametric set of probabilistic encoders, $\\mathcal{D}_Z$ is an arbitrary divergence between $Q_Z$ and $P_Z$ , and $\\lambda \u0026gt; 0$ is a hyperparameter. The authors propose two implementations of the WAE model, based on two different divergences.\nWAE-GAN The WAE-GAN model employs the Jensen-Shannon divergence, i.e.\n$$ \\mathcal{D}_Z(Q_Z, P_Z) = \\mathcal{D}_{JS}(Q_Z, P_Z) = \\frac{1}{2} D_{KL}(P_Z, M) + \\frac{1}{2} D_{KL}(Q_Z, M) $$\nwhere $M = \\frac{1}{2}(P_Z + Q_Z)$ and uses the adversarial training to estimate it. The authors introduce an adversary (discriminator) in the latent space $\\mathcal{Z}$ trying to separate \u0026ldquo;true\u0026rdquo; points sampled from $P_Z$ and \u0026ldquo;fake\u0026rdquo; ones sampled from $Q_Z$.\nWAE-MMD For a positive-definite reproducing kernel $k : \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ the following expression is called the Maximum Mean Discrepancy (MMD):\n$$ \\text{MMD}_k(P_Z, Q_Z) = \\Big | \\Big | \\int_\\mathcal{Z} k(z, \\cdot) dP_Z(z) - \\int_\\mathcal{Z} k(z, \\cdot) dQ_Z(z) \\Big | \\Big |_{\\mathcal{H}_k} $$\nwhere $\\mathcal{H}_k$ is the RKHS of real-valued functions mapping $\\mathcal{Z}$ to $\\mathcal{R}$. If $k$ is characteristic then MMD$_k$ defines a metric and can be used as a divergence measure. The authors propose to use an unbiased estimate of the MMD that is the sum of two $U$-statistics and a sample average. Given i.i.d. samples $\\mathbf{P} = { z_1, \\dots, z_n }$ from $P_Z$ and i.i.d. samples $\\mathbf{Q} = {\\tilde{z}_1, \\dots, \\tilde{z}_n}$ from $Q(Z|z_i)$ , the unbiased estimate4 is\n$$ \\widehat{MMD}^2_U \\big [\\mathcal{H}_k, \\mathbf{P}, \\mathbf{Q} \\big ] = \\frac{1}{n(n-1)} \\sum_{i=1}^n \\sum_{j \\neq i} k(z_i, z_j) + \\sum_{i=1}^n \\sum_{j \\neq i} k(\\tilde{z}_i, \\tilde{z}_j) - \\frac{2}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n k(z_i, \\tilde{z}_j) $$\nExperiments details Architechtures Encoder $Q_\\phi$, decoder $G_{\\theta}$ and discriminator $D_{\\gamma}$ are parametrized by deep neural nets. Given the structure of the data, binary-valued tensors of shape (H,W,5) (where 5, the number of instruments, plays naturally the role of the number of channels), both encoder and decoder are CNNs, while the discriminator (which acts in the latent space $\\mathcal{Z}$) is a \u0026ldquo;vanilla\u0026rdquo; NN. You can check out the various models I\u0026rsquo;ve coded here, they are almost all equals except for the strides of the convolutions.\nThe main architecture I used was inspired by DCGAN 5. In the following picture I\u0026rsquo;m showing the encoder that, starting from the (192, 84, 5) input tensor, applies 4 convolutions with $[6,6]$ filters, followed by Batch Normalizations and ReLUs. The last layer employs a reshape (not shown in the picture) followed by a (small dropout of 0.2 and a) fully-connected layer that maps the previous inputs into a vector of size $dim(\\mathcal{Z}) = d_{\\mathcal{Z}}$ (in the following picture $d_{\\mathcal{Z}} = 8$).\n My encoder (made with NN-SVG)   The decoder acts simmetrically wrt the encoder, using 2D transposed convolutions instead of convolutions. The discriminator network maps a vector from $\\mathcal{Z}$ to $[0,1]$ through 6 dense layers followed by ReLUs.\nPrior I\u0026rsquo;ve used a Gaussian prior $P_Z = \\mathcal{N} \\big ( 0, \\sigma_z^2 \\cdot I_{d_z} \\big )$ where the variance $\\sigma_z^2$ is an hyperparameter.\nLatent space dimensionality The authors studied the choice of the dimensionality of the latent space in a follow-up paper6. They investigated the relationship between the latent space dimension $d_{\\mathcal{Z}}$ and the *intrinsic dimensionality* of the data distribution, $d_{\\mathcal{I}}$. This last quantity is, informally, *the minimum number of parameters required to continuously parametrise the data manifold*. If this quantity is greater than $d_{\\mathcal{Z}}$ then the encoder will map the data distribution $P_X$ to $Q_Z$ supported on a latent manifold of dimension at most $d_{\\mathcal{I}}$ while the divergence regularizer $\\lambda \\mathcal{D}_Z (Q_Z, P_Z)$ will encourage the encoder to fill the latent space similarly to the prior $P_Z$ as much as possible. This can potentially lead to the absence of any learning capability.\nAdversarial training I\u0026rsquo;ve decided to use (as the authors suggest) the non-saturating GAN loss 7: this resulted in a discriminator $D_\\gamma$ trained by the maximization of\n$$ \\frac{\\lambda}{n} \\sum_{i=1}^n \\log D_\\gamma(z_i) + \\log D_\\gamma(1-\\tilde{z}_i) $$\nand in an autoencoder ($Q_\\phi, G_\\theta$) trained by the minimization of\n$$ \\frac{1}{n}\\sum_{i=1}^n c(z_i, G_\\theta(\\tilde{z}_i)) - \\lambda D_\\gamma \\log (\\tilde{z}_i) $$\nAs the authors point out this procedure falls into the min-max problem which makes GAN unstable. This training strategy can then possibly yield instabilities in the training phase but the authors claim that by executing an adversarial training in a \u0026ldquo;simpler\u0026rdquo; space (wrt the pixel space where GANs are usually trained on), the whole process could benefit.\nCost function I\u0026rsquo;ve used $c(x, y) = || x - y ||_2^2$ for both implementations (WAE-GAN and WAE-MMD).\nHyperparameters Since there is no objective measure of performace for this kind of task, running an hyperparameter optimization procedure is hard. I\u0026rsquo;ve adjusted the parameters manually and so the results I obtained might not be the best that these model can produce.\nResults As mentioned in the data section, the input samples are binary-valued tensors of shape (192, 84, 5). The decoder, although, maps a latent code into $[0, 1]^{192 \\times 84 \\times 5}$ through the action of the sigmoid function. These output tensors $\\hat{x}$ must then be binarized, and I\u0026rsquo;ve tried two strategies to do so:\n  Hard thresholding (HT): The resulting tensor entry $\\hat{x}_{ijk}$ needs to be greater than a fixed threshold (0.5) in order to be mapped to 1. $$\\hat{x}_{ijk}^{HT} = \\begin{cases} 1 \u0026amp; \\text{if } \\hspace{10pt} x_{ijk} \u0026gt; 0.5 \\\\\n0 \u0026amp; \\text {o.w }\\end{cases}$$\n  Bernoulli sampling (BS) (by the authors of MuseGAN): The resulting tensor entry $\\hat{x}_{ijk}$ needs to be greater than a sample from the uniform distribution over scalars between 0 and 1. $$\\hat{x}_{ijk}^{BS} = \\begin{cases} 1 \u0026amp; \\text{if } \\hspace{10pt} x_{ijk} \u0026gt; b \\sim \\mathcal{U}[0,1] \\\\\n0 \u0026amp; \\text {o.w }\\end{cases}$$\n  I\u0026rsquo;m presenting samples binarized with the HT method.\n For a \u0026ldquo;fair\u0026rdquo; comparison the following results have been obtained with the same architecture and hyperparameters. Both WAE-GAN and WAE-MMD models have been trained with a latent dimension of $d_{\\mathcal{Z}} = 128$. The number of filters in the four convolutions (see the picture above) are [8, 16, 32, 64] for the encoder and [64, 32, 16, 8] for the decoder. Both sequences of convolution used a fixed filter size of 6x6. I\u0026rsquo;ve trained both models for 500 epochs with Adam. Both autoencoder\u0026rsquo;s and discriminator\u0026rsquo;s optimizer started with a learning rate of $10^{-4}$ and both employed a decay to $\\approx 10^{-5}$.\n  WAE-GAN Loss    WAE-GAN Discriminator Loss    WAE-MMD Loss    WAE-MMD Penalty   Random Generation The following are a few of the \u0026ldquo;best\u0026rdquo; samples (wrt my personal taste) that I picked: please make sure to lower the level of your audio output3 before playing! I suggest listening with headphones.\n   WAE-GAN  WAE-MMD                                               The samples I was able to generete lightly suffer the problem of over-fragmentation. The notes generated tend to be fragmented, sometimes yielding chaotic and unpleasant results. This problem has also been recognized in 2 and the authors of MuseGAN proposed a new version of the model where their generator was equipped with binary neurons8, making it able to map the input noise directly into binary-valued output tensors. This partially solved the issue.\nIn my opinion WAE-GAN was able to produce better results: this was also the opinion of the authors of WAE.\nReconstruction These are a few reconstructed samples from the test set.\n   Original Sample  WAE-GAN  WAE-MMD                                                              Qualitatively speaking both models gave comparable results in terms of recostruction capabilities. It\u0026rsquo;s clear that they both struggle reconstructing the drums. Drums events are more sparse and they might need wider (and possibly more) filters in order to be reconstructed clearly.\nReferences    Wasserstein Auto-Encoders \u0026#x21a9;\u0026#xfe0e;\n  MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment \u0026#x21a9;\u0026#xfe0e;\n I\u0026rsquo;ve done the MIDI to MP3 conversions with the help of VLC and its integrated synthesizer FluidSynth. I\u0026rsquo;ve done no post-processing (eq, fx, mixing, mastering) so what you hear comes straight out-of-the box. The sounds produced are unpleasant: please think of them as part of an experiment and not music meant to be played. \u0026#x21a9;\u0026#xfe0e;\n  Kernel Mean Embedding of Distributions: A Review and Beyond page 52 \u0026#x21a9;\u0026#xfe0e;\n  Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks \u0026#x21a9;\u0026#xfe0e;\n  On the Latent Space of Wasserstein Auto-Encoders \u0026#x21a9;\u0026#xfe0e;\n  Are GANs Created Equal? A Large-Scale Study \u0026#x21a9;\u0026#xfe0e;\n  Convolutional Generative Adversarial Networks with Binary Neurons for Polyphonic Music Generation \u0026#x21a9;\u0026#xfe0e;\n   ","date":1596293173,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596293173,"objectID":"4b0abaabd8b95711a47845b486d43498","permalink":"https://w00zie.github.io/post/wae/","publishdate":"2020-08-01T16:46:13+02:00","relpermalink":"/post/wae/","section":"post","summary":"Table of Contents  Introduction The data Theory  WAE-GAN WAE-MMD   Experiments details  Architechtures Prior Latent space dimensionality Adversarial training Cost function Hyperparameters   Results  Random Generation Reconstruction      Introduction This is a project I started working on for my Machine Learning class.","tags":["generative-models","autoencoders","wasserstein"],"title":"Music generation with Wasserstein Autoencoders","type":"post"}]