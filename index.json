[{"authors":["admin"],"categories":null,"content":"This is my blog. I might write here some random stuff that interests me during my studies in Information Engineering. I\u0026rsquo;m not much of a coder but I may also embed some poorly-written python code along my posts. Hope you will find my content (sort of) interesting.\nCiao!\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https:w00zie.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"This is my blog. I might write here some random stuff that interests me during my studies in Information Engineering. I\u0026rsquo;m not much of a coder but I may also embed some poorly-written python code along my posts. Hope you will find my content (sort of) interesting.\nCiao!","tags":null,"title":"Giovanni Bindi","type":"authors"},{"authors":["open"],"categories":null,"content":" Books Scientific English  Deep Learning - Ian Goodfellow, Yoshua Bengio, Aaron Courville, MIT press, 2016. Information Theory, Pattern Recognition and Neural Networks - David McKay, Cambridge University Press, 2003. Reinforcement Learning, an Introduction - Richard S. Sutton, Andrew G. Barto, MIT press, 2015. Neural Networks and Deep Learning - Michael A. Nielsen1, 2015. An Introduction to Statistical Learning - Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani, Springer, 2017. The Elements of Statistical Learning - Trevor Hastie, Robert Tibshirani, Jerome Friedman, Springer, 2009. Bayesian Reasoning and Machine Learning - David Barber, Cambridge, 2012. Python Data Science Handbook - Jake VanderPlas, O\u0026rsquo;Reilly, 2016.  Italian Non-scientific English Italian Articles Scientific  Computational Complexity for Physicists - Stephan Mertens, 2002.  Non-scientific  Michael Nielsen is also a co-author of a beautiful book: Quantum Computation \u0026amp; Quantum Information. ^   ","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"6c0c4357bafb20bffeea0f4b3fc20200","permalink":"https:w00zie.github.io/authors/books/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/books/","section":"authors","summary":"Books Scientific English  Deep Learning - Ian Goodfellow, Yoshua Bengio, Aaron Courville, MIT press, 2016. Information Theory, Pattern Recognition and Neural Networks - David McKay, Cambridge University Press, 2003. Reinforcement Learning, an Introduction - Richard S. Sutton, Andrew G. Barto, MIT press, 2015. Neural Networks and Deep Learning - Michael A. Nielsen1, 2015. An Introduction to Statistical Learning - Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani, Springer, 2017.","tags":null,"title":"Articles \u0026 Books","type":"authors"},{"authors":["open"],"categories":null,"content":" Courses English  Information Theory, Pattern Recognition and Neural Networks - David McKay, Cambridge (also book freely available online, thank you David!). Reinforcement Learning - David Silver, UCL/DeepMind. Advanced Deep Learning \u0026amp; Reiforcement Learning - Various, UCL. Deep Learning - Nando de Freitas, Oxford (check his channel for other courses!). General Relativity - Leonard Susskind, Stanford.  Italian  Fisica dei Sistemi Complessi e Teoria dell\u0026rsquo;Informazione - Università di Firenze. Relatività Generale - Random Physics. Geometria Differenziale - Università di Padova.  Tutorials \u0026amp; Channels  Tensorflow tutorials - Hvass Laboratories. deeplearning.ai - An YouTube channel run by Andrew Ng.  ","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"c72708463944c028601583f0552cfe44","permalink":"https:w00zie.github.io/authors/open/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/open/","section":"authors","summary":"Courses English  Information Theory, Pattern Recognition and Neural Networks - David McKay, Cambridge (also book freely available online, thank you David!). Reinforcement Learning - David Silver, UCL/DeepMind. Advanced Deep Learning \u0026amp; Reiforcement Learning - Various, UCL. Deep Learning - Nando de Freitas, Oxford (check his channel for other courses!). General Relativity - Leonard Susskind, Stanford.  Italian  Fisica dei Sistemi Complessi e Teoria dell\u0026rsquo;Informazione - Università di Firenze.","tags":null,"title":"Courses","type":"authors"},{"authors":["open"],"categories":null,"content":" Podcasts Scientific English Italian Non-scientific English Italian ","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"176a51b25c6b65526c779194a70a2bd2","permalink":"https:w00zie.github.io/authors/podcasts/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/podcasts/","section":"authors","summary":" Podcasts Scientific English Italian Non-scientific English Italian ","tags":null,"title":"Podcasts","type":"authors"},{"authors":["wabri"],"categories":null,"content":"Gabriele is a friend of mine.\nHe is an active developer interested in different fields of Computer Science, make sure to check out his blog!\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"5231dca2ff835897771ddc2d48aefd90","permalink":"https:w00zie.github.io/authors/wabri/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/wabri/","section":"authors","summary":"Gabriele is a friend of mine.\nHe is an active developer interested in different fields of Computer Science, make sure to check out his blog!","tags":null,"title":"Gabriele Puliti","type":"authors"},{"authors":null,"categories":null,"content":"If you want to lurk other configs in the same style as mine take a look at the masters.\nSoftware specifications  OS: Debian GNU/Linux Testing WM: i3wm Bar: i3status Terminal: urxvt Shell: Bash File Manager: Thunar (graphical), ranger (terminal) Launcher: rofi Editor: VSCode (graphical), vim (terminal) Browser: Firefox Mail: Thunderbird Chat: Rambox   i3wm I'm using i3-gaps with i3status for window managing and lightdm as display manager.\n   My config is pretty simple, these are the main shortcuts I use:\nEvery key in this list is intented to be pressed along with super (in my case the Windows) key. For example if you want to launch a terminal you hold super+enter, if you want to move to workspace 2 you hold super+2.\n enter: new terminal d: launch rofi f: toggle fullscreen w: toggle tabbed layout shift+q: close focused window [1-10]: change workspace shift+[1-10]: move focused window to selected workspace ctrl+right: move to the next workspace ctrl+left: move to the previous workspace  I'm using fixed workspaces for certain apps, for example: if I want to launch Firefox I press super+d and then type or select Firefox from rofi. The Firefox window will be automatically launched in the second workspace, as I imposed i3wm to contain the browser windows there.\nAs you can notice from the previous screenshot I added an icon to every workspace (browser to the second, file manager to the third, etc.) and you will need fontawesome installed to replicate that.\nYou can dig my i3 config here and my i3status config here.\nIf you want to know more about i3wm take a look at this or this.\nWallpaper (1920x1080)    Thunar    I'm using the Equilux Compact theme with the Numix icon pack.\nURxvt \u0026amp; Bash Take a look at my .Xresources and .bashrc\n   Firefox These are the extensions I use:\n NoScript uBlock Origin Cookie AutoDelete Decentraleyes HTTPS Everywhere Link Cleaner Privacy Badger Privacy Possum Markdown Here and sometimes Mailvelope for e-mails.   Plus I've tweaked a bit Firefox's own config, but maybe I'll talk about that in a post.\nVim    I'm using a slightly modified version of this config (the basic one) for my own usage.\nPlugins installed:\n YouCompleteMe fugitive.vim Tabular vim-markdown NERD tree flake8  Visual Studio Code    These are the extensions I've installed:\n Remote - this is a very nice feature if you want to remotely connect and edit files to an host without having to use vim. It may be one of the strongest reasons I'm using a Microsoft product in my system. Vim Haskell Syntax Highliting Markdown All in One Python C/C++  ","date":1564876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564876800,"objectID":"f3cfa75cbd275cabffc58692040f7014","permalink":"https:w00zie.github.io/post/dotfiles/","publishdate":"2019-08-04T00:00:00Z","relpermalink":"/post/dotfiles/","section":"post","summary":"In this post I will show you my Linux setup on my laptop.","tags":null,"title":"Linux + ThinkPad = ♥ ","type":"post"},{"authors":null,"categories":null,"content":"    One of the most known and widely used learning model is SVM (Support Vector Machine).\nIn this first post we are going to analyze the formal structure of the problem and we are going through the (simple) math needed to write down the SVM equations for linear case. In the next post we'll be extending this to the non-linear behaviour, introducing the concept of kernels.\nLet's start with definitions.\nProblem Structure We are given a set of couples, named $D$, which is going to be our data set.\n\\[ D = \\{ (x_i, y_i) : x_i \\in X \\subseteq \\mathbb{R}^n, y_i \\in Y, i=1,\\dots,N \\}\\]\nThe $x_i$ are usually referred as observations while the $y_i$ are called labels or targets. We have $|D|=N$ data points.\nIf $Y = \\mathbb{R}$ we are dealing with regression, otherwise with classification. For this walkthrough we will deal with classification and so we'll adopt, without loss of generality, \\(Y = \\{+1, -1\\}\\).\n Linearly separable sets This is the simplest case: we have two disjoint sets, namely $P$ and $N$:\n\\[ P = \\{ x_i \\in D: y_i = +1 \\} \\\\ N = \\{ x_i \\in D: y_i = -1 \\} \\]\nthat are linearly separable. This condition is expressed through the existence of a separating hyperplane\n\\[ H(w,b) = \\{ x \\in \\mathbb{R}^n : w \\cdot x + b = 0 \\} \\]\nsuch that the points $x_i \\in P$ belong to an half space and the points $x_i \\in N$ to the other one. Formally $\\exists \\hspace{3pt} w \\in \\mathbb{R}^n, b \\in \\mathbb{R}$:\n\\[ w \\cdot x_i + b \\geq \\epsilon, \\hspace{15pt} \\forall x_i \\in P \\\\ w \\cdot x_j + b \\leq -\\epsilon, \\hspace{15pt} \\forall x_j \\in N \\]\nwith $\\epsilon \u0026gt; 0$. Dividing by $\\epsilon$ both members of both equations we obtain:\n\\[ w \\cdot x_i + b \\geq 1, \\hspace{15pt} \\forall x_i \\in P \\\\ w \\cdot x_j + b \\leq -1, \\hspace{15pt} \\forall x_j \\in N \\]\n  Two linearly separable sets.    Margin A fundamental concept in SVM is the margin, expressed as the minimum distance $\\gamma$ between the points in $D \\overset{\\Delta}{=} P \\cup N$ and the separating hyperplane $H(w,b)$:\n\\[ \\gamma(w, b) = \\min_{x \\in D} \\bigg \\{ \\frac{|w \\cdot x + b|}{||w||} \\bigg \\} \\]\nWhere $|| \\cdot || := || \\cdot ||_2$ represents the euclidean norm.\nDerivation This minimum distance formulation is well known in Geometry, but we can derive it from an optimization point of view as well. Let's start calculating the distance between a point $\\bar{x}$ and the hyperplane $H(w,b)$. We can write down this minimization problem as:\n\\[ \\min_{x \\in \\mathbb{R}^n} \\quad ||\\bar{x} - x|| \\\\ \\hspace{15pt}\\textrm{s.t.} \\quad w \\cdot x + b = 0 \\]\nWe can apply a strictly increasing transformation to our objective function (to get rid of the non-differentiability) without changing the global optima of our problem:\n\\[ \\min_{x \\in \\mathbb{R}^n} \\quad \\frac{1}{2}||\\bar{x} - x||^2 \\\\ \\hspace{2pt} \\textrm{s.t.} \\quad w \\cdot x + b = 0 \\]\nWe now have a strictly convex objective function subject to continuously differentiable linear constraints so we can apply the Karush–Kuhn–Tucker optimality conditions:\n\\[ \\nabla_x \\bigg [ \\frac{1}{2} ||\\bar{x} - x||^2 + \\lambda(w \\cdot x + b) \\bigg ] = 0 \\]\n\\[ \\hspace{67pt}= (\\bar{x} - x) + \\lambda w = 0 \\]\nExpressing $x = \\bar{x} + \\lambda w$ and substituting it into the constraint equation we obtain:\n\\[ w \\cdot (\\bar{x} + \\lambda w) + b \\]\n\\[ \\hspace{3pt} = w \\cdot \\bar{x} + \\lambda ( w \\cdot w) + b \\]\n\\[ \\hspace{14pt}= w \\cdot \\bar{x} + \\lambda ||w||^2 + b = 0 \\]\nFrom which we can isolate the lagrangean multiplier and write\n\\[\\lambda = -\\frac{w \\cdot \\bar{x} + b}{||w||^2}\\]\nWe were originally interested in $||\\bar{x} - x||$ and from the optimality conditions we can now re-write this quantity as:\n\\[||\\bar{x} - x|| = ||\\lambda w|| = \\frac{|w \\cdot \\bar{x} + b|}{||w||}\\]\nWhich is the projection of a given point $\\bar{x}$ on an hyperplane $H(w,b)$.\nWe can now obtain the definition of margin given above using this result, extending the definition so that we are interested in the minimum distance between every point $x \\in D$ to $H(w,b)$.\nIt is quite intuitive that the margin of a given separating hyperplane is related to the generalization capability of the corresponding linear classifier. This relationship is analyzed in the Statistical Learning Theory, which theoretically motivates the importance of defining the hyperplane with maximum margin, the so-called optimal separating hyperplane.\n Optimal Separating Hyperplane The optimal separating hyperplane is a separating hyperplane \\(H(w^*, b^*)\\) having maximum margin:\n\\[ \\max_{w \\in \\mathbb{R}^n, b \\in \\mathbb{R}} \\gamma(w,b) = \\max_{w \\in \\mathbb{R}^n, b \\in \\mathbb{R}} \\min_{x \\in D} \\bigg \\{ \\frac{|w \\cdot x + b|}{||w||} \\bigg \\} \\]\nThis thing does not look good: it's a max-min problem. We can re-write this quantity as\n\\[ \\max_{\\eta, w, b} \\eta \\\\ \\hspace{60pt}\\text{s.t } \\hspace{5pt} \\eta \\leq \\frac{|w \\cdot x + b|}{||w||} \\\\ \\hspace{150pt}y(w \\cdot x +b) \\geq 1 \\quad \\quad \\forall (x,y) \\in D \\]\nsince we are dealing with a finite number of inequalities (this assures us that the optimal $\\eta$ will satisfy the first constraint inequality as an equality).\nThis thing still does not look good: both the numerator and the denominator are non-differentiable. It can be (quite easily) shown that the numerator $|w \\cdot x + b|$ can be always set to $1$, leaving us with:\n\\[ \\max_{\\eta, w, b} \\eta \\\\ \\hspace{35pt}\\text{s.t } \\hspace{5pt} \\eta \\leq \\frac{1}{||w||} \\\\ \\hspace{150pt}y(w \\cdot x +b) \\geq 1 \\quad \\quad \\forall (x,y) \\in D \\]\nAs you can notice in the first constraint we passed from $|D|$ inequalities to only one, as we lost the dependance from $x$. We can re-write our problem as:\n\\[ \\max_{w, b} \\frac{1}{||w||} \\\\ \\hspace{120pt}\\text{s.t } \\hspace{10pt} y(w \\cdot x +b) \\geq 1 \\quad \\quad \\forall (x,y) \\in D \\]\nWe get rid of the non-differentiability of the denominator by transforming this $max$ problem to a $min$ problem and applying a transformation to the objective function:\n\\[ \\max_{w, b} \\frac{1}{2}||w||^2 \\\\ \\hspace{110pt}\\text{s.t } \\hspace{10pt} y(w \\cdot x +b) \\geq 1 \\quad \\quad \\forall (x,y) \\in D \\]\nThis is now a minimization problem of a quadratic function subject to linear constraints, this means that we can easily solve this!\n Lagrange Dual The final step towards the solution of this problem is imposing the optimality conditions in the Lagrange Dual. This is the classical approach to the solution but it's not the only one.\nBefore doing that it's important to highlight that we can demonstrate the equivalency of the starting problem and the one we derived, but we are not going to do that.\nLet's relax the constraints and embed them into the objective function: the problem associated with the Lagrangian dual is\n\\[ \\max_{\\lambda \\geq 0} \\Big \\{ \\min_{w,b} \\frac{1}{2} w^{\\intercal}w + \\sum_{i} \\lambda_{i} \\big[ 1 - y_i (w \\cdot x_i + b) \\big ] \\Big \\} = \\\\ \\max_{\\lambda \\geq 0} \\sum_i \\lambda_i + \\min_{w,b} \\frac{1}{2} w^{\\intercal}w - \\sum_i \\lambda_i y_i (w \\cdot x_i + b) \\quad \\quad (\\alpha) \\]\nwhich is a convex unconstrained optimization problem. Let's leave the $max$ apart for a moment and concentrate on the internal $min$ problem, we know that the optimality conditions are the nullification of the gradient of the Lagrangian:\n\\[ \\nabla \\mathcal{L}(w,b) = \\nabla_{w,b} \\Big [ \\min_{w,b} \\frac{1}{2}w^\\intercal w - \\sum_i \\lambda_i y_i(w \\cdot x_i +b) \\Big ] = 0 \\]\nfrom which we have:\n\\[ \\nabla_w \\mathcal{L}(w,b) = 0 \\implies w - \\sum_i \\lambda_i x_i y_i = 0 \\implies \\boxed{w = \\sum_i \\lambda_i y_i x_i \\quad} (1) \\\\ \\nabla_b \\mathcal{L}(w,b) = 0 \\implies 0 - \\sum_i \\lambda_i y_i = 0 \\implies \\boxed{\\sum_i \\lambda_i y_i = 0} \\hspace{35pt} (2) \\]\nWe can now solve the $max$ problem. Plugging $(1)$ into $(\\alpha)$ we obtain:\n\\[ \\max_{\\lambda \\geq 0} \\sum_i \\lambda_i + \\frac{1}{2}\\sum_i \\sum_j \\lambda_i \\lambda_j y_i y_j x_i^\\intercal x_j - \\sum_i \\lambda_i y_i \\Big (\\sum_j \\lambda_j y_j x_j^\\intercal \\Big )x_i - \\sum_i \\lambda_i y_i b \\quad (\\alpha^{'}) \\]\nSubstituting $(2)$ into $(\\alpha^{'})$ and switching signs (in order to pass from a $max$ to a $min$ problem) we have:\n\\[ \\min_{\\lambda \\geq 0} \\frac{1}{2} \\sum_i \\sum_j \\lambda_i \\lambda_j y_i y_j x_i^\\intercal x_j - \\sum_i \\lambda_i \\\\ \\hspace{-75pt}{\\text{s.t. } \\sum_i \\lambda_i y_i = 0} \\]\nSetting \\(X = [y_1x_1 \\dots y_nx_n]\\), \\(\\lambda = [\\lambda_1 \\dots \\lambda_n]^\\intercal\\) and \\(y = [y_1 \\dots y_n]\\) we can finally write this in a compact form:\n\\[ \\min_{\\lambda \\geq 0} \\frac{1}{2} \\lambda^\\intercal X^\\intercal X \\lambda - e^\\intercal \\lambda\\\\ \\hspace{-33pt}{\\text{s.t } \\hspace{10pt} \\lambda \\cdot y = 0} \\]\n ","date":1564704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564704000,"objectID":"5387f46fd4d5ac5a0e82e20e2fa80c12","permalink":"https:w00zie.github.io/post/svm/","publishdate":"2019-08-02T00:00:00Z","relpermalink":"/post/svm/","section":"post","summary":"A mathematical tour of SVM: linear classification.","tags":null,"title":"SVM explained - Part 1: Binary linear classification.","type":"post"},{"authors":null,"categories":null,"content":"    In this first post we'll give a brief introduction to the necessary linear algebra for Quantum Mechanics.\nLinear Algebra Vectors The basic objects of linear algebra are vector spaces. The vector space of most interest in QM is $\\mathbb{C}^n$, the space of all $n$-tuples of complex numbers $(z_1, \\dots, z_n)$. The elements of a vector space are called vectors. The standard quantum mechanical notation for a vector in a vector space is the following:\n\\[ \\lvert \\psi \\rangle \\]\nA spanning set for a vector space is a set of vectors \\(\\lvert v_1 \\rangle, \\dots, \\lvert v_n \\rangle\\) such that any vector \\(\\lvert v \\rangle\\) can be written as a linear combination \\(\\lvert v \\rangle = \\sum_{i} a_i \\lvert v_i \\rangle\\) of vectors in that set. For example, a spanning set for the vector space $\\mathbb{C}^2$ is the set:\n\\[ \\lvert v_1 \\rangle = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix}; \\quad \\lvert v_2 \\rangle = \\begin{bmatrix} 0 \\\\ 1 \\\\ \\end{bmatrix}, \\]\nsince any vector\n\\[ \\lvert v \\rangle = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\end{bmatrix} \\]\ncan be expressed as a linear combination $\\lvert v \\rangle = a_1 \\lvert v_1 \\rangle + a_2 \\lvert v_2 \\rangle$ of the vectors $\\lvert v_1 \\rangle$ and $\\lvert v_2 \\rangle$.\nGenerally, a vector space may have many different spanning sets. A second spanning set for $\\mathbb{C}^2$ is the set:\n\\[ \\lvert v_1 \\rangle = \\frac{1}{\\sqrt2} \\begin{bmatrix} 1 \\\\ 1 \\\\ \\end{bmatrix}; \\quad \\lvert v_2 \\rangle = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ -1 \\\\ \\end{bmatrix}, \\]\nA set of non-zero vectors $\\lvert v_1 \\rangle, \\dots, \\lvert v_n \\rangle$ are linearly dependent if there exists a set of complex numbers $a_1, \\dots, a_n$ with $a_i \\neq 0$ for at least one value of $i$, such that:\n\\[ a_1 \\lvert v_1 \\rangle + a_2 \\lvert v_2 \\rangle + \\dots a_n \\lvert v_n \\rangle = 0 \\]\nA set of vectors is linearly independent if it is not linearly dependent. A set of linearly independent vectors form a basis for their relative vector space.\nLinear operators and matrices A linear operator between vector spaces $V$ and $W$ is defined to be any function $A:V \\to W$ which is linear in its inputs:\n\\[ A \\bigg ( \\sum_{i} a_i \\lvert v_i \\rangle \\bigg ) = \\sum_{i} a_i A(\\lvert v_i \\rangle)\\]\nAnd we just write $A \\lvert v \\rangle$ to denote this application.\nThe most convenient way to understand linear operators is in terms of their matrix representations. In fact, the linear operator and matrix viewpoints turn out to be completely equivalent.\nSuppose $\\lvert v_1 \\rangle, \\dots, \\lvert v_m \\rangle$ is a basis for \\(V\\) and \\(\\lvert w_1 \\rangle, \\dots, \\lvert w_n \\rangle\\) is a basis for \\(W\\). Then $\\forall j = {1, \\dots, m }$ there exists complex numbers $A_{1j}, \\dots, A_{nj}$ such that:\n\\[ A \\lvert v_j \\rangle = \\sum_{i} A_{ij} \\lvert w_i \\rangle \\]\nInner products An inner product is a function which takes as input two vectors $\\lvert v \\rangle$ and $\\lvert w \\rangle$ from a vector space and produces a complex number as output. The standard notation for the inner product is $\\langle v \\lvert w \\rangle$ where $\\langle v \\rvert$ is the dual vector to the vector $\\lvert v \\rangle$. A function $(\\cdot , \\cdot): V \\times V \\to \\mathbb{C}$ is an inner product if it satisfies:\n Linearity in the second argument: \\( \\bigg (\\lvert v \\rangle, \\sum_{i} \\lambda_i \\lvert w_i \\rangle \\bigg ) = \\sum_{i} \\lambda_{i} (\\lvert v \\rangle, \\lvert w_i \\rangle) \\)\n $(\\lvert v \\rangle , \\lvert w \\rangle) = (\\lvert w \\rangle, \\lvert v \\rangle)^*$\n $(\\lvert v \\rangle, \\lvert v \\rangle) \\geq 0$ with equality iff $\\lvert v \\rangle = 0$.\n  For example $\\mathbb{C}^n$ has an inner product defined by:\n\\[ ((y_1, \\dots, y_n ), (z_1, \\dots z_n)) = \\sum_{i} y^*_{i}z_i \\]\nWe call a vector space with an inner product an inner product space or (usually) an Hilbert space.\nWe define the norm of a vector $\\lvert v \\rangle$ by:\n\\[ || \\lvert v \\rangle || = \\sqrt{\\langle v \\lvert v \\rangle}\\]\nFor any non-zero vector $\\lvert v \\rangle$ we talk about normalization or normalized form the vector $\\lvert v \\rangle / || \\lvert v \\rangle ||$.\n With these conventions, the inner product on a Hilbert space can be given a convenient matrix representation. Let $\\lvert w \\rangle = \\sum_i w_i \\lvert i \\rangle$ and \\(\\lvert v \\rangle = \\sum_i v_i \\lvert i \\rangle\\) be two vectors defined on an orthonormal basis $\\lvert i \\rangle$ then, since $\\langle i \\lvert j \\rangle = \\delta_{ij}$ we have:\n\\[ \\langle v \\lvert w \\rangle = \\Big ( \\sum_i v_i \\lvert i \\rangle, \\sum_j w_j\\lvert j \\rangle \\Big ) = \\sum_{ij} v_i^*w_j \\delta_{ij} = \\sum_i v_i^*w_i = \\\\ \\begin{bmatrix} v_1^* \\dots, v_n^* \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ \\vdots \\\\ w_n \\end{bmatrix} \\]\nThat is, the inner product of two vectors is equal to the vector inner product between two matrix representations of those vectors, provided the representations are written with respect to the same orthonormal basis.\nWe also see that the dual vector $\\langle v \\lvert$ has a nice interpretation as the row vector whose components are complex conjugates of the corresponding components of the column vector representation of $\\lvert v \\rangle$.\n","date":1555977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555977600,"objectID":"38a12ab686f836bad3b63eedaa5c1a41","permalink":"https:w00zie.github.io/post/quantum_1_linalg/","publishdate":"2019-04-23T00:00:00Z","relpermalink":"/post/quantum_1_linalg/","section":"post","summary":"This is the first chapter of a series of articles about Quantum Information. We quickly talk about linear algebra here.","tags":null,"title":"Quantum Information - Part 1: Linear Algebra","type":"post"}]