<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Giovanni Bindi</title>
    <link>https:w00zie.github.io/post/</link>
      <atom:link href="https:w00zie.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 04 Aug 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https:w00zie.github.io</url>
      <title>Posts</title>
      <link>https:w00zie.github.io/post/</link>
    </image>
    
    <item>
      <title>Linux &#43; ThinkPad = ♥ </title>
      <link>https:w00zie.github.io/post/dotfiles/</link>
      <pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https:w00zie.github.io/post/dotfiles/</guid>
      <description>&lt;p&gt;If you want to lurk other configs in the same style as mine take a look at the &lt;a href=&#34;https://www.reddit.com/r/unixporn/&#34;&gt;masters&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;software-specifications&#34;&gt;Software specifications&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;OS&lt;/strong&gt;: Debian GNU/Linux Testing&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;WM&lt;/strong&gt;: &lt;a href=&#34;https://i3wm.org/&#34;&gt;i3wm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bar&lt;/strong&gt;: i3status&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Terminal&lt;/strong&gt;: &lt;a href=&#34;https://wiki.archlinux.org/index.php/Rxvt-unicode&#34;&gt;urxvt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Shell&lt;/strong&gt;: Bash&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;File Manager&lt;/strong&gt;: Thunar (graphical), &lt;a href=&#34;https://ranger.github.io/&#34;&gt;ranger&lt;/a&gt; (terminal)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Launcher&lt;/strong&gt;: &lt;a href=&#34;https://github.com/davatorium/rofi&#34;&gt;rofi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Editor&lt;/strong&gt;: VSCode (graphical), vim (terminal)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Browser&lt;/strong&gt;: Firefox&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mail&lt;/strong&gt;: Thunderbird&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Chat&lt;/strong&gt;: &lt;a href=&#34;https://github.com/ramboxapp/community-edition&#34;&gt;Rambox&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr&gt;

&lt;h2 id=&#34;i3wm&#34;&gt;i3wm&lt;/h2&gt;

&lt;p&gt;I&#39;m using &lt;a href=&#34;https://github.com/Airblader/i3&#34;&gt;i3-gaps&lt;/a&gt; with &lt;a href=&#34;https://i3wm.org/i3status/manpage.html&#34;&gt;i3status&lt;/a&gt; for window managing and &lt;a href=&#34;https://wiki.archlinux.org/index.php/LightDM&#34;&gt;lightdm&lt;/a&gt; as display manager.&lt;/p&gt;







&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https:w00zie.github.io/img/linux/screen1.jpg&#34; &gt;

&lt;img src=&#34;https:w00zie.github.io/img/linux/screen1.jpg&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;p&gt;My config is pretty simple, these are the main shortcuts I use:&lt;/p&gt;

&lt;p&gt;Every key in this list is intented to be pressed along with &lt;code&gt;super&lt;/code&gt; (in my case the Windows) key. For example if you want to launch a terminal you hold &lt;code&gt;super+enter&lt;/code&gt;, if you want to move to workspace 2 you hold &lt;code&gt;super+2&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;enter&lt;/code&gt;: new terminal&lt;/li&gt;
&lt;li&gt;&lt;code&gt;d&lt;/code&gt;: launch rofi&lt;/li&gt;
&lt;li&gt;&lt;code&gt;f&lt;/code&gt;: toggle fullscreen&lt;/li&gt;
&lt;li&gt;&lt;code&gt;w&lt;/code&gt;: toggle tabbed layout&lt;/li&gt;
&lt;li&gt;&lt;code&gt;shift+q&lt;/code&gt;: close focused window&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[1-10]&lt;/code&gt;: change workspace&lt;/li&gt;
&lt;li&gt;&lt;code&gt;shift+[1-10]&lt;/code&gt;: move focused window to selected workspace&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ctrl+right&lt;/code&gt;: move to the next workspace&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ctrl+left&lt;/code&gt;: move to the previous workspace&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&#39;m using fixed workspaces for certain apps, for example: if I want to launch Firefox I press &lt;code&gt;super+d&lt;/code&gt; and then type or select Firefox from rofi. The Firefox window will be automatically launched in the second workspace, as I imposed i3wm to contain the browser windows there.&lt;/p&gt;

&lt;p&gt;As you can notice from the previous screenshot I added an icon to every workspace (browser to the second, file manager to the third, etc.) and you will need &lt;a href=&#34;https://fontawesome.com/&#34;&gt;fontawesome&lt;/a&gt; installed to replicate that.&lt;/p&gt;

&lt;p&gt;You can dig my i3 config &lt;a href=&#34;https://github.com/w00zie/dotfiles/blob/master/i3/config&#34;&gt;here&lt;/a&gt; and my i3status config &lt;a href=&#34;https://github.com/w00zie/dotfiles/blob/master/i3status/config&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you want to know more about i3wm take a look at &lt;a href=&#34;https://www.youtube.com/watch?v=j1I63wGcvU4&#34;&gt;this&lt;/a&gt; or &lt;a href=&#34;https://www.youtube.com/watch?v=1tAFXThjzsY&#34;&gt;this&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;wallpaper-1920x1080&#34;&gt;Wallpaper (1920x1080)&lt;/h3&gt;







&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https:w00zie.github.io/img/homer.jpg&#34; &gt;

&lt;img src=&#34;https:w00zie.github.io/img/homer.jpg&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;h2 id=&#34;thunar&#34;&gt;Thunar&lt;/h2&gt;







&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https:w00zie.github.io/img/linux/screen2.jpg&#34; &gt;

&lt;img src=&#34;https:w00zie.github.io/img/linux/screen2.jpg&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;p&gt;I&#39;m using the &lt;a href=&#34;https://github.com/ddnexus/equilux-theme&#34;&gt;Equilux Compact&lt;/a&gt; theme with the &lt;a href=&#34;https://numixproject.github.io/&#34;&gt;Numix&lt;/a&gt; icon pack.&lt;/p&gt;

&lt;h2 id=&#34;urxvt--bash&#34;&gt;URxvt &amp;amp; Bash&lt;/h2&gt;

&lt;p&gt;Take a look at my &lt;a href=&#34;https://github.com/w00zie/dotfiles/blob/master/.Xresources&#34;&gt;.Xresources&lt;/a&gt; and &lt;a href=&#34;https://github.com/w00zie/dotfiles/blob/master/.bashrc&#34;&gt;.bashrc&lt;/a&gt;&lt;/p&gt;







&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https:w00zie.github.io/img/linux/screen3.jpg&#34; &gt;

&lt;img src=&#34;https:w00zie.github.io/img/linux/screen3.jpg&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;h2 id=&#34;firefox&#34;&gt;Firefox&lt;/h2&gt;

&lt;p&gt;These are the extensions I use:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://noscript.net/&#34;&gt;NoScript&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gorhill/uBlock&#34;&gt;uBlock Origin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://addons.mozilla.org/it/firefox/addon/cookie-autodelete/&#34;&gt;Cookie AutoDelete&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://addons.mozilla.org/it/firefox/addon/decentraleyes/&#34;&gt;Decentraleyes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://addons.mozilla.org/it/firefox/addon/https-everywhere/&#34;&gt;HTTPS Everywhere&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://addons.mozilla.org/it/firefox/addon/link-cleaner/&#34;&gt;Link Cleaner&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://addons.mozilla.org/it/firefox/addon/privacy-badger17/?src=recommended&#34;&gt;Privacy Badger&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://addons.mozilla.org/it/firefox/addon/privacy-possum/&#34;&gt;Privacy Possum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://addons.mozilla.org/it/firefox/addon/markdown-here/&#34;&gt;Markdown Here&lt;/a&gt; and sometimes &lt;a href=&#34;https://www.mailvelope.com/en&#34;&gt;Mailvelope&lt;/a&gt; for e-mails.
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Plus I&#39;ve tweaked a bit Firefox&#39;s own config, but maybe I&#39;ll talk about that in a post.&lt;/p&gt;

&lt;h2 id=&#34;vim&#34;&gt;Vim&lt;/h2&gt;







&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https:w00zie.github.io/img/linux/screen5.jpg&#34; &gt;

&lt;img src=&#34;https:w00zie.github.io/img/linux/screen5.jpg&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;p&gt;I&#39;m using a slightly modified version of this &lt;a href=&#34;https://github.com/amix/vimrc&#34;&gt;config&lt;/a&gt; (the &lt;em&gt;basic&lt;/em&gt; one) for &lt;a href=&#34;https://github.com/w00zie/dotfiles/blob/master/.vimrc&#34;&gt;my own&lt;/a&gt; usage.&lt;/p&gt;

&lt;p&gt;Plugins installed:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ycm-core/YouCompleteMe&#34;&gt;YouCompleteMe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/tpope/vim-fugitive&#34;&gt;fugitive.vim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/godlygeek/tabular&#34;&gt;Tabular&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/plasticboy/vim-markdown&#34;&gt;vim-markdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/scrooloose/nerdtree&#34;&gt;NERD tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/nvie/vim-flake8&#34;&gt;flake8&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;visual-studio-code&#34;&gt;Visual Studio Code&lt;/h2&gt;







&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https:w00zie.github.io/img/linux/screen4.jpg&#34; &gt;

&lt;img src=&#34;https:w00zie.github.io/img/linux/screen4.jpg&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;p&gt;These are the extensions I&#39;ve installed:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://code.visualstudio.com/docs/remote/remote-overview&#34;&gt;Remote&lt;/a&gt; - this is a very nice feature if you want to remotely connect and edit files to an host without having to use vim. It may be one of the strongest reasons I&#39;m using a Microsoft product in my system.&lt;/li&gt;
&lt;li&gt;Vim&lt;/li&gt;
&lt;li&gt;Haskell Syntax Highliting&lt;/li&gt;
&lt;li&gt;Markdown All in One&lt;/li&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;C/C++&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>SVM explained - Part 1: Binary linear classification.</title>
      <link>https:w00zie.github.io/post/svm/</link>
      <pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https:w00zie.github.io/post/svm/</guid>
      <description>





&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https:w00zie.github.io/img/svm/front.jpg&#34; &gt;

&lt;img src=&#34;https:w00zie.github.io/img/svm/front.jpg&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;p&gt;One of the most known and widely used learning model is &lt;strong&gt;SVM&lt;/strong&gt; (&lt;a href=&#34;https://en.wikipedia.org/wiki/Support-vector_machine&#34;&gt;Support Vector Machine&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;In this first post we are going to analyze the formal structure of the problem and we are going through the (simple) math needed to write down the SVM equations for linear case. In the next post we&#39;ll be extending this to the non-linear behaviour, introducing the concept of &lt;strong&gt;kernels&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Let&#39;s start with definitions.&lt;/p&gt;

&lt;h1 id=&#34;problem-structure&#34;&gt;Problem Structure&lt;/h1&gt;

&lt;p&gt;We are given a set of couples, named $D$, which is going to be our data set.&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ D = \{ (x_i, y_i) : x_i \in X \subseteq \mathbb{R}^n, y_i \in Y, i=1,\dots,N \}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The $x_i$ are usually referred as &lt;em&gt;observations&lt;/em&gt; while the $y_i$ are called &lt;em&gt;labels&lt;/em&gt; or &lt;em&gt;targets&lt;/em&gt;. We have $|D|=N$ data points.&lt;/p&gt;

&lt;p&gt;If $Y = \mathbb{R}$ we are dealing with &lt;em&gt;regression&lt;/em&gt;, otherwise with &lt;em&gt;classification&lt;/em&gt;. For this walkthrough we will deal with classification and so we&#39;ll adopt, without loss of generality, &lt;span  class=&#34;math&#34;&gt;\(Y = \{+1, -1\}\)&lt;/span&gt;.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id=&#34;linearly-separable-sets&#34;&gt;Linearly separable sets&lt;/h2&gt;

&lt;p&gt;This is the simplest case: we have two disjoint sets, namely $P$ and $N$:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ P = \{ x_i \in D: y_i = +1 \} \\
   N = \{ x_i \in D: y_i = -1 \} \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;that are &lt;em&gt;linearly separable&lt;/em&gt;. This condition is expressed through the existence of a &lt;strong&gt;separating hyperplane&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ H(w,b) = \{ x \in \mathbb{R}^n : w \cdot x + b = 0 \} \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;such that the points $x_i \in P$ belong to an half space and the points $x_i \in N$ to the other one. Formally $\exists \hspace{3pt} w \in \mathbb{R}^n, b \in \mathbb{R}$:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ w \cdot x_i + b \geq \epsilon, \hspace{15pt} \forall x_i \in P \\
 w \cdot x_j + b \leq -\epsilon, \hspace{15pt} \forall x_j \in N \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;with $\epsilon &amp;gt; 0$. Dividing by $\epsilon$ both members of both equations we obtain:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ w \cdot x_i + b \geq 1, \hspace{15pt} \forall x_i \in P \\
 w \cdot x_j + b \leq -1, \hspace{15pt} \forall x_j \in N \]&lt;/span&gt;&lt;/p&gt;







&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https:w00zie.github.io/img/svm/linsep.jpg&#34; &gt;

&lt;img src=&#34;https:w00zie.github.io/img/svm/linsep.jpg&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Two linearly separable sets.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;hr&gt;

&lt;h2 id=&#34;margin&#34;&gt;Margin&lt;/h2&gt;

&lt;p&gt;A fundamental concept in SVM is the &lt;strong&gt;margin&lt;/strong&gt;, expressed as the minimum distance $\gamma$ between the points in $D \overset{\Delta}{=} P \cup N$ and the separating hyperplane $H(w,b)$:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ \gamma(w, b) = \min_{x \in D} \bigg \{ \frac{|w \cdot x + b|}{||w||} \bigg \} \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Where $|| \cdot || := || \cdot ||_2$ represents the euclidean norm.&lt;/p&gt;

&lt;h3 id=&#34;derivation&#34;&gt;Derivation&lt;/h3&gt;

&lt;p&gt;This &lt;em&gt;minimum distance&lt;/em&gt; formulation is well known in Geometry, but we can derive it from &lt;em&gt;an optimization point of view&lt;/em&gt; as well. Let&#39;s start calculating the distance between a point $\bar{x}$ and the hyperplane $H(w,b)$. We can write down this minimization problem as:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ \min_{x \in \mathbb{R}^n} \quad ||\bar{x} - x|| \\
   \hspace{15pt}\textrm{s.t.} \quad w \cdot x + b = 0 \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;We can apply a strictly increasing transformation to our objective function (to get rid of the non-differentiability) without changing the global optima of our problem:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ \min_{x \in \mathbb{R}^n} \quad \frac{1}{2}||\bar{x} - x||^2 \\
   \hspace{2pt} \textrm{s.t.} \quad w \cdot x + b = 0 \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;We now have a strictly convex objective function subject to continuously differentiable linear constraints so we can apply the &lt;a href=&#34;https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions&#34;&gt;&lt;strong&gt;Karush–Kuhn–Tucker&lt;/strong&gt;&lt;/a&gt; optimality conditions:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ \nabla_x \bigg [ \frac{1}{2} ||\bar{x} - x||^2 + \lambda(w \cdot x + b) \bigg ] = 0 \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ \hspace{67pt}= (\bar{x} - x) + \lambda w = 0 \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Expressing $x = \bar{x} + \lambda w$ and substituting it into the constraint equation we obtain:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ w \cdot (\bar{x} + \lambda w) + b \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ \hspace{3pt} = w \cdot \bar{x} + \lambda ( w \cdot w) + b \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ \hspace{14pt}= w \cdot \bar{x} + \lambda ||w||^2 + b = 0 \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;From which we can isolate the lagrangean multiplier and write&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\lambda = -\frac{w \cdot \bar{x} + b}{||w||^2}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;We were originally interested in $||\bar{x} - x||$ and from the optimality conditions we can now re-write this quantity as:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[||\bar{x} - x|| = ||\lambda w|| = \frac{|w \cdot \bar{x} + b|}{||w||}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Which is the projection of a given point $\bar{x}$ on an hyperplane $H(w,b)$.&lt;/p&gt;

&lt;p&gt;We can now obtain the definition of margin given above using this result, extending the definition so that we are interested in the minimum distance between every point $x \in D$ to $H(w,b)$.&lt;/p&gt;

&lt;p&gt;It is quite intuitive that the margin of a given separating hyperplane is related to the
&lt;em&gt;generalization capability&lt;/em&gt; of the corresponding linear classifier. This relationship is analyzed in the &lt;a href=&#34;https://www.springer.com/us/book/9780387987804&#34;&gt;Statistical Learning Theory&lt;/a&gt;, which theoretically motivates the importance of defining
the hyperplane with maximum margin, the so-called &lt;strong&gt;optimal separating hyperplane&lt;/strong&gt;.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id=&#34;optimal-separating-hyperplane&#34;&gt;Optimal Separating Hyperplane&lt;/h2&gt;

&lt;p&gt;The optimal separating hyperplane is a separating hyperplane &lt;span  class=&#34;math&#34;&gt;\(H(w^*, b^*)\)&lt;/span&gt; having maximum margin:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ \max_{w \in \mathbb{R}^n, b \in \mathbb{R}} \gamma(w,b) = \max_{w \in \mathbb{R}^n, b \in \mathbb{R}} \min_{x \in D} \bigg \{ \frac{|w \cdot x + b|}{||w||} \bigg \}  \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This thing does not look good: it&#39;s a &lt;em&gt;max-min&lt;/em&gt; problem. We can re-write this quantity as&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\max_{\eta, w, b} \eta \\
\hspace{60pt}\text{s.t } \hspace{5pt} \eta \leq \frac{|w \cdot x + b|}{||w||} \\
\hspace{150pt}y(w \cdot x +b) \geq 1 \quad \quad \forall (x,y) \in D
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;since we are dealing with a &lt;strong&gt;finite&lt;/strong&gt; number of inequalities (this assures us that the optimal $\eta$ will satisfy the first constraint inequality as an equality).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This thing still does not look good&lt;/em&gt;: both the numerator and the denominator are &lt;em&gt;non-differentiable&lt;/em&gt;. It can be (quite easily) shown that the numerator $|w \cdot x + b|$ can be always set to $1$, leaving us with:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\max_{\eta, w, b} \eta \\
\hspace{35pt}\text{s.t } \hspace{5pt} \eta \leq \frac{1}{||w||} \\
\hspace{150pt}y(w \cdot x +b) \geq 1 \quad \quad \forall (x,y) \in D
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;As you can notice in the first constraint we passed from $|D|$ inequalities to only one, as we lost the dependance from $x$. We can re-write our problem as:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\max_{w, b} \frac{1}{||w||} \\
\hspace{120pt}\text{s.t } \hspace{10pt} y(w \cdot x +b) \geq 1 \quad \quad \forall (x,y) \in D
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;We get rid of the non-differentiability of the denominator by transforming this $max$ problem to a $min$ problem and applying a transformation to the objective function:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\max_{w, b} \frac{1}{2}||w||^2 \qquad \qquad (P)\\
\hspace{55pt}\text{s.t } \hspace{10pt} y(w \cdot x +b) \geq 1 \quad \quad \forall (x,y) \in D
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This is now a minimization problem of a quadratic function subject to linear constraints, which means that we can easily solve this. The difficulties we encounter are related to the &lt;em&gt;dimensions&lt;/em&gt; of the problem. Usually SVMs are &lt;a href=&#34;https://pdfs.semanticscholar.org/d1fa/8485ad749d51e7470d801bc1931706597601.pdf&#34;&gt;trained&lt;/a&gt; with a well-known technique: &lt;a href=&#34;http://web.cs.iastate.edu/~honavar/smo-svm.pdf&#34;&gt;Sequential Minimal Optimization&lt;/a&gt;.&lt;/p&gt;

&lt;hr&gt;

&lt;h1 id=&#34;lagrange-dual&#34;&gt;Lagrange Dual&lt;/h1&gt;

&lt;p&gt;The final step towards the solution of this problem is imposing the optimality conditions in the &lt;a href=&#34;https://en.wikipedia.org/wiki/Duality_(optimization)&#34;&gt;Lagrange Dual&lt;/a&gt;. This is the &lt;em&gt;classical&lt;/em&gt; approach to the solution but it&#39;s not the only one.&lt;/p&gt;

&lt;p&gt;Before doing that it&#39;s important to highlight that we can demonstrate the equivalency of the starting problem and the one we derived, but we are not going to do that.&lt;/p&gt;

&lt;p&gt;Let&#39;s relax the constraints and embed them into the objective function: the problem associated with the Lagrangian dual is&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\max_{\lambda \geq 0} \Big \{ \min_{w,b} \frac{1}{2} w^{\intercal}w + \sum_{i} \lambda_{i} \big[ 1 - y_i (w \cdot x_i + b) \big ] \Big \} = \\
\max_{\lambda \geq 0} \sum_i \lambda_i + \min_{w,b} \frac{1}{2} w^{\intercal}w - \sum_i \lambda_i y_i (w \cdot x_i + b) \quad \quad (\alpha)
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;which is a convex unconstrained optimization problem. Let&#39;s leave the $max$ apart for a moment and concentrate on the internal $min$ problem, we know that the optimality conditions are the nullification of the gradient of the Lagrangian:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\nabla \mathcal{L}(w,b) = \nabla_{w,b} \Big [ \min_{w,b} \frac{1}{2}w^\intercal w - \sum_i \lambda_i y_i(w \cdot x_i +b) \Big ] = 0
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;from which we have:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\nabla_w \mathcal{L}(w,b) = 0 \implies w - \sum_i \lambda_i x_i y_i = 0 \implies \boxed{w = \sum_i \lambda_i y_i x_i \quad} (1) \\
\nabla_b \mathcal{L}(w,b) = 0 \implies 0 - \sum_i \lambda_i y_i = 0 \implies \boxed{\sum_i \lambda_i y_i = 0} \hspace{35pt} (2)
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;We can now solve the $max$ problem. Plugging $(1)$ into $(\alpha)$ we obtain:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\max_{\lambda \geq 0} \sum_i \lambda_i + \frac{1}{2}\sum_i \sum_j \lambda_i \lambda_j y_i y_j x_i^\intercal x_j - \sum_i \lambda_i y_i \Big (\sum_j \lambda_j y_j x_j^\intercal \Big )x_i - \sum_i \lambda_i y_i b \quad (\alpha^{&#39;})
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Substituting $(2)$ into $(\alpha^{&#39;})$ and switching signs (in order to pass from a $max$ to a $min$ problem) we have:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\min_{\lambda \geq 0} \frac{1}{2} \sum_i \sum_j \lambda_i \lambda_j y_i y_j x_i^\intercal x_j - \sum_i \lambda_i \\
\hspace{-75pt}{\text{s.t. } \sum_i \lambda_i y_i = 0}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Setting &lt;span  class=&#34;math&#34;&gt;\(X = [y_1x_1 \dots y_nx_n]\)&lt;/span&gt;, &lt;span  class=&#34;math&#34;&gt;\(\lambda = [\lambda_1 \dots \lambda_n]^\intercal\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(y = [y_1 \dots y_n]\)&lt;/span&gt; we can finally write this in a compact form:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\min_{\lambda \geq 0} \frac{1}{2} \lambda^\intercal X^\intercal X \lambda - e^\intercal \lambda \qquad \qquad (L)\\
\hspace{-88pt}{\text{s.t } \hspace{10pt} \lambda ^\intercal y = 0}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where &lt;span  class=&#34;math&#34;&gt;\(e = [1 \dots 1]\)&lt;/span&gt;.&lt;/p&gt;

&lt;hr&gt;

&lt;h1 id=&#34;solution&#34;&gt;Solution&lt;/h1&gt;

&lt;p&gt;We can now either solve problem $(P)$ or $(L)$ indifferently. Suppose we solved problem $(L)$, obtaining a &lt;span  class=&#34;math&#34;&gt;\(\lambda^*\)&lt;/span&gt;. We can retrieve &lt;span  class=&#34;math&#34;&gt;\(w^*\)&lt;/span&gt; from $(1)$ as&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
w^* = \sum_i \lambda_i x_i y_i
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;An important fact, that comes from the KKT complementary slackness, is that the solution $w^*$ depends only on the so-called &lt;strong&gt;support vectors&lt;/strong&gt; &lt;span  class=&#34;math&#34;&gt;\(x_i\)&lt;/span&gt;, whose correspondent multipliers &lt;span  class=&#34;math&#34;&gt;\(\lambda_i^*\)&lt;/span&gt; are not null.&lt;/p&gt;

&lt;p&gt;Once computed &lt;span  class=&#34;math&#34;&gt;\(w^*\)&lt;/span&gt;, by considering any multiplier &lt;span  class=&#34;math&#34;&gt;\(\lambda_i^*&gt;0\)&lt;/span&gt; the scalar $b^*$ can be determined by means of the corresponding complementarity condition&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\lambda_i^* \big [ y_i(w^* \cdot x_i + b^*) - 1 \big ] = 0, \qquad \qquad i=1, \dots, n
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;When we&#39;ll be in possess of a new data point $\hat{x}$ we can decide if $\hat{x} \in P$ or $\hat{x} \in N$ by applying the &lt;em&gt;decision function&lt;/em&gt; $d(\cdot)$:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\boxed{
d(\hat{x}) = sgn(w^* \cdot \hat{x} + b^*) = sgn \Bigg (\sum_i^n \lambda_i^* y_i x_i \cdot \hat{x} + b^* \Bigg )}
\]&lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quantum Information - Part 1: Linear Algebra</title>
      <link>https:w00zie.github.io/post/quantum_1_linalg/</link>
      <pubDate>Tue, 23 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https:w00zie.github.io/post/quantum_1_linalg/</guid>
      <description>





&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https:w00zie.github.io/img/qm/bloch.jpg&#34; &gt;

&lt;img src=&#34;https:w00zie.github.io/img/qm/bloch.jpg&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;p&gt;In this first post we&#39;ll give a brief introduction to the necessary linear algebra for Quantum Mechanics.&lt;/p&gt;

&lt;h2 id=&#34;linear-algebra&#34;&gt;Linear Algebra&lt;/h2&gt;

&lt;h3 id=&#34;vectors&#34;&gt;Vectors&lt;/h3&gt;

&lt;p&gt;The basic objects of linear algebra are &lt;em&gt;vector spaces&lt;/em&gt;. The vector space of most interest in QM is $\mathbb{C}^n$, the space of all $n$-tuples of complex numbers $(z_1, \dots, z_n)$. The elements of a vector space are called &lt;em&gt;vectors&lt;/em&gt;. The standard quantum mechanical notation for a vector in a vector space is the following:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ \lvert \psi \rangle \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;A &lt;em&gt;spanning set&lt;/em&gt; for a vector space is a set of vectors &lt;span  class=&#34;math&#34;&gt;\(\lvert v_1 \rangle, \dots, \lvert v_n \rangle\)&lt;/span&gt; such that any vector &lt;span  class=&#34;math&#34;&gt;\(\lvert v \rangle\)&lt;/span&gt; can be written as a linear combination &lt;span  class=&#34;math&#34;&gt;\(\lvert v \rangle = \sum_{i} a_i \lvert v_i \rangle\)&lt;/span&gt; of vectors in that set. For example, a spanning set for the vector space $\mathbb{C}^2$ is the set:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ \lvert v_1 \rangle = \begin{bmatrix} 
1  \\
0 \\
\end{bmatrix}; \quad 
\lvert v_2 \rangle = 
\begin{bmatrix}
0 \\
1 \\
\end{bmatrix}, \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;since any vector&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ \lvert v \rangle = 
\begin{bmatrix}
a_1 \\
a_2 \\
\end{bmatrix} \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;can be expressed as a linear combination $\lvert v \rangle = a_1 \lvert v_1 \rangle + a_2 \lvert v_2 \rangle$ of the vectors $\lvert v_1 \rangle$ and $\lvert v_2 \rangle$.&lt;/p&gt;

&lt;p&gt;Generally, a vector space may have many different spanning sets. A second spanning set for $\mathbb{C}^2$ is the set:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ \lvert v_1 \rangle = \frac{1}{\sqrt2}
\begin{bmatrix}
1 \\
1 \\
\end{bmatrix}; \quad 
\lvert v_2 \rangle = \frac{1}{\sqrt{2}}
\begin{bmatrix}
1 \\
-1 \\
\end{bmatrix}, \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;A set of non-zero vectors $\lvert v_1 \rangle, \dots, \lvert v_n \rangle$ are &lt;em&gt;linearly dependent&lt;/em&gt; if there exists a set of complex numbers $a_1, \dots, a_n$ with $a_i \neq 0$ for at least one value of $i$, such that:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ a_1 \lvert v_1 \rangle + a_2 \lvert v_2 \rangle + \dots a_n \lvert v_n \rangle = 0 \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;A set of vectors is &lt;em&gt;linearly independent&lt;/em&gt; if it is not linearly dependent. A set of linearly independent vectors form a &lt;em&gt;basis&lt;/em&gt; for their relative vector space.&lt;/p&gt;

&lt;h3 id=&#34;linear-operators-and-matrices&#34;&gt;Linear operators and matrices&lt;/h3&gt;

&lt;p&gt;A &lt;em&gt;linear operator&lt;/em&gt; between vector spaces $V$ and $W$ is defined to be any function $A:V \to W$ which is linear in its inputs:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ A \bigg ( \sum_{i} a_i \lvert v_i \rangle \bigg ) = \sum_{i} a_i A(\lvert v_i \rangle)\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;And we just write $A \lvert v \rangle$ to denote this application.&lt;/p&gt;

&lt;p&gt;The most convenient way to understand linear operators is in terms of their &lt;em&gt;matrix representations&lt;/em&gt;. In fact, the linear operator and matrix viewpoints turn out to be completely equivalent.&lt;/p&gt;

&lt;p&gt;Suppose $\lvert v_1 \rangle, \dots, \lvert v_m \rangle$ is a basis for &lt;span  class=&#34;math&#34;&gt;\(V\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(\lvert w_1 \rangle, \dots, \lvert w_n \rangle\)&lt;/span&gt; is a basis for &lt;span  class=&#34;math&#34;&gt;\(W\)&lt;/span&gt;. Then $\forall j = {1, \dots, m }$ there exists complex numbers $A_{1j}, \dots, A_{nj}$ such that:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ A \lvert v_j \rangle = \sum_{i} A_{ij} \lvert w_i \rangle \]&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&#34;inner-products&#34;&gt;Inner products&lt;/h3&gt;

&lt;p&gt;An &lt;em&gt;inner product&lt;/em&gt; is a function which takes as input two vectors $\lvert v \rangle$ and $\lvert w \rangle$ from a vector space and produces a complex number as output. The standard notation for the inner product is $\langle v \lvert w \rangle$ where $\langle v \rvert$ is the &lt;em&gt;dual vector&lt;/em&gt; to the vector $\lvert v \rangle$. A function $(\cdot , \cdot): V \times V \to \mathbb{C}$ is an inner product if it satisfies:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Linearity in the second argument:
&lt;span  class=&#34;math&#34;&gt;\( \bigg (\lvert v \rangle, \sum_{i} \lambda_i \lvert w_i \rangle \bigg ) = \sum_{i} \lambda_{i} (\lvert v \rangle, \lvert w_i \rangle) \)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;$(\lvert v \rangle , \lvert w \rangle) = (\lvert w \rangle, \lvert v \rangle)^*$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;$(\lvert v \rangle, \lvert v \rangle) \geq 0$ with equality iff $\lvert v \rangle = 0$.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For example $\mathbb{C}^n$ has an inner product defined by:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ ((y_1, \dots, y_n ), (z_1, \dots z_n)) = \sum_{i} y^*_{i}z_i \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;We call a vector space with an inner product an &lt;em&gt;inner product space&lt;/em&gt; or (usually) an &lt;strong&gt;Hilbert space&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;We define the &lt;em&gt;norm&lt;/em&gt; of a vector $\lvert v \rangle$ by:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ || \lvert v \rangle || = \sqrt{\langle v \lvert v \rangle}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;For any non-zero vector $\lvert v \rangle$ we talk about &lt;em&gt;normalization&lt;/em&gt; or &lt;em&gt;normalized form&lt;/em&gt; the vector $\lvert v \rangle / || \lvert v \rangle ||$.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;With these conventions, the inner product on a Hilbert space can be given a convenient matrix representation. Let $\lvert w \rangle = \sum_i w_i \lvert i \rangle$ and &lt;span  class=&#34;math&#34;&gt;\(\lvert v \rangle = \sum_i v_i \lvert i \rangle\)&lt;/span&gt; be two vectors defined on an &lt;strong&gt;orthonormal&lt;/strong&gt; basis $\lvert i \rangle$ then, since $\langle i \lvert j \rangle = \delta_{ij}$ we have:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ \langle v \lvert w \rangle = \Big ( \sum_i v_i \lvert i \rangle, \sum_j w_j\lvert j \rangle \Big ) = \sum_{ij} v_i^*w_j \delta_{ij} = \sum_i v_i^*w_i = \\
\begin{bmatrix} v_1^* \dots, v_n^* \end{bmatrix} \begin{bmatrix} w_1 \\ \vdots \\ w_n \end{bmatrix}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;That is, the inner product of two vectors is equal to the vector inner product between two matrix representations of those vectors, provided the representations are written with respect to the same orthonormal basis.&lt;/p&gt;

&lt;p&gt;We also see that the dual vector $\langle v \lvert$ has a
nice interpretation as the row vector whose components are complex conjugates of the corresponding components of the column vector representation of $\lvert v \rangle$.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
