<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Giovanni Bindi</title>
    <link>https://w00zie.github.io/post/</link>
      <atom:link href="https://w00zie.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 01 Aug 2020 16:46:13 +0200</lastBuildDate>
    <image>
      <url>https://w00zie.github.io/media/giova.jpg</url>
      <title>Posts</title>
      <link>https://w00zie.github.io/post/</link>
    </image>
    
    <item>
      <title>Music generation with Wasserstein Autoencoders</title>
      <link>https://w00zie.github.io/post/wae/</link>
      <pubDate>Sat, 01 Aug 2020 16:46:13 +0200</pubDate>
      <guid>https://w00zie.github.io/post/wae/</guid>
      <description>&lt;div align=&#34;center&#34;&gt; &lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#the-data&#34;&gt;The data&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#theory&#34;&gt;Theory&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#wae-gan&#34;&gt;WAE-GAN&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#wae-mmd&#34;&gt;WAE-MMD&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#experiments-details&#34;&gt;Experiments details&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#architechtures&#34;&gt;Architechtures&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#prior&#34;&gt;Prior&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#latent-space-dimensionality&#34;&gt;Latent space dimensionality&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#adversarial-training&#34;&gt;Adversarial training&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#cost-function&#34;&gt;Cost function&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#hyperparameters&#34;&gt;Hyperparameters&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#results&#34;&gt;Results&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#survey&#34;&gt;Survey&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
 &lt;/div&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is a project I started working on for my Machine Learning class. The aim, as you can read from the title, is being able to generate music that is, at least, not unpleasant.&lt;/p&gt;
&lt;p&gt;In order to do so I investigated the use of Wasserstein Autoencoders (WAE), a generative model firstly proposed by Tolstikhin &lt;em&gt;et al.&lt;/em&gt; &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, which employs a regularizer derived from the theory of optimal transport.&lt;/p&gt;
&lt;h2 id=&#34;the-data&#34;&gt;The data&lt;/h2&gt;
&lt;p&gt;The data I used was published by a group of researchers from the 
&lt;a href=&#34;https://musicai.citi.sinica.edu.tw/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academia Sinica&lt;/a&gt; (Taiwan), as part of a paper&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; where they jointly proposed a novel GAN (MuseGAN) and released the dataset where the model was trained on. They open-sourced almost everything so you can access to their 
&lt;a href=&#34;https://github.com/salu133445/musegan&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt; and 
&lt;a href=&#34;https://salu133445.github.io/musegan/data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;data&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This dataset contains 174154 unique 4/4 multi-track music pieces of 4 bars, coming from different rock music compositions. Every element of this dataset is represented as a multi-track (binary-valued) pianoroll. For those of you who are not familiar with DAWs or audio processing, a single-track pianoroll is basically a matrix whose entries represent the behaviour of the notes played by the single instrument. The number of columns in this matrix is the number of time-steps used to quantize the composition while the number of rows represents the octave extension, i.e. the number of possible notes that the instrument can execute.&lt;/p&gt;






  
  











&lt;figure id=&#34;figure-example-of-a-pianoroll&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://w00zie.github.io/post/wae/img/pianoroll_hu0665c07db140e9b058b96a7d644ee03b_63264_2000x2000_fit_q90_lanczos.jpeg&#34; data-caption=&#34;Example of a pianoroll&#34;&gt;


  &lt;img data-src=&#34;https://w00zie.github.io/post/wae/img/pianoroll_hu0665c07db140e9b058b96a7d644ee03b_63264_2000x2000_fit_q90_lanczos.jpeg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;800&#34; height=&#34;600&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Example of a pianoroll
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In this dataset a single bar is quantized into 48 time-steps (the minimum duration of a note is then 1/48th) and can span between 84 notes from C1 to B7. Every element of the dataset contains 4 bars of 5 instruments, resulting in a binary-valued (note on/off - velocity information is discarded) tensor of shape (4*48, 84, 5) = (192, 84, 5). The tempo was set to 100 bpm and the authors transposed every element to the same tonality.
The instruments involved are &lt;em&gt;Drums, Bass, Guitar, Piano&lt;/em&gt; and &lt;em&gt;Strings&lt;/em&gt;.&lt;/p&gt;






  
  











&lt;figure id=&#34;figure-one-element-of-the-dataset-4-bars-of-5-instruments-best-with-the-white-theme&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://w00zie.github.io/post/wae/img/mydata_hud05689e6339d7b054e3f596aaa3e4ce0_289573_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;One element of the dataset: 4 bars of 5 instruments (best with the white theme)&#34;&gt;


  &lt;img data-src=&#34;https://w00zie.github.io/post/wae/img/mydata_hud05689e6339d7b054e3f596aaa3e4ce0_289573_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1442&#34; height=&#34;1634&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    One element of the dataset: 4 bars of 5 instruments (best with the white theme)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The pianoroll is one of the most commonly used representations, although it has some limitations. An important one, compared to the MIDI representation, is that there is no &lt;em&gt;note-off&lt;/em&gt; information. As a result, there is no way to distinguish between a long note and repeated short notes.
In order to partially solve this issue the authors did as following&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that during the conversion from MIDI files to pianorolls, an additional minimal-length (of one time step) pause is added between two consecutive (without a pause) notes of the same pitch to distinguish them from one single note.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A few samples from the dataset&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; (&lt;strong&gt;lower your audio output&lt;/strong&gt;):&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt; 
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/orig/orig_1.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
 &lt;/div&gt;
&lt;div align=&#34;center&#34;&gt; 
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/orig/orig_2.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
 &lt;/div&gt;
&lt;div align=&#34;center&#34;&gt; 
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/orig/orig_3.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
 &lt;/div&gt;
&lt;div align=&#34;center&#34;&gt; 
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/orig/orig_4.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
 &lt;/div&gt;
&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: &lt;em&gt;a lot of this theory is taken from the original paper&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, I did not re-write it from scratch&lt;/em&gt;!&lt;/p&gt;
&lt;p&gt;WAEs are a class of Autoencoders derived by theory of 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Transportation_theory_%28mathematics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;optimal transport&lt;/a&gt;. Given a collection of (unlabelled) data points $S_X$, the ultimate goal of the task is to tune a model capable of generating sets of synthetic points $S_G$ which look similar to $S_X$. There are various ways of defining the notion of similarity between two sets of data points: the most common approach assumes that both $S_X$ and $S_G$ are sampled independently from two probability distributions $P_X$ and $P_G$ respectively, and employ some of the known divergence measures for distributions.&lt;/p&gt;
&lt;p&gt;Two major approaches currently dominate this field.
Variational Auto-Encoders (VAEs) minimize the Kullback-Leibler (KL) divergence
$D_{KL}(P_X, P_G)$, which is equivalent to maximizing the marginal log-likelihood of the model $P_G$. Generative Adversarial Networks (GANs) employ an elegant framework, commonly referred to as adversarial training, which is suitable for many different divergence measures, including (but not limited to) $f-$divergences, 1-Wasserstein distance, and Maximum Mean Discrepancy (MMD).&lt;/p&gt;
&lt;p&gt;Similarly to VAEs, WAEs describe a particular way to train probabilistic &lt;em&gt;latent variable models&lt;/em&gt; (LVMs) $P_G$. LVMs act by first sampling a code (feature) vector $Z$ from a prior distribution $P_Z$ defined over the latent space $\mathcal{Z}$ and then mapping it to a random input point $X \in \mathcal{X}$ using a conditional distribution $P_G(X|Z)$ also known as the &lt;em&gt;decoder&lt;/em&gt;. This results is a density of the form&lt;/p&gt;
&lt;p&gt;$$
p_G(x) = \int_{\mathcal{Z}} p_G(x|z)p(z)dz
$$&lt;/p&gt;
&lt;p&gt;assuming all involved densities are properly defined.&lt;/p&gt;
&lt;p&gt;Instead of minimizing the KL divergence between the LVM $P_G$ and the unknown data distribution $P_X$ as done by VAEs, WAEs aim at minimizing any optimal transport distance between them. Kantorovich&amp;rsquo;s formulation of the optimal transport problem (OT) is given by&lt;/p&gt;
&lt;p&gt;$$
W_c(P_X, P_G) = \inf_{\Gamma \in \mathcal{P}(X \sim P_X, Y \sim P_G)} \mathbb{E}_{(X,Y) \sim \Gamma} [c(X,Y)]
$$&lt;/p&gt;
&lt;p&gt;where $c:\mathcal{X} \times \mathcal{Y} \to \mathbb{R}^+$ is any measurable cost function and $\mathcal{P}(X \sim P_X, Y \sim P_G)$ is the set of all joint distributions of $(X, Y)$ with marginals $P_X$ and $P_G$ respectively. When $(\mathcal{X}, d)$ is a metric space (with distance $d$) and $c(x,y) = d^p(x,y)$ for $p \geq 1$ then $W_p$ (the $p$-th root of $W_c$) is known as the $p$-&lt;em&gt;Wasserstein distance&lt;/em&gt;. When $p=1$ we then have the $1$-Wasserstein distance and this duality holds (Kantorovich-Rubinstein duality)&lt;/p&gt;
&lt;p&gt;$$
W_1(P_X, P_G) = \sup_{f \in \mathcal{F}_L} \mathbb{E}_{X \sim P_X} [f(X)] - \mathbb{E}_{Y \sim P_G}[f(Y)]
$$&lt;/p&gt;
&lt;p&gt;where $\mathcal{F}_L$ is the set of all bounded 1-Lipschitz functions on $(\mathcal{X}, d)$.
The authors state (and prove) that for deterministic decoders, i.e. $P_G(X|Z=z) = \delta_{G(z)}$ deterministically mapping latent codes $z \in \mathcal{Z}$ to the input space for a given map $G: \mathcal{Z} \to \mathcal{X}$, there is a simpler formulation for the OT problem: instead of finding a coupling $\Gamma$ between two random variables in $\mathcal{X}$ it is sufficient to find a conditional distribution $Q(Z|X)$ such that its $Z$ marginal $Q(Z) = \mathbb{E}_{X \sim P_X} [Q(Z|X)]$ is identical to the prior distribution $P_Z$. This resulted in a theorem where the OT problem was re-formulated (under these assumptions) as&lt;/p&gt;
&lt;p&gt;$$
W_c(P_X, P_G) = \inf_{Q:Q_Z = P_Z} \mathbb{E}_{P_X}\big [ \mathbb{E}_{Q(Z|X)}[c(X, G(Z))] \big ]
$$&lt;/p&gt;
&lt;p&gt;In order to implement a numerical solution to this problem the authors relax the constraint on $Q_Z$ by adding a penalty term to the objective, which, in the end, takes the following form&lt;/p&gt;
&lt;p&gt;$$
D_{WAE}(P_X, P_G) = \inf_{Q(Z|X) \in \mathcal{Q}} \mathbb{E}_{P_X}\big [ \mathbb{E}_{Q(Z|X)}[c(X, G(Z))] \big ] + \lambda \mathcal{D}_Z(Q_Z, P_Z)
$$&lt;/p&gt;
&lt;p&gt;where $\mathcal{Q}$ is any nonparametric set of probabilistic encoders, $\mathcal{D}_Z$ is an arbitrary divergence between $Q_Z$ and $P_Z$ , and $\lambda &amp;gt; 0$ is a hyperparameter. The authors propose two implementations of the WAE model, based on two different divergences.&lt;/p&gt;
&lt;h3 id=&#34;wae-gan&#34;&gt;WAE-GAN&lt;/h3&gt;
&lt;p&gt;The WAE-GAN model employs the Jensen-Shannon divergence, i.e.&lt;/p&gt;
&lt;p&gt;$$
\mathcal{D}_Z(Q_Z, P_Z) = \mathcal{D}_{JS}(Q_Z, P_Z) = \frac{1}{2} D_{KL}(P_Z, M) + \frac{1}{2} D_{KL}(Q_Z, M)
$$&lt;/p&gt;
&lt;p&gt;where $M = \frac{1}{2}(P_Z + Q_Z)$ and uses the adversarial training to estimate it. The authors introduce an adversary (discriminator) in the latent space $\mathcal{Z}$ trying to separate &amp;ldquo;true&amp;rdquo; points sampled from $P_Z$ and &amp;ldquo;fake&amp;rdquo; ones sampled from $Q_Z$.&lt;/p&gt;
&lt;h3 id=&#34;wae-mmd&#34;&gt;WAE-MMD&lt;/h3&gt;
&lt;p&gt;For a positive-definite reproducing kernel $k : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ the following expression is called the &lt;em&gt;Maximum Mean Discrepancy&lt;/em&gt; (MMD):&lt;/p&gt;
&lt;p&gt;$$
\text{MMD}_k(P_Z, Q_Z) = \Big | \Big | \int_\mathcal{Z} k(z, \cdot) dP_Z(z) - \int_\mathcal{Z} k(z, \cdot) dQ_Z(z) \Big | \Big |_{\mathcal{H}_k}
$$&lt;/p&gt;
&lt;p&gt;where $\mathcal{H}_k$ is the RKHS of real-valued functions mapping $\mathcal{Z}$ to $\mathcal{R}$. If $k$ is &lt;em&gt;characteristic&lt;/em&gt; then MMD$_k$
defines a metric and can be used as a divergence measure. The authors propose to use an unbiased estimate of the MMD that is the sum of two $U$-statistics and a sample average.
Given i.i.d. samples $\mathbf{P} = { z_1, \dots, z_n }$ from $P_Z$ and i.i.d. samples $\mathbf{Q} = {\tilde{z}_1, \dots, \tilde{z}_n}$ from $Q(Z|z_i)$ , the unbiased estimate&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; is&lt;/p&gt;
&lt;p&gt;$$
\widehat{MMD}^2_U \big [\mathcal{H}_k, \mathbf{P}, \mathbf{Q} \big ] = \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j \neq i} k(z_i, z_j) + \sum_{i=1}^n \sum_{j \neq i} k(\tilde{z}_i, \tilde{z}_j) - \frac{2}{n^2} \sum_{i=1}^n \sum_{j=1}^n k(z_i, \tilde{z}_j)
$$&lt;/p&gt;
&lt;h2 id=&#34;experiments-details&#34;&gt;Experiments details&lt;/h2&gt;
&lt;h3 id=&#34;architechtures&#34;&gt;Architechtures&lt;/h3&gt;
&lt;p&gt;Encoder $Q_\phi$, decoder $G_{\theta}$ and discriminator $D_{\gamma}$ are parametrized by deep neural nets. Given the structure of the data, binary-valued tensors of shape (H,W,5) (where 5, the number of instruments, plays naturally the role of the number of channels), both encoder and decoder are CNNs, while the discriminator (which acts in the latent space $\mathcal{Z}$) is a &amp;ldquo;vanilla&amp;rdquo; NN. You can check out the various models I&amp;rsquo;ve coded 
&lt;a href=&#34;https://github.com/w00zie/wae_music&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, they are almost all equals except for the strides of the convolutions.&lt;/p&gt;
&lt;p&gt;The main architecture I used was inspired by DCGAN &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;. In the following picture I&amp;rsquo;m showing the encoder that, starting from the (192, 84, 5) input tensor, applies 4 convolutions with $[6,6]$ filters, followed by Batch Normalizations and ReLUs. The last layer employs a reshape (not shown in the picture) followed by a fully-connected layer that maps the previous inputs into a vector of size $dim(\mathcal{Z}) = d_{\mathcal{Z}}$ (in this case I went with $d_{\mathcal{Z}} = 8$).&lt;/p&gt;






  
  











&lt;figure id=&#34;figure-my-encoder-made-with-nn-svghttpsalexlenailmenn-svg&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://w00zie.github.io/post/wae/img/encoder_hu03db60d98e0fb8a5cd64c7aee1dc2862_38313_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;My encoder (made with &amp;lt;a href=&amp;#34;https://alexlenail.me/NN-SVG/&amp;#34;&amp;gt;NN-SVG&amp;lt;/a&amp;gt;)&#34;&gt;


  &lt;img data-src=&#34;https://w00zie.github.io/post/wae/img/encoder_hu03db60d98e0fb8a5cd64c7aee1dc2862_38313_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1024&#34; height=&#34;586&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    My encoder (made with &lt;a href=&#34;https://alexlenail.me/NN-SVG/&#34;&gt;NN-SVG&lt;/a&gt;)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The decoder acts simmetrically wrt the encoder, using 
&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2D transposed convolutions&lt;/a&gt; instead of convolutions. The discriminator network maps a vector from $\mathcal{Z}$ to $[0,1]$ through 6 dense layers of 512 neurons followed by ReLUs.&lt;/p&gt;
&lt;h3 id=&#34;prior&#34;&gt;Prior&lt;/h3&gt;
&lt;p&gt;I&amp;rsquo;ve used a Gaussian prior $P_Z = \mathcal{N} \big ( 0, \sigma_z^2 \cdot I_{d_z} \big )$ where the variance $\sigma_z^2$ is an hyperparameter.&lt;/p&gt;
&lt;h3 id=&#34;latent-space-dimensionality&#34;&gt;Latent space dimensionality&lt;/h3&gt;
&lt;p&gt;The authors studied the choice of the dimensionality of the latent space in a follow-up paper&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;. They investigated the relationship between the latent space dimension $d_{\mathcal{Z}}$ and the *intrinsic dimensionality* of the data distribution, $d_{\mathcal{I}}$. This last quantity is, informally, *the minimum number of parameters required to continuously parametrise the data manifold*. If this quantity is greater than $d_{\mathcal{Z}}$ then the encoder will map the data distribution $P_X$ to $Q_Z$ supported on a latent manifold of dimension at most $d_{\mathcal{I}}$ while the divergence regularizer $\lambda \mathcal{D}_Z (Q_Z, P_Z)$ will encourage the encoder to fill the latent space similarly to the prior $P_Z$ as much as possible. This can potentially lead to the absence of any learning capability.&lt;/p&gt;
&lt;h3 id=&#34;adversarial-training&#34;&gt;Adversarial training&lt;/h3&gt;
&lt;p&gt;I&amp;rsquo;ve decided to use (as the authors suggest) the non-saturating GAN loss &lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;: this resulted in a discriminator $D_\gamma$ trained by the maximization of&lt;/p&gt;
&lt;p&gt;$$
\frac{\lambda}{n} \sum_{i=1}^n \log D_\gamma(z_i) + \log D_\gamma(1-\tilde{z}_i)
$$&lt;/p&gt;
&lt;p&gt;and in an autoencoder ($Q_\phi, G_\theta$) trained by the minimization of&lt;/p&gt;
&lt;p&gt;$$
\frac{1}{n}\sum_{i=1}^n c(z_i, G_\theta(\tilde{z}_i)) - \lambda D_\gamma \log (\tilde{z}_i)
$$&lt;/p&gt;
&lt;p&gt;As the authors point out this procedure falls into the min-max problem which makes GAN unstable. This training strategy can then possibly yield instabilities in the training phase but the authors claim that by executing an adversarial training in a &amp;ldquo;simpler&amp;rdquo; space (wrt the pixel space where GANs are usually trained on), the whole process could benefit.&lt;/p&gt;
&lt;h3 id=&#34;cost-function&#34;&gt;Cost function&lt;/h3&gt;
&lt;p&gt;I&amp;rsquo;ve used $c(x, y) = || x - y ||_2^2$ for both implementations (WAE-GAN and WAE-MMD).&lt;/p&gt;
&lt;h3 id=&#34;hyperparameters&#34;&gt;Hyperparameters&lt;/h3&gt;
&lt;p&gt;Since there is no objective measure of performace for this kind of task, running an hyperparameter optimization procedure is hard. I&amp;rsquo;ve adjusted the parameters manually and so the results I obtained might not be the best that these model can produce.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;As mentioned in the data section, the input samples are binary-valued tensors of shape (192, 84, 5). The decoder, although, maps a latent code into $[0, 1]^{192 \times 84 \times 5}$ through the action of the sigmoid function. These output tensors $\hat{x}$ must then be &lt;em&gt;binarized&lt;/em&gt;, and I&amp;rsquo;ve used two strategies to do so:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Hard thresholding (HT): The resulting tensor entry $\hat{x}_{ijk}$ needs to be greater than a fixed threshold (0.5) in order to be mapped to 1.
$$\hat{x}_{ijk}^{HT} = \begin{cases} 1 &amp;amp; \text{if } \hspace{10pt} x_{ijk} &amp;gt; 0.5 \\&lt;br&gt;
0 &amp;amp; \text {o.w }\end{cases}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bernoulli sampling (BS): The resulting tensor entry $\hat{x}_{ijk}$ needs to be greater than a sample from the uniform distribution over scalars between 0 and 1.
$$\hat{x}_{ijk}^{BS} = \begin{cases} 1 &amp;amp; \text{if } \hspace{10pt} x_{ijk} &amp;gt; b \sim \mathcal{U}[0,1] \\&lt;br&gt;
0 &amp;amp; \text {o.w }\end{cases}$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I&amp;rsquo;m presenting samples binarized with the HT method.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;For a &amp;ldquo;fair&amp;rdquo; comparison the following results have been obtained with the same architecture and hyperparameters. Both WAE-GAN and WAE-MMD models have been trained with a latent dimension of $d_{\mathcal{Z}} = 16$. The number of filters in the four convolutions (see the picture above) are [16, 32, 64, 128] for the encoder and [128, 64, 32, 16] for the decoder. Both sequences of convolution used a fixed filter size of 6x6. I&amp;rsquo;ve trained both models for 300 epochs with Adam. The autoencoder&amp;rsquo;s optimizer started with a learning rate of $10^{-3}$ while the discriminator optimizer (in WAE-GAN) started with a learning rate of $10^{-4}$.&lt;/p&gt;
&lt;p&gt;The following are some of the &amp;ldquo;best&amp;rdquo; samples (wrt my personal taste) that I picked: please &lt;strong&gt;make sure to lower the level of your audio output&lt;/strong&gt;&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; before playing! I suggest listening with headphones.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;div align=&#34;center&#34;&gt; WAE-GAN &lt;/div&gt;&lt;/th&gt;
&lt;th&gt;&lt;div align=&#34;center&#34;&gt; WAE-MMD &lt;/div&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waegan/035_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waemmd/032_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waegan/026_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waemmd/009_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waegan/031_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waemmd/010_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waegan/045_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waemmd/021_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waegan/050_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waemmd/000_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waegan/008_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waemmd/033_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waegan/040_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waemmd/038_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waegan/052_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waemmd/057_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waegan/011_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waemmd/062_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waegan/059_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waemmd/063_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The next samples, instead, are randomly picked. Some of them totally lack the basic musical structures.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;div align=&#34;center&#34;&gt; WAE-GAN &lt;/div&gt;&lt;/th&gt;
&lt;th&gt;&lt;div align=&#34;center&#34;&gt; WAE-MMD &lt;/div&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/random/waegan/016_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/random/waemmd/025_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/random/waegan/023_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/random/waemmd/028_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/random/waegan/033_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/random/waemmd/035_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/random/waegan/041_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/random/waemmd/006_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/random/waegan/047_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/random/waemmd/039_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/random/waegan/053_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/random/waemmd/050_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The samples I was able to generete suffer the problem of &lt;strong&gt;over-fragmentation&lt;/strong&gt;. The notes generated tend to be fragmented, sometimes yielding chaotic and unpleasant results. This problem has also been recognized in &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; and the authors of MuseGAN proposed a new version of the model where their generator was equipped with binary neurons&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;, making it able to map the input noise directly into binary-valued output tensors. This partially solved the issue.&lt;/p&gt;
&lt;p&gt;The following samples come from a different set of WAE-GAN models I&amp;rsquo;ve trained (doubling the number of filters for each convolutional layer, using $d_{\mathcal{Z}} = 128$ and more epochs), I&amp;rsquo;ve cherry picked them hence they are not the &amp;ldquo;average&amp;rdquo; case, but the drums seem to be more predominant wrt the &amp;ldquo;simpler&amp;rdquo; models proposed above.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;div align=&#34;center&#34;&gt; WAE-GANs &lt;/div&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/general/001_HD_converted_wae_gan_27.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/general/027_HD_converted_wae_gan_04.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/general/002_HD_converted_wae_gan_27.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/general/033_HD_converted_wae_gan_04.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/general/005_HD_converted_wae_gan_04.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/general/039_HD_converted_wae_gan_05.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/general/006_HD_converted_wae_gan_04.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/general/050_HD_converted_wae_gan_05.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/general/011_HD_converted_wae_gan_27.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/general/055_HD_converted_wae_gan_06.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/general/026_HD_converted_wae_gan_05.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/general/062_HD_converted_wae_gan_27.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;survey&#34;&gt;Survey&lt;/h2&gt;
&lt;p&gt;TODO&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1711.01558&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wasserstein Auto-Encoders&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1709.06298&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment&lt;/a&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;I&amp;rsquo;ve done the MIDI to MP3 conversions with the help of VLC and its integrated synthesizer 
&lt;a href=&#34;http://www.fluidsynth.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FluidSynth&lt;/a&gt;. I&amp;rsquo;ve done no post-processing (eq, fx, mixing, mastering) so what you hear comes straight out-of-the box. The sounds produced are unpleasant: please think of them as part of an experiment and not music meant to be played. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1605.09522&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kernel Mean Embedding of Distributions:
A Review and Beyond&lt;/a&gt; page 52 &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1511.06434&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks&lt;/a&gt; &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1802.03761&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On the Latent Space of Wasserstein Auto-Encoders&lt;/a&gt; &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1711.10337&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Are GANs Created Equal? A Large-Scale Study&lt;/a&gt; &lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1804.09399&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Convolutional Generative Adversarial Networks with Binary Neurons for Polyphonic Music Generation&lt;/a&gt; &lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
