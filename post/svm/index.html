<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.4.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Giovanni Bindi">

  
  
  
    
  
  <meta name="description" content="A mathematical tour of SVM: linear classification.">

  
  <link rel="alternate" hreflang="en-us" href="https:w00zie.github.io/post/svm/">

  


  

  
  
  
  <meta name="theme-color" content="#4caf50">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.ab256391e26eaad5a122d3843a5dbe21.css">

  

  
  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https:w00zie.github.io/post/svm/">

  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Giovanni Bindi">
  <meta property="og:url" content="https:w00zie.github.io/post/svm/">
  <meta property="og:title" content="SVM explained - Part 1: Binary linear classification. | Giovanni Bindi">
  <meta property="og:description" content="A mathematical tour of SVM: linear classification."><meta property="og:image" content="https:w00zie.github.io">
  <meta property="twitter:image" content="https:w00zie.github.io"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-08-02T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2019-08-02T00:00:00&#43;00:00">
  

  


  





  <title>SVM explained - Part 1: Binary linear classification. | Giovanni Bindi</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/"><img src="/img/giova.jpg" alt="Giovanni Bindi"></a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#people"><span>Interesting stuff</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">SVM explained - Part 1: Binary linear classification.</h1>

  

  
    



<meta content="2019-08-02 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2019-08-02 00:00:00 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    <time>Aug 2, 2019</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    8 min read
  </span>
  

  
  
  

  
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https:w00zie.github.io/post/svm/&amp;text=SVM%20explained%20-%20Part%201:%20Binary%20linear%20classification." target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https:w00zie.github.io/post/svm/&amp;t=SVM%20explained%20-%20Part%201:%20Binary%20linear%20classification." target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=SVM%20explained%20-%20Part%201:%20Binary%20linear%20classification.&amp;body=https:w00zie.github.io/post/svm/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https:w00zie.github.io/post/svm/&amp;title=SVM%20explained%20-%20Part%201:%20Binary%20linear%20classification." target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=SVM%20explained%20-%20Part%201:%20Binary%20linear%20classification.%20https:w00zie.github.io/post/svm/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https:w00zie.github.io/post/svm/&amp;title=SVM%20explained%20-%20Part%201:%20Binary%20linear%20classification." target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      





<figure>

  <a data-fancybox="" href="/img/svm/front.jpg" >

<img src="/img/svm/front.jpg" >
</a>

</figure>


<p>One of the most known and widely used learning model is <strong>SVM</strong> (<a href="https://en.wikipedia.org/wiki/Support-vector_machine">Support Vector Machine</a>).</p>

<p>In this first post we are going to analyze the formal structure of the problem and we are going through the (simple) math needed to write down the SVM equations for linear case. In the <a href="https://w00zie.github.io/post/svm_2/">next post</a> we'll be extending this to the non-linear behaviour, introducing the concept of <strong>kernels</strong>.</p>

<p>Let's start with definitions.</p>

<h1 id="problem-structure">Problem Structure</h1>

<p>We are given a set of couples, named $D$, which is going to be our data set.</p>

<p><span  class="math">\[ D = \{ (x_i, y_i) : x_i \in X \subseteq \mathbb{R}^n, y_i \in Y, i=1,\dots,N \}\]</span></p>

<p>The $x_i$ are usually referred as <em>observations</em> while the $y_i$ are called <em>labels</em> or <em>targets</em>. We have $|D|=N$ data points.</p>

<p>If $Y = \mathbb{R}$ we are dealing with <em>regression</em>, otherwise with <em>classification</em>. For this walkthrough we will deal with classification and so we'll adopt, without loss of generality, <span  class="math">\(Y = \{+1, -1\}\)</span>.</p>

<hr>

<h2 id="linearly-separable-sets">Linearly separable sets</h2>

<p>This is the simplest case: we have two disjoint sets, namely $P$ and $N$:</p>

<p><span  class="math">\[ P = \{ x_i \in D: y_i = +1 \} \\
   N = \{ x_i \in D: y_i = -1 \} \]</span></p>

<p>that are <em>linearly separable</em>. This condition is expressed through the existence of a <strong>separating hyperplane</strong></p>

<p><span  class="math">\[ H(w,b) = \{ x \in \mathbb{R}^n : w \cdot x + b = 0 \} \]</span></p>

<p>such that the points $x_i \in P$ belong to an half space and the points $x_i \in N$ to the other one. Formally $\exists \hspace{3pt} w \in \mathbb{R}^n, b \in \mathbb{R}$:</p>

<p><span  class="math">\[ w \cdot x_i + b \geq \epsilon, \hspace{15pt} \forall x_i \in P \\
 w \cdot x_j + b \leq -\epsilon, \hspace{15pt} \forall x_j \in N \]</span></p>

<p>with $\epsilon &gt; 0$. Dividing by $\epsilon$ both members of both equations we obtain:</p>

<p><span  class="math">\[ w \cdot x_i + b \geq 1, \hspace{15pt} \forall x_i \in P \\
 w \cdot x_j + b \leq -1, \hspace{15pt} \forall x_j \in N \]</span></p>







<figure>

  <a data-fancybox="" href="/img/svm/linsep.jpg" >

<img src="/img/svm/linsep.jpg" >
</a>


<figcaption data-pre="Figure " data-post=":" >
  <h4>Two linearly separable sets.</h4>
  
</figcaption>

</figure>


<hr>

<h2 id="margin">Margin</h2>

<p>A fundamental concept in SVM is the <strong>margin</strong>, expressed as the minimum distance $\gamma$ between the points in $D \overset{\Delta}{=} P \cup N$ and the separating hyperplane $H(w,b)$:</p>

<p><span  class="math">\[ \gamma(w, b) = \min_{x \in D} \bigg \{ \frac{|w \cdot x + b|}{||w||} \bigg \} \]</span></p>

<p>Where $|| \cdot || := || \cdot ||_2$ represents the euclidean norm.</p>

<h3 id="derivation">Derivation</h3>

<p>This <em>minimum distance</em> formulation is well known in Geometry, but we can derive it from <em>an optimization point of view</em> as well. Let's start calculating the distance between a point $\bar{x}$ and the hyperplane $H(w,b)$. We can write down this minimization problem as:</p>

<p><span  class="math">\[ \min_{x \in \mathbb{R}^n} \quad ||\bar{x} - x|| \\
   \hspace{15pt}\textrm{s.t.} \quad w \cdot x + b = 0 \]</span></p>

<p>We can apply a strictly increasing transformation to our objective function (to get rid of the non-differentiability) without changing the global optima of our problem:</p>

<p><span  class="math">\[ \min_{x \in \mathbb{R}^n} \quad \frac{1}{2}||\bar{x} - x||^2 \\
   \hspace{2pt} \textrm{s.t.} \quad w \cdot x + b = 0 \]</span></p>

<p>We now have a strictly convex objective function subject to continuously differentiable linear constraints so we can apply the <a href="https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions"><strong>Karush–Kuhn–Tucker</strong></a> optimality conditions:</p>

<p><span  class="math">\[ \nabla_x \bigg [ \frac{1}{2} ||\bar{x} - x||^2 + \lambda(w \cdot x + b) \bigg ] = 0 \]</span></p>

<p><span  class="math">\[ \hspace{67pt}= (\bar{x} - x) + \lambda w = 0 \]</span></p>

<p>Expressing $x = \bar{x} + \lambda w$ and substituting it into the constraint equation we obtain:</p>

<p><span  class="math">\[ w \cdot (\bar{x} + \lambda w) + b \]</span></p>

<p><span  class="math">\[ \hspace{3pt} = w \cdot \bar{x} + \lambda ( w \cdot w) + b \]</span></p>

<p><span  class="math">\[ \hspace{14pt}= w \cdot \bar{x} + \lambda ||w||^2 + b = 0 \]</span></p>

<p>From which we can isolate the lagrangean multiplier and write</p>

<p><span  class="math">\[\lambda = -\frac{w \cdot \bar{x} + b}{||w||^2}\]</span></p>

<p>We were originally interested in $||\bar{x} - x||$ and from the optimality conditions we can now re-write this quantity as:</p>

<p><span  class="math">\[||\bar{x} - x|| = ||\lambda w|| = \frac{|w \cdot \bar{x} + b|}{||w||}\]</span></p>

<p>Which is the projection of a given point $\bar{x}$ on an hyperplane $H(w,b)$.</p>

<p>We can now obtain the definition of margin given above using this result, extending the definition so that we are interested in the minimum distance between every point $x \in D$ to $H(w,b)$.</p>

<p>It is quite intuitive that the margin of a given separating hyperplane is related to the
<em>generalization capability</em> of the corresponding linear classifier. This relationship is analyzed in the <a href="https://www.springer.com/us/book/9780387987804">Statistical Learning Theory</a>, which theoretically motivates the importance of defining
the hyperplane with maximum margin, the so-called <strong>optimal separating hyperplane</strong>.</p>

<hr>

<h2 id="optimal-separating-hyperplane">Optimal Separating Hyperplane</h2>

<p>The optimal separating hyperplane is a separating hyperplane <span  class="math">\(H(w^*, b^*)\)</span> having maximum margin:</p>

<p><span  class="math">\[ \max_{w \in \mathbb{R}^n, b \in \mathbb{R}} \gamma(w,b) = \max_{w \in \mathbb{R}^n, b \in \mathbb{R}} \min_{x \in D} \bigg \{ \frac{|w \cdot x + b|}{||w||} \bigg \}  \]</span></p>

<p>This thing does not look good: it's a <em>max-min</em> problem. We can re-write this quantity as</p>

<p><span  class="math">\[
\max_{\eta, w, b} \eta \\
\hspace{60pt}\text{s.t } \hspace{5pt} \eta \leq \frac{|w \cdot x + b|}{||w||} \\
\hspace{150pt}y(w \cdot x +b) \geq 1 \quad \quad \forall (x,y) \in D
\]</span></p>

<p>since we are dealing with a <strong>finite</strong> number of inequalities (this assures us that the optimal $\eta$ will satisfy the first constraint inequality as an equality).</p>

<p><em>This thing still does not look good</em>: both the numerator and the denominator are <em>non-differentiable</em>. It can be (quite easily) shown that the numerator $|w \cdot x + b|$ can be always set to $1$, leaving us with:</p>

<p><span  class="math">\[
\max_{\eta, w, b} \eta \\
\hspace{35pt}\text{s.t } \hspace{5pt} \eta \leq \frac{1}{||w||} \\
\hspace{150pt}y(w \cdot x +b) \geq 1 \quad \quad \forall (x,y) \in D
\]</span></p>

<p>As you can notice in the first constraint we passed from $|D|$ inequalities to only one, as we lost the dependance from $x$. We can re-write our problem as:</p>

<p><span  class="math">\[
\max_{w, b} \frac{1}{||w||} \\
\hspace{120pt}\text{s.t } \hspace{10pt} y(w \cdot x +b) \geq 1 \quad \quad \forall (x,y) \in D
\]</span></p>

<p>We get rid of the non-differentiability of the denominator by transforming this $max$ problem to a $min$ problem and applying a transformation to the objective function:</p>

<p><span  class="math">\[
\max_{w, b} \frac{1}{2}||w||^2 \qquad \qquad (P)\\
\hspace{55pt}\text{s.t } \hspace{10pt} y(w \cdot x +b) \geq 1 \quad \quad \forall (x,y) \in D
\]</span></p>

<p>This is now a minimization problem of a quadratic function subject to linear constraints, which means that we can easily solve this. The difficulties we encounter are related to the <em>dimensions</em> of the problem. Usually SVMs are <a href="https://pdfs.semanticscholar.org/d1fa/8485ad749d51e7470d801bc1931706597601.pdf">trained</a> with a well-known technique: <a href="http://web.cs.iastate.edu/~honavar/smo-svm.pdf">Sequential Minimal Optimization</a>.</p>

<hr>

<h1 id="lagrange-dual">Lagrange Dual</h1>

<p>The final step towards the solution of this problem is imposing the optimality conditions in the <a href="https://en.wikipedia.org/wiki/Duality_(optimization)">Lagrange Dual</a>. This is the <em>classical</em> approach to the solution but it's not the only one.</p>

<p>Before doing that it's important to highlight that we can demonstrate the equivalency of the starting problem and the one we derived, but we are not going to do that.</p>

<p>Let's relax the constraints and embed them into the objective function: the problem associated with the Lagrangian dual is</p>

<p><span  class="math">\[
\max_{\lambda \geq 0} \Big \{ \min_{w,b} \frac{1}{2} w^{\intercal}w + \sum_{i} \lambda_{i} \big[ 1 - y_i (w \cdot x_i + b) \big ] \Big \} = \\
\max_{\lambda \geq 0} \sum_i \lambda_i + \min_{w,b} \frac{1}{2} w^{\intercal}w - \sum_i \lambda_i y_i (w \cdot x_i + b) \quad \quad (\alpha)
\]</span></p>

<p>which is a convex unconstrained optimization problem. Let's leave the $max$ apart for a moment and concentrate on the internal $min$ problem, we know that the optimality conditions are the nullification of the gradient of the Lagrangian:</p>

<p><span  class="math">\[
\nabla \mathcal{L}(w,b) = \nabla_{w,b} \Big [ \min_{w,b} \frac{1}{2}w^\intercal w - \sum_i \lambda_i y_i(w \cdot x_i +b) \Big ] = 0
\]</span></p>

<p>from which we have:</p>

<p><span  class="math">\[
\nabla_w \mathcal{L}(w,b) = 0 \implies w - \sum_i \lambda_i x_i y_i = 0 \implies \boxed{w = \sum_i \lambda_i y_i x_i \quad} (1) \\
\nabla_b \mathcal{L}(w,b) = 0 \implies 0 - \sum_i \lambda_i y_i = 0 \implies \boxed{\sum_i \lambda_i y_i = 0} \hspace{35pt} (2)
\]</span></p>

<p>We can now solve the $max$ problem. Plugging $(1)$ into $(\alpha)$ we obtain:</p>

<p><span  class="math">\[
\max_{\lambda \geq 0} \sum_i \lambda_i + \frac{1}{2}\sum_i \sum_j \lambda_i \lambda_j y_i y_j x_i^\intercal x_j - \sum_i \lambda_i y_i \Big (\sum_j \lambda_j y_j x_j^\intercal \Big )x_i - \sum_i \lambda_i y_i b \quad (\alpha^{'})
\]</span></p>

<p>Substituting $(2)$ into $(\alpha^{'})$ and switching signs (in order to pass from a $max$ to a $min$ problem) we have:</p>

<p><span  class="math">\[
\min_{\lambda \geq 0} \frac{1}{2} \sum_i \sum_j \lambda_i \lambda_j y_i y_j x_i^\intercal x_j - \sum_i \lambda_i \\
\hspace{-75pt}{\text{s.t. } \sum_i \lambda_i y_i = 0}
\]</span></p>

<p>Setting <span  class="math">\(X = [y_1x_1 \dots y_nx_n]\)</span>, <span  class="math">\(\lambda = [\lambda_1 \dots \lambda_n]^\intercal\)</span> and <span  class="math">\(y = [y_1 \dots y_n]\)</span> we can finally write this in a compact form:</p>

<p><span  class="math">\[
\min_{\lambda \geq 0} \frac{1}{2} \lambda^\intercal X^\intercal X \lambda - e^\intercal \lambda \qquad \qquad (L)\\
\hspace{-88pt}{\text{s.t } \hspace{10pt} \lambda ^\intercal y = 0}
\]</span></p>

<p>where <span  class="math">\(e = [1 \dots 1]\)</span>.</p>

<hr>

<h1 id="solution">Solution</h1>

<p>We can now either solve problem $(P)$ or $(L)$ indifferently. Suppose we solved problem $(L)$, obtaining a <span  class="math">\(\lambda^*\)</span>. We can retrieve <span  class="math">\(w^*\)</span> from $(1)$ as</p>

<p><span  class="math">\[
w^* = \sum_i \lambda_i^* x_i y_i
\]</span></p>

<p>An important fact, that comes from the KKT complementary slackness, is that the solution $w^*$ depends only on the so-called <strong>support vectors</strong> <span  class="math">\(x_i\)</span>, whose correspondent multipliers <span  class="math">\(\lambda_i^*\)</span> are not null.</p>

<p>Once computed <span  class="math">\(w^*\)</span>, by considering any multiplier <span  class="math">\(\lambda_i^*>0\)</span> the scalar $b^*$ can be determined by means of the corresponding complementarity condition</p>

<p><span  class="math">\[
\lambda_i^* \big [ y_i(w^* \cdot x_i + b^*) - 1 \big ] = 0, \qquad \qquad i=1, \dots, n
\]</span></p>

<p>When we'll be in possess of a new data point $\hat{x}$ we can decide if $\hat{x} \in P$ or $\hat{x} \in N$ by applying the <em>decision function</em> $d(\cdot)$:</p>

<p><span  class="math">\[\boxed{
d(\hat{x}) = sgn(w^* \cdot \hat{x} + b^*) = sgn \Bigg (\sum_i^n \lambda_i^* y_i x_i \cdot \hat{x} + b^* \Bigg )}
\]</span></p>

    </div>

    


    



    
      








  





  
  
  
    
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hu351c70bc5cdcd92ea95125a134391ca5_80941_250x250_fill_q90_lanczos_center.jpg" itemprop="image" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="https:w00zie.github.io">Giovanni Bindi</a></h5>
      <h6 class="card-subtitle">Student</h6>
      <p class="card-text" itemprop="description">I write stuff in my spare time.</p>
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
          <li>
            <a itemprop="sameAs" href="mailto:giovanni.bindi41@gmail.com" >
              <i class="fas fa-envelope"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://github.com/w00zie" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
    

    

    


  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.0.0/mermaid.min.js" integrity="sha256-0w92bcB21IY5+rGI84MGj52jNfHNbXVeQLrZ0CGdjNY=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/c.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.16bbb3750feb7244c9bc409a5a4fe678.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
