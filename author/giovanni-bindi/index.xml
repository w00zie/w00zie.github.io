<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Giovanni Bindi</title>
    <link>https://w00zie.github.io/author/giovanni-bindi/</link>
      <atom:link href="https://w00zie.github.io/author/giovanni-bindi/index.xml" rel="self" type="application/rss+xml" />
    <description>Giovanni Bindi</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 12 Dec 2020 17:09:13 +0200</lastBuildDate>
    <image>
      <url>https://w00zie.github.io/media/giova.jpg</url>
      <title>Giovanni Bindi</title>
      <link>https://w00zie.github.io/author/giovanni-bindi/</link>
    </image>
    
    <item>
      <title>Winning the CHSH game with Cirq and Bayesian Optimization</title>
      <link>https://w00zie.github.io/post/bell/</link>
      <pubDate>Sat, 12 Dec 2020 17:09:13 +0200</pubDate>
      <guid>https://w00zie.github.io/post/bell/</guid>
      <description>&lt;div align=&#34;center&#34;&gt; &lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#description-of-the-game&#34;&gt;Description of the game&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#deterministic-strategy&#34;&gt;Deterministic strategy&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#quantum-strategy&#34;&gt;Quantum strategy&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#winning-at-the-game-without-designing-a-winning-strategy&#34;&gt;Winning at the game without designing a winning strategy&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
 &lt;/div&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this post we&amp;rsquo;ll be going through a simple experiment I&amp;rsquo;ve set up in order to violate the Bell inequality.&lt;/p&gt;
&lt;p&gt;The goal is to &amp;ldquo;win&amp;rdquo; (one of the formulations of) the CHSH game: this &amp;ldquo;game&amp;rdquo; was proposed by Clauser, Horn, Shimony and Holt in their famous 1969 PRL paper 
&lt;a href=&#34;https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.23.880&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;1&lt;/a&gt; as a generalizatio of 1964 Bell&amp;rsquo;s Theorem. The formulation we&amp;rsquo;ll be dealing with is the one proposed by Mark Wilde in his book 
&lt;a href=&#34;http://www.markwilde.com/qit-notes.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2&lt;/a&gt; and by Ronald de Wolf in his lecture notes 
&lt;a href=&#34;https://homepages.cwi.nl/~rdewolf/qcnotes.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can find the code 
&lt;a href=&#34;https://github.com/w00zie/pqc_chsh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;description-of-the-game&#34;&gt;Description of the game&lt;/h2&gt;
&lt;p&gt;Here&amp;rsquo;s the game description by Mark Wilde:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This game is a 2-player game where the two players, Alice and Bob, are spatially separated from each other from the time that the game starts until it is over. The game begins with a referee selecting two bits $x$ and $y$ uniformly at random. The referee then sends $x$ to Alice and $y$ to Bob. Alice and Bob are not allowed to communicate with each other in any way at this point. Alice sends back to the referee a bit $a$, and Bob sends back a bit $b$. Since they are spatially separated, Alice’s response bit $a$ cannot depend on Bob’s input bit $y$, and similarly, Bob’s response bit $b$ cannot depend on Alice’s input bit $x$. After receiving the response bits $a$ and $b$, the referee determines if the &lt;em&gt;AND&lt;/em&gt; of $x$ and $y$ is equal to the exclusive &lt;em&gt;OR&lt;/em&gt; of $a$ and $b$. If so, then Alice and Bob win the game.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Given that, the winning conditions is&lt;/p&gt;
&lt;p&gt;$$
x \land y = a \oplus b
$$&lt;/p&gt;
&lt;h2 id=&#34;deterministic-strategy&#34;&gt;Deterministic strategy&lt;/h2&gt;
&lt;p&gt;A deterministic strategy would have Alice select a bit $a_x$ conditioned on the bit $x$ that she receives, and similarly, Bob would select a bit $b_y$ conditioned on $y$. The following table presents the winning conditions for the four different values of $x$ and $y$ with this deterministic strategy:&lt;/p&gt;
&lt;center&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$x$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$y$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$x \land y$&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;$a_x \oplus b_y$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$a_0 \oplus b_0$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$a_0 \oplus b_1$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$a_1 \oplus b_0$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$a_1 \oplus b_1$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;At most, only three out of four of them can be satisfied, so that the maximal winning probability with a classical deterministic strategy is at most $3 / 4$. We can then see that a strategy for them to achieve this upper bound is for Alice and Bob always to return $a = 0$ and $b = 0$ no matter the values of $x$ and $y$.&lt;/p&gt;
&lt;h2 id=&#34;quantum-strategy&#34;&gt;Quantum strategy&lt;/h2&gt;
&lt;p&gt;Now consider the same problem but where Alice and Bob are supplied with a shared 2-qubit system initialized to the entangled state&lt;/p&gt;
&lt;p&gt;$$
\frac{1}{\sqrt{2}} (|00\rangle - |11\rangle)
$$&lt;/p&gt;
&lt;p&gt;The strategy is the following: if $x = 0$ then Alice applies $R(-\pi/16)$ to her qubit and if $x = 1$ she applies $R(3\pi/16)$, where $R(\theta)$ denotes a rotation of $\theta$:&lt;/p&gt;
&lt;p&gt;$$
R(\theta) = \begin{bmatrix} cos(\theta) &amp;amp; -sin(\theta) \\ sin(\theta) &amp;amp; \cos(\theta) \end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;Bob’s procedure is the same, depending on his input bit $y$.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;If Alice now rotates her qubit by $\theta_A$ and Bob rotates his qubit by $\theta_B$ then the state becomes:&lt;/p&gt;
&lt;p&gt;$$
\frac{1}{\sqrt{2}} (cos(\theta_A + \theta_B))(|00\rangle - |11\rangle) + \sin(\theta_A + \theta_B)(|01\rangle + |10\rangle)
$$&lt;/p&gt;
&lt;p&gt;Measuring in the computational basis we see that, after the measurements, the probability that $a \oplus b = 0$ is $cos(\theta_A + \theta_B)^2$. If $x \land y = 0$ then $\theta_A + \theta_B = \pm \pi / 8$, else if $x \land y = 1$ then $\theta_A + \theta_B = 3 \pi / 8$.&lt;/p&gt;
&lt;p&gt;The winning condition is then satisfied with probability $cos(\pi / 8)^2 \approx 0.853$, for all four input possibilities, showing that quantum entanglement allows Alice and Bob to win the game with a probability that&amp;rsquo;s higher than what the best classical strategy can achieve. Tsirelson 
&lt;a href=&#34;https://www.tau.ac.il/~tsirel/download/qbell80.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;5&lt;/a&gt; showed that $cos(\pi / 8)^2$ is the best that quantum strategies can
do for CHSH, even if they are allowed to use much more entanglement than one Bell state.&lt;/p&gt;
&lt;h2 id=&#34;winning-at-the-game-without-designing-a-winning-strategy&#34;&gt;Winning at the game without designing a winning strategy&lt;/h2&gt;
&lt;p&gt;Quantum computing (QC) is one of the currently &amp;ldquo;hot&amp;rdquo; research areas, gaining substantious advantages year after year.
Many industries and Universities are developing interesting tools for experimenting with the power of QC, such as IBM&amp;rsquo;s 
&lt;a href=&#34;https://qiskit.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;qiskit&lt;/a&gt; and Google&amp;rsquo;s 
&lt;a href=&#34;https://quantumai.google/cirq&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cirq&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;These two examples are Python-based frameworks for programming quantum computers and, in this experiment, I used Cirq. My idea is simple: I wanted to construct a parameterized quantum circuit (PQC) that, along with an optimization strategy, could find the best parameters for winning at the CHSH game.&lt;/p&gt;
&lt;p&gt;The optimization strategy I employed is Bayesian Optimization, which is a strategy for global optimization of black-box functions. Bayesian optimization is particularly advantageous for problems where the function we want to maximize is difficult to evaluate, is a black box with some known structure, relies upon less than 20 dimensions, and where derivatives are not evaluated 
&lt;a href=&#34;https://arxiv.org/abs/1807.02811&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;4&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Turns out this is exactly my setting, since I want to maximize the probability of winning the CHSH game through a series of simulations of a few-parameterized-gates quantum circuit. The circuit I designed is the following&lt;/p&gt;






  
  











&lt;figure id=&#34;figure-parameterized-quantum-circuit&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://w00zie.github.io/post/bell/img/circuit_hu4c0bf548b1fa199fd10b3a8d446cb0bc_14743_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Parameterized Quantum Circuit&#34;&gt;


  &lt;img data-src=&#34;https://w00zie.github.io/post/bell/img/circuit_hu4c0bf548b1fa199fd10b3a8d446cb0bc_14743_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;513&#34; height=&#34;204&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Parameterized Quantum Circuit
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;where the four qubits are initialized to $|0\rangle$. The transformations which operate on these qubits are Hadamard gates (denoted by $H$), rotations $R_X$ and $R_Z$ (respectively by the $X$ and $Z$ axis) and CNOTs. Rotations and controlled-not gates are parameterized by the $x*$ you see in the pictures: I&amp;rsquo;ve constrained the rotational parameters into $[0, 2\pi]$ and the exponents of the CNOTs to $[-2, 2]$. The last *moment* of the circuit is the one dedicated to Measurements, which happen in the computational basis.&lt;/p&gt;
&lt;p&gt;For every set of fixed parameters a simulation of $N=5000$ runs is operated and, in the end, a winning statistic is calculated as $N^w / N \in [0, 1]$, where $N^w$ is the number of times that the winning condition is satisfied. This function is then maximized through 
&lt;a href=&#34;https://github.com/fmfn/BayesianOptimization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Optimization&lt;/a&gt; resulting in a set of parameters which are able to win the game:&lt;/p&gt;






  
  











&lt;figure id=&#34;figure-convergence-of-bo&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://w00zie.github.io/post/bell/img/bo_hudeda4c1d4ab9458826b6547fa4ae320a_55004_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;&amp;amp;lsquo;&amp;amp;lsquo;Convergence&amp;amp;rsquo;&amp;amp;rsquo; of BO&#34;&gt;


  &lt;img data-src=&#34;https://w00zie.github.io/post/bell/img/bo_hudeda4c1d4ab9458826b6547fa4ae320a_55004_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;720&#34; height=&#34;496&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &amp;lsquo;&amp;lsquo;Convergence&amp;rsquo;&amp;rsquo; of BO
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Music generation with Wasserstein Autoencoders</title>
      <link>https://w00zie.github.io/post/wae/</link>
      <pubDate>Sat, 01 Aug 2020 16:46:13 +0200</pubDate>
      <guid>https://w00zie.github.io/post/wae/</guid>
      <description>&lt;div align=&#34;center&#34;&gt; &lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#the-data&#34;&gt;The data&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#theory&#34;&gt;Theory&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#wae-gan&#34;&gt;WAE-GAN&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#wae-mmd&#34;&gt;WAE-MMD&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#experiments-details&#34;&gt;Experiments details&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#architechtures&#34;&gt;Architechtures&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#prior&#34;&gt;Prior&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#latent-space-dimensionality&#34;&gt;Latent space dimensionality&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#adversarial-training&#34;&gt;Adversarial training&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#cost-function&#34;&gt;Cost function&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#hyperparameters&#34;&gt;Hyperparameters&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#results&#34;&gt;Results&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#random-generation&#34;&gt;Random Generation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#reconstruction&#34;&gt;Reconstruction&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
 &lt;/div&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is a project I started working on for my Machine Learning class. The aim, as you can read from the title, is being able to generate music that is, at least, not unpleasant.&lt;/p&gt;
&lt;p&gt;In order to do so I investigated the use of Wasserstein Autoencoders (WAE), a generative model firstly proposed by Tolstikhin &lt;em&gt;et al.&lt;/em&gt; &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, which employs a regularizer derived from the theory of optimal transport.&lt;/p&gt;
&lt;h2 id=&#34;the-data&#34;&gt;The data&lt;/h2&gt;
&lt;p&gt;The data I used was published by a group of researchers from the 
&lt;a href=&#34;https://musicai.citi.sinica.edu.tw/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academia Sinica&lt;/a&gt; (Taiwan), as part of a paper&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; where they jointly proposed a novel GAN (MuseGAN) and released the dataset where the model was trained on. They open-sourced almost everything so you can access to their 
&lt;a href=&#34;https://github.com/salu133445/musegan&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt; and 
&lt;a href=&#34;https://salu133445.github.io/musegan/data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;data&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This dataset contains 174154 unique 4/4 multi-track music pieces of 4 bars, coming from different rock music compositions. Every element of this dataset is represented as a multi-track (binary-valued) pianoroll. For those of you who are not familiar with DAWs or audio processing, a single-track pianoroll is basically a matrix whose entries represent the behaviour of the notes played by the single instrument. The number of columns in this matrix is the number of time-steps used to quantize the composition while the number of rows represents the octave extension, i.e. the number of possible notes that the instrument can execute.&lt;/p&gt;






  
  











&lt;figure id=&#34;figure-example-of-a-pianoroll&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://w00zie.github.io/post/wae/img/pianoroll_hu0665c07db140e9b058b96a7d644ee03b_63264_2000x2000_fit_q90_lanczos.jpeg&#34; data-caption=&#34;Example of a pianoroll&#34;&gt;


  &lt;img data-src=&#34;https://w00zie.github.io/post/wae/img/pianoroll_hu0665c07db140e9b058b96a7d644ee03b_63264_2000x2000_fit_q90_lanczos.jpeg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;800&#34; height=&#34;600&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Example of a pianoroll
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In this dataset a single bar is quantized into 48 time-steps (the minimum duration of a note is then 1/48th) and can span between 84 notes from C1 to B7. Every element of the dataset contains 4 bars of 5 instruments, resulting in a binary-valued (note on/off - velocity information is discarded) tensor of shape (4*48, 84, 5) = (192, 84, 5). The tempo was set to 100 bpm and the authors transposed every element to the same tonality.
The instruments involved are &lt;em&gt;Drums, Bass, Guitar, Piano&lt;/em&gt; and &lt;em&gt;Strings&lt;/em&gt;.&lt;/p&gt;






  
  











&lt;figure id=&#34;figure-one-element-of-the-dataset-4-bars-of-5-instruments-best-with-the-white-theme&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://w00zie.github.io/post/wae/img/mydata_hud05689e6339d7b054e3f596aaa3e4ce0_289573_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;One element of the dataset: 4 bars of 5 instruments (best with the white theme)&#34;&gt;


  &lt;img data-src=&#34;https://w00zie.github.io/post/wae/img/mydata_hud05689e6339d7b054e3f596aaa3e4ce0_289573_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1442&#34; height=&#34;1634&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    One element of the dataset: 4 bars of 5 instruments (best with the white theme)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The pianoroll is one of the most commonly used representations, although it has some limitations. An important one, compared to the MIDI representation, is that there is no &lt;em&gt;note-off&lt;/em&gt; information. As a result, there is no way to distinguish between a long note and repeated short notes.
In order to partially solve this issue the authors did as following&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that during the conversion from MIDI files to pianorolls, an additional minimal-length (of one time step) pause is added between two consecutive (without a pause) notes of the same pitch to distinguish them from one single note.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A few samples from the dataset&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; (&lt;strong&gt;lower your audio output&lt;/strong&gt;):&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt; 
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/orig/orig_1.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
 &lt;/div&gt;
&lt;div align=&#34;center&#34;&gt; 
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/orig/orig_2.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
 &lt;/div&gt;
&lt;div align=&#34;center&#34;&gt; 
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/orig/orig_3.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
 &lt;/div&gt;
&lt;div align=&#34;center&#34;&gt; 
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/orig/orig_4.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
 &lt;/div&gt;
&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: &lt;em&gt;a lot of this theory is taken from the original paper&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, I did not re-write it from scratch&lt;/em&gt;!&lt;/p&gt;
&lt;p&gt;WAEs are a class of Autoencoders derived by theory of 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Transportation_theory_%28mathematics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;optimal transport&lt;/a&gt;. Given a collection of (unlabelled) data points $S_X$, the ultimate goal of the task is to tune a model capable of generating sets of synthetic points $S_G$ which look similar to $S_X$. There are various ways of defining the notion of similarity between two sets of data points: the most common approach assumes that both $S_X$ and $S_G$ are sampled independently from two probability distributions $P_X$ and $P_G$ respectively, and employ some of the known divergence measures for distributions.&lt;/p&gt;
&lt;p&gt;Two major approaches currently dominate this field.
Variational Auto-Encoders (VAEs) minimize the Kullback-Leibler (KL) divergence
$D_{KL}(P_X, P_G)$, which is equivalent to maximizing the marginal log-likelihood of the model $P_G$. Generative Adversarial Networks (GANs) employ an elegant framework, commonly referred to as adversarial training, which is suitable for many different divergence measures, including (but not limited to) $f-$divergences, 1-Wasserstein distance, and Maximum Mean Discrepancy (MMD).&lt;/p&gt;
&lt;p&gt;Similarly to VAEs, WAEs describe a particular way to train probabilistic &lt;em&gt;latent variable models&lt;/em&gt; (LVMs) $P_G$. LVMs act by first sampling a code (feature) vector $Z$ from a prior distribution $P_Z$ defined over the latent space $\mathcal{Z}$ and then mapping it to a random input point $X \in \mathcal{X}$ using a conditional distribution $P_G(X|Z)$ also known as the &lt;em&gt;decoder&lt;/em&gt;. This results is a density of the form&lt;/p&gt;
&lt;p&gt;$$
p_G(x) = \int_{\mathcal{Z}} p_G(x|z)p(z)dz
$$&lt;/p&gt;
&lt;p&gt;assuming all involved densities are properly defined.&lt;/p&gt;
&lt;p&gt;Instead of minimizing the KL divergence between the LVM $P_G$ and the unknown data distribution $P_X$ as done by VAEs, WAEs aim at minimizing any optimal transport distance between them. Kantorovich&amp;rsquo;s formulation of the optimal transport problem (OT) is given by&lt;/p&gt;
&lt;p&gt;$$
W_c(P_X, P_G) = \inf_{\Gamma \in \mathcal{P}(X \sim P_X, Y \sim P_G)} \mathbb{E}_{(X,Y) \sim \Gamma} [c(X,Y)]
$$&lt;/p&gt;
&lt;p&gt;where $c:\mathcal{X} \times \mathcal{Y} \to \mathbb{R}^+$ is any measurable cost function and $\mathcal{P}(X \sim P_X, Y \sim P_G)$ is the set of all joint distributions of $(X, Y)$ with marginals $P_X$ and $P_G$ respectively. When $(\mathcal{X}, d)$ is a metric space (with distance $d$) and $c(x,y) = d^p(x,y)$ for $p \geq 1$ then $W_p$ (the $p$-th root of $W_c$) is known as the $p$-&lt;em&gt;Wasserstein distance&lt;/em&gt;. When $p=1$ we then have the $1$-Wasserstein distance and this duality holds (Kantorovich-Rubinstein duality)&lt;/p&gt;
&lt;p&gt;$$
W_1(P_X, P_G) = \sup_{f \in \mathcal{F}_L} \mathbb{E}_{X \sim P_X} [f(X)] - \mathbb{E}_{Y \sim P_G}[f(Y)]
$$&lt;/p&gt;
&lt;p&gt;where $\mathcal{F}_L$ is the set of all bounded 1-Lipschitz functions on $(\mathcal{X}, d)$.
The authors state (and prove) that for deterministic decoders, i.e. $P_G(X|Z=z) = \delta_{G(z)}$ deterministically mapping latent codes $z \in \mathcal{Z}$ to the input space for a given map $G: \mathcal{Z} \to \mathcal{X}$, there is a simpler formulation for the OT problem: instead of finding a coupling $\Gamma$ between two random variables in $\mathcal{X}$ it is sufficient to find a conditional distribution $Q(Z|X)$ such that its $Z$ marginal $Q(Z) = \mathbb{E}_{X \sim P_X} [Q(Z|X)]$ is identical to the prior distribution $P_Z$. This resulted in a theorem where the OT problem was re-formulated (under these assumptions) as&lt;/p&gt;
&lt;p&gt;$$
W_c(P_X, P_G) = \inf_{Q:Q_Z = P_Z} \mathbb{E}_{P_X}\big [ \mathbb{E}_{Q(Z|X)}[c(X, G(Z))] \big ]
$$&lt;/p&gt;
&lt;p&gt;In order to implement a numerical solution to this problem the authors relax the constraint on $Q_Z$ by adding a penalty term to the objective, which, in the end, takes the following form&lt;/p&gt;
&lt;p&gt;$$
D_{WAE}(P_X, P_G) = \inf_{Q(Z|X) \in \mathcal{Q}} \mathbb{E}_{P_X}\big [ \mathbb{E}_{Q(Z|X)}[c(X, G(Z))] \big ] + \lambda \mathcal{D}_Z(Q_Z, P_Z)
$$&lt;/p&gt;
&lt;p&gt;where $\mathcal{Q}$ is any nonparametric set of probabilistic encoders, $\mathcal{D}_Z$ is an arbitrary divergence between $Q_Z$ and $P_Z$ , and $\lambda &amp;gt; 0$ is a hyperparameter. The authors propose two implementations of the WAE model, based on two different divergences.&lt;/p&gt;
&lt;h3 id=&#34;wae-gan&#34;&gt;WAE-GAN&lt;/h3&gt;
&lt;p&gt;The WAE-GAN model employs the Jensen-Shannon divergence, i.e.&lt;/p&gt;
&lt;p&gt;$$
\mathcal{D}_Z(Q_Z, P_Z) = \mathcal{D}_{JS}(Q_Z, P_Z) = \frac{1}{2} D_{KL}(P_Z, M) + \frac{1}{2} D_{KL}(Q_Z, M)
$$&lt;/p&gt;
&lt;p&gt;where $M = \frac{1}{2}(P_Z + Q_Z)$ and uses the adversarial training to estimate it. The authors introduce an adversary (discriminator) in the latent space $\mathcal{Z}$ trying to separate &amp;ldquo;true&amp;rdquo; points sampled from $P_Z$ and &amp;ldquo;fake&amp;rdquo; ones sampled from $Q_Z$.&lt;/p&gt;
&lt;h3 id=&#34;wae-mmd&#34;&gt;WAE-MMD&lt;/h3&gt;
&lt;p&gt;For a positive-definite reproducing kernel $k : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ the following expression is called the &lt;em&gt;Maximum Mean Discrepancy&lt;/em&gt; (MMD):&lt;/p&gt;
&lt;p&gt;$$
\text{MMD}_k(P_Z, Q_Z) = \Big | \Big | \int_\mathcal{Z} k(z, \cdot) dP_Z(z) - \int_\mathcal{Z} k(z, \cdot) dQ_Z(z) \Big | \Big |_{\mathcal{H}_k}
$$&lt;/p&gt;
&lt;p&gt;where $\mathcal{H}_k$ is the RKHS of real-valued functions mapping $\mathcal{Z}$ to $\mathcal{R}$. If $k$ is &lt;em&gt;characteristic&lt;/em&gt; then MMD$_k$
defines a metric and can be used as a divergence measure. The authors propose to use an unbiased estimate of the MMD that is the sum of two $U$-statistics and a sample average.
Given i.i.d. samples $\mathbf{P} = { z_1, \dots, z_n }$ from $P_Z$ and i.i.d. samples $\mathbf{Q} = {\tilde{z}_1, \dots, \tilde{z}_n}$ from $Q(Z|z_i)$ , the unbiased estimate&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; is&lt;/p&gt;
&lt;p&gt;$$
\widehat{MMD}^2_U \big [\mathcal{H}_k, \mathbf{P}, \mathbf{Q} \big ] = \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j \neq i} k(z_i, z_j) + \sum_{i=1}^n \sum_{j \neq i} k(\tilde{z}_i, \tilde{z}_j) - \frac{2}{n^2} \sum_{i=1}^n \sum_{j=1}^n k(z_i, \tilde{z}_j)
$$&lt;/p&gt;
&lt;h2 id=&#34;experiments-details&#34;&gt;Experiments details&lt;/h2&gt;
&lt;h3 id=&#34;architechtures&#34;&gt;Architechtures&lt;/h3&gt;
&lt;p&gt;Encoder $Q_\phi$, decoder $G_{\theta}$ and discriminator $D_{\gamma}$ are parametrized by deep neural nets. Given the structure of the data, binary-valued tensors of shape (H,W,5) (where 5, the number of instruments, plays naturally the role of the number of channels), both encoder and decoder are CNNs, while the discriminator (which acts in the latent space $\mathcal{Z}$) is a &amp;ldquo;vanilla&amp;rdquo; NN. You can check out the various models I&amp;rsquo;ve coded 
&lt;a href=&#34;https://github.com/w00zie/wae_music&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, they are almost all equals except for the strides of the convolutions.&lt;/p&gt;
&lt;p&gt;The main architecture I used was inspired by DCGAN &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;. In the following picture I&amp;rsquo;m showing the encoder that, starting from the (192, 84, 5) input tensor, applies 4 convolutions with $[6,6]$ filters, followed by Batch Normalizations and ReLUs. The last layer employs a reshape (not shown in the picture) followed by a (small dropout of 0.2 and a) fully-connected layer that maps the previous inputs into a vector of size $dim(\mathcal{Z}) = d_{\mathcal{Z}}$ (in the following picture $d_{\mathcal{Z}} = 8$).&lt;/p&gt;






  
  











&lt;figure id=&#34;figure-my-encoder-made-with-nn-svghttpsalexlenailmenn-svg&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://w00zie.github.io/post/wae/img/encoder_hu03db60d98e0fb8a5cd64c7aee1dc2862_38313_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;My encoder (made with &amp;lt;a href=&amp;#34;https://alexlenail.me/NN-SVG/&amp;#34;&amp;gt;NN-SVG&amp;lt;/a&amp;gt;)&#34;&gt;


  &lt;img data-src=&#34;https://w00zie.github.io/post/wae/img/encoder_hu03db60d98e0fb8a5cd64c7aee1dc2862_38313_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1024&#34; height=&#34;586&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    My encoder (made with &lt;a href=&#34;https://alexlenail.me/NN-SVG/&#34;&gt;NN-SVG&lt;/a&gt;)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The decoder acts simmetrically wrt the encoder, using 
&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2D transposed convolutions&lt;/a&gt; instead of convolutions. The discriminator network maps a vector from $\mathcal{Z}$ to $[0,1]$ through 6 dense layers followed by ReLUs.&lt;/p&gt;
&lt;h3 id=&#34;prior&#34;&gt;Prior&lt;/h3&gt;
&lt;p&gt;I&amp;rsquo;ve used a Gaussian prior $P_Z = \mathcal{N} \big ( 0, \sigma_z^2 \cdot I_{d_z} \big )$ where the variance $\sigma_z^2$ is an hyperparameter.&lt;/p&gt;
&lt;h3 id=&#34;latent-space-dimensionality&#34;&gt;Latent space dimensionality&lt;/h3&gt;
&lt;p&gt;The authors studied the choice of the dimensionality of the latent space in a follow-up paper&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;. They investigated the relationship between the latent space dimension $d_{\mathcal{Z}}$ and the *intrinsic dimensionality* of the data distribution, $d_{\mathcal{I}}$. This last quantity is, informally, *the minimum number of parameters required to continuously parametrise the data manifold*. If this quantity is greater than $d_{\mathcal{Z}}$ then the encoder will map the data distribution $P_X$ to $Q_Z$ supported on a latent manifold of dimension at most $d_{\mathcal{I}}$ while the divergence regularizer $\lambda \mathcal{D}_Z (Q_Z, P_Z)$ will encourage the encoder to fill the latent space similarly to the prior $P_Z$ as much as possible. This can potentially lead to the absence of any learning capability.&lt;/p&gt;
&lt;h3 id=&#34;adversarial-training&#34;&gt;Adversarial training&lt;/h3&gt;
&lt;p&gt;I&amp;rsquo;ve decided to use (as the authors suggest) the non-saturating GAN loss &lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;: this resulted in a discriminator $D_\gamma$ trained by the maximization of&lt;/p&gt;
&lt;p&gt;$$
\frac{\lambda}{n} \sum_{i=1}^n \log D_\gamma(z_i) + \log D_\gamma(1-\tilde{z}_i)
$$&lt;/p&gt;
&lt;p&gt;and in an autoencoder ($Q_\phi, G_\theta$) trained by the minimization of&lt;/p&gt;
&lt;p&gt;$$
\frac{1}{n}\sum_{i=1}^n c(z_i, G_\theta(\tilde{z}_i)) - \lambda D_\gamma \log (\tilde{z}_i)
$$&lt;/p&gt;
&lt;p&gt;As the authors point out this procedure falls into the min-max problem which makes GAN unstable. This training strategy can then possibly yield instabilities in the training phase but the authors claim that by executing an adversarial training in a &amp;ldquo;simpler&amp;rdquo; space (wrt the pixel space where GANs are usually trained on), the whole process could benefit.&lt;/p&gt;
&lt;h3 id=&#34;cost-function&#34;&gt;Cost function&lt;/h3&gt;
&lt;p&gt;I&amp;rsquo;ve used $c(x, y) = || x - y ||_2^2$ for both implementations (WAE-GAN and WAE-MMD).&lt;/p&gt;
&lt;h3 id=&#34;hyperparameters&#34;&gt;Hyperparameters&lt;/h3&gt;
&lt;p&gt;Since there is no objective measure of performace for this kind of task, running an hyperparameter optimization procedure is hard. I&amp;rsquo;ve adjusted the parameters manually and so the results I obtained might not be the best that these model can produce.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;As mentioned in the data section, the input samples are binary-valued tensors of shape (192, 84, 5). The decoder, although, maps a latent code into $[0, 1]^{192 \times 84 \times 5}$ through the action of the sigmoid function. These output tensors $\hat{x}$ must then be &lt;em&gt;binarized&lt;/em&gt;, and I&amp;rsquo;ve tried two strategies to do so:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Hard thresholding (HT): The resulting tensor entry $\hat{x}_{ijk}$ needs to be greater than a fixed threshold (0.5) in order to be mapped to 1.
$$\hat{x}_{ijk}^{HT} = \begin{cases} 1 &amp;amp; \text{if } \hspace{10pt} x_{ijk} &amp;gt; 0.5 \\&lt;br&gt;
0 &amp;amp; \text {o.w }\end{cases}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bernoulli sampling (BS) (by the authors of MuseGAN): The resulting tensor entry $\hat{x}_{ijk}$ needs to be greater than a sample from the uniform distribution over scalars between 0 and 1.
$$\hat{x}_{ijk}^{BS} = \begin{cases} 1 &amp;amp; \text{if } \hspace{10pt} x_{ijk} &amp;gt; b \sim \mathcal{U}[0,1] \\&lt;br&gt;
0 &amp;amp; \text {o.w }\end{cases}$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I&amp;rsquo;m presenting samples binarized with the HT method.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;For a &amp;ldquo;fair&amp;rdquo; comparison the following results have been obtained with the same architecture and hyperparameters. Both WAE-GAN and WAE-MMD models have been trained with a latent dimension of $d_{\mathcal{Z}} = 128$. The number of filters in the four convolutions (see the picture above) are [8, 16, 32, 64] for the encoder and [64, 32, 16, 8] for the decoder. Both sequences of convolution used a fixed filter size of 6x6. I&amp;rsquo;ve trained both models for 500 epochs with Adam. Both autoencoder&amp;rsquo;s and discriminator&amp;rsquo;s optimizer started with a learning rate of $10^{-4}$ and both employed a decay to $\approx 10^{-5}$.&lt;/p&gt;
&lt;p&gt;





  
  











&lt;figure id=&#34;figure-wae-gan-loss&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://w00zie.github.io/post/wae/img/gan_wae_loss_hua84152800d785313dec94bb60ea81f41_38988_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;WAE-GAN Loss&#34;&gt;


  &lt;img data-src=&#34;https://w00zie.github.io/post/wae/img/gan_wae_loss_hua84152800d785313dec94bb60ea81f41_38988_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    WAE-GAN Loss
  &lt;/figcaption&gt;


&lt;/figure&gt;







  
  











&lt;figure id=&#34;figure-wae-gan-discriminator-loss&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://w00zie.github.io/post/wae/img/gan_discrim_hu3d6d9768a3a4a2b640775946fe41a839_58542_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;WAE-GAN Discriminator Loss&#34;&gt;


  &lt;img data-src=&#34;https://w00zie.github.io/post/wae/img/gan_discrim_hu3d6d9768a3a4a2b640775946fe41a839_58542_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    WAE-GAN Discriminator Loss
  &lt;/figcaption&gt;


&lt;/figure&gt;







  
  











&lt;figure id=&#34;figure-wae-mmd-loss&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://w00zie.github.io/post/wae/img/mmd_wae_loss_hu28c1ca953174befbae54124e25204875_26143_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;WAE-MMD Loss&#34;&gt;


  &lt;img data-src=&#34;https://w00zie.github.io/post/wae/img/mmd_wae_loss_hu28c1ca953174befbae54124e25204875_26143_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    WAE-MMD Loss
  &lt;/figcaption&gt;


&lt;/figure&gt;







  
  











&lt;figure id=&#34;figure-wae-mmd-penalty&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://w00zie.github.io/post/wae/img/mmd_penalty_hud51e3a5e3de122a467f8387f03d75af9_31553_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;WAE-MMD Penalty&#34;&gt;


  &lt;img data-src=&#34;https://w00zie.github.io/post/wae/img/mmd_penalty_hud51e3a5e3de122a467f8387f03d75af9_31553_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    WAE-MMD Penalty
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;random-generation&#34;&gt;Random Generation&lt;/h3&gt;
&lt;p&gt;The following are a few of the &amp;ldquo;best&amp;rdquo; samples (wrt my personal taste) that I picked: please &lt;strong&gt;make sure to lower the level of your audio output&lt;/strong&gt;&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; before playing! I suggest listening with headphones.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;div align=&#34;center&#34;&gt; WAE-GAN &lt;/div&gt;&lt;/th&gt;
&lt;th&gt;&lt;div align=&#34;center&#34;&gt; WAE-MMD &lt;/div&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waegan/015_HD_b.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waemmd/mmd_02_09_015_HD_b.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waegan/016_HD_b.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waemmd/mmd_02_09_025_HD_b.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waegan/024_HD_b.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waemmd/mmd_02_09_048_HD_b.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waegan/037_HD_b.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waemmd/mmd_02_09_052_HD_b.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waegan/058_HD_b.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/good/waemmd/mmd_02_09_055_HD_b.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The samples I was able to generete lightly suffer the problem of &lt;strong&gt;over-fragmentation&lt;/strong&gt;. The notes generated tend to be fragmented, sometimes yielding chaotic and unpleasant results. This problem has also been recognized in &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; and the authors of MuseGAN proposed a new version of the model where their generator was equipped with binary neurons&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;, making it able to map the input noise directly into binary-valued output tensors. This partially solved the issue.&lt;/p&gt;
&lt;p&gt;In my opinion WAE-GAN was able to produce better results: this was also the opinion of the authors of WAE.&lt;/p&gt;
&lt;h3 id=&#34;reconstruction&#34;&gt;Reconstruction&lt;/h3&gt;
&lt;p&gt;These are a few reconstructed samples from the test set.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;div align=&#34;center&#34;&gt; Original Sample &lt;/div&gt;&lt;/th&gt;
&lt;th&gt;&lt;div align=&#34;center&#34;&gt; WAE-GAN &lt;/div&gt;&lt;/th&gt;
&lt;th&gt;&lt;div align=&#34;center&#34;&gt; WAE-MMD &lt;/div&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/recon/007_true_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/recon/gan_007_fake_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/recon/mmd_007_fake_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/recon/012_true_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/recon/gan_012_fake_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/recon/mmd_012_fake_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/recon/020_true_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/recon/gan_020_fake_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/recon/mmd_020_fake_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/recon/023_true_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/recon/gan_023_fake_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/recon/mmd_023_fake_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/recon/042_true_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/recon/gan_042_fake_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;td&gt;
















  &lt;audio controls&gt;
    &lt;source src=&#34;music/recon/mmd_042_fake_HD.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Qualitatively speaking both models gave comparable results in terms of recostruction capabilities. It&amp;rsquo;s clear that they both struggle reconstructing the drums. Drums events are more sparse and they might need wider (and possibly more) filters in order to be reconstructed clearly.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1711.01558&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wasserstein Auto-Encoders&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1709.06298&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment&lt;/a&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;I&amp;rsquo;ve done the MIDI to MP3 conversions with the help of VLC and its integrated synthesizer 
&lt;a href=&#34;http://www.fluidsynth.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FluidSynth&lt;/a&gt;. I&amp;rsquo;ve done no post-processing (eq, fx, mixing, mastering) so what you hear comes straight out-of-the box. The sounds produced are unpleasant: please think of them as part of an experiment and not music meant to be played. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1605.09522&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kernel Mean Embedding of Distributions:
A Review and Beyond&lt;/a&gt; page 52 &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1511.06434&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks&lt;/a&gt; &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1802.03761&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On the Latent Space of Wasserstein Auto-Encoders&lt;/a&gt; &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1711.10337&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Are GANs Created Equal? A Large-Scale Study&lt;/a&gt; &lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1804.09399&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Convolutional Generative Adversarial Networks with Binary Neurons for Polyphonic Music Generation&lt;/a&gt; &lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
